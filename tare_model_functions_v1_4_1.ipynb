{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b2b5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Packages\n",
    "# Finish updating this later\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "project_root = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5d8339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importnb import Notebook\n",
    "with Notebook():\n",
    "    import tare_model_IO_functions_v1_4_1 as TARE_IO\n",
    "    # Gather data\n",
    "    hdd_factor_lookup = TARE_IO.load_hdd_factors(project_root)\n",
    "    emis_preIRA_cambium21_lookup, emis_IRA_2024_cambium22_lookup, emis_IRA_2025_2050_cambium23_lookup = TARE_IO.load_cambium_lookup(project_root)\n",
    "    emis_factor_co2e_naturalGas_ton_perkWh, emis_factor_co2e_propane_ton_perkWh, emis_factor_co2e_fuelOil_ton_perkWh = TARE_IO.load_emis_factors()\n",
    "    cpi_ratio_2023_2023, cpi_ratio_2023_2022, cpi_ratio_2023_2021, cpi_ratio_2023_2020, cpi_ratio_2023_2019, cpi_ratio_2023_2018, cpi_ratio_2023_2013, cpi_ratio_2023_2010, cpi_ratio_2023_2008 = TARE_IO.load_cpi_data(project_root)\n",
    "    epa_scc_usd2023_per_ton = TARE_IO.load_scc(cpi_ratio_2023_2020)\n",
    "    preIRA_fuel_price_lookup, iraRef_fuel_price_lookup = TARE_IO.load_fuel_price_lookups(project_root, cpi_ratio_2023_2018, cpi_ratio_2023_2019, cpi_ratio_2023_2020, cpi_ratio_2023_2021, cpi_ratio_2023_2022)\n",
    "    rsMeans_national_avg = TARE_IO.load_rsMeans_national_avg(cpi_ratio_2023_2019)\n",
    "    dict_heating_equipment_cost = TARE_IO.load_dict_heating_equipment_cost(project_root)\n",
    "    df_puma_medianIncome = TARE_IO.load_df_puma_medianIncome(project_root, cpi_ratio_2023_2022)\n",
    "    df_county_medianIncome = TARE_IO.load_df_county_medianIncome(project_root, cpi_ratio_2023_2022)\n",
    "    df_state_medianIncome = TARE_IO.load_df_state_medianIncome(project_root, cpi_ratio_2023_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2875f8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_prompt = \"\"\"\n",
    "Would you like to filter for a specific state's data? Please enter one of the following:\n",
    "N. I'd like to analyze all of the United States.\n",
    "Y. I'd like to filter data for a specific state.\n",
    "\"\"\"\n",
    "\n",
    "city_prompt = \"\"\"\n",
    "To accurately characterize load profile, it is recommended to select subsets of data with >= 1000 models (~240,000 representative dwelling units).\n",
    "\n",
    "The following cities (number of models also shown) are available for this state:\n",
    "\"\"\"\n",
    "\n",
    "city_menu_prompt = \"\"\"\n",
    "Would you like to filter a subset of city-level data? Please enter one of the following:\n",
    "N. I'd like to analyze all of my selected state.\n",
    "Y. I'd like to filter by city in the state.\n",
    "\"\"\"\n",
    "\n",
    "def get_menu_choice(prompt, choices):\n",
    "    while True:\n",
    "        choice = input(prompt).upper()\n",
    "        if choice in choices:\n",
    "            return choice\n",
    "        print(\"Invalid option. Please try again.\")\n",
    "\n",
    "def get_state_choice(df_copy):\n",
    "    while True:\n",
    "        input_state = input(\"Which state would you like to analyze data for? Please enter the two-letter abbreviation: \").upper()\n",
    "        if df_copy['in.state'].eq(input_state).any():\n",
    "            return input_state\n",
    "        print(\"Invalid state abbreviation. Please try again.\")\n",
    "\n",
    "def get_city_choice(df_copy, input_state):\n",
    "    while True:\n",
    "        input_cityFilter = input(\"Please enter the city name ONLY (e.g., Pittsburgh): \")\n",
    "        city_filter = df_copy['in.city'].eq(f\"{input_state}, {input_cityFilter}\")\n",
    "        if city_filter.any():\n",
    "            return input_cityFilter\n",
    "        print(\"Invalid city name. Please try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee82ce6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c9fc382",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def standardize_fuel_name(fuel_desc):\n",
    "    # Ensure that the input is a string\n",
    "    if pd.isna(fuel_desc):\n",
    "        return 'Other'  # Return 'Other' for NaN values\n",
    "    elif isinstance(fuel_desc, str):\n",
    "        if 'Electric' in fuel_desc:\n",
    "            return 'Electricity'\n",
    "        elif 'Gas' in fuel_desc:\n",
    "            return 'Natural Gas'\n",
    "        elif 'Propane' in fuel_desc:\n",
    "            return 'Propane'\n",
    "        elif 'Oil' in fuel_desc:\n",
    "            return 'Fuel Oil'\n",
    "        else:\n",
    "            return 'Other'  # For any unexpected types, categorize as 'Other'\n",
    "    else:\n",
    "        return 'Other'  # Non-string, non-NaN values are categorized as 'Other'\n",
    "\n",
    "def preprocess_fuel_data(df, column_name):\n",
    "    \"\"\"Applies standardization to a specified column in the DataFrame.\"\"\"\n",
    "    print(f\"Processing column: {column_name}\")\n",
    "    print(f\"Initial data types: {df[column_name].dtype}\")\n",
    "    \n",
    "    # Updated this portion of the code to prevent the setting with copy warning\n",
    "    df.loc[:, column_name] = df[column_name].apply(standardize_fuel_name)\n",
    "    \n",
    "    print(f\"Data types after processing: {df[column_name].dtype}\")\n",
    "    return df\n",
    "\n",
    "def apply_fuel_filter(df, category, enable):\n",
    "    if enable == 'Yes':\n",
    "        fuel_list = ['Natural Gas', 'Electricity', 'Propane', 'Fuel Oil']\n",
    "        df_filtered = df[df[f'base_{category}_fuel'].isin(fuel_list)]\n",
    "        print(f\"Filtered for the following fuels: {fuel_list}\")\n",
    "        return df_filtered\n",
    "    return df\n",
    "\n",
    "def apply_technology_filter(df, category, enable):\n",
    "    \"\"\"\n",
    "    Applies technology filters to the dataframe based on the category and whether filtering is enabled.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame to filter.\n",
    "    - category: The category of consumption (e.g., 'heating', 'waterHeating').\n",
    "    - enable: String flag ('Yes' or 'No') indicating whether to apply the filter.\n",
    "    \"\"\"\n",
    "    if enable == 'Yes':\n",
    "        if category == 'heating':\n",
    "            tech_list = [\n",
    "                'Electricity ASHP', 'Electricity Baseboard', 'Electricity Electric Boiler', 'Electricity Electric Furnace',\n",
    "                'Fuel Oil Fuel Boiler', 'Fuel Oil Fuel Furnace', 'Natural Gas Fuel Boiler', 'Natural Gas Fuel Furnace',\n",
    "                'Propane Fuel Boiler', 'Propane Fuel Furnace'\n",
    "            ]\n",
    "            df_filtered = df[df['heating_type'].isin(tech_list)]\n",
    "            print(f\"Filtered for the following Heating technologies: {tech_list}\")    \n",
    "            return df_filtered\n",
    "        \n",
    "        elif category == 'waterHeating':\n",
    "            tech_list = [\n",
    "                'Electric Heat Pump, 80 gal', 'Electric Premium', 'Electric Standard',\n",
    "                'Fuel Oil Premium', 'Fuel Oil Standard', 'Natural Gas Premium', 'Natural Gas Standard',\n",
    "                'Propane Premium', 'Propane Standard'\n",
    "            ]\n",
    "            df_filtered = df[df['waterHeating_type'].isin(tech_list)]\n",
    "            print(f\"Filtered for the following Water Heating technologies: {tech_list}\")\n",
    "            return df_filtered\n",
    "    \n",
    "    return df\n",
    "\n",
    "def debug_filters(df, filter_name):\n",
    "    if df.empty:\n",
    "        print(f\"No rows left after applying {filter_name}\")\n",
    "    else:\n",
    "        print(f\"{len(df)} rows remain after applying {filter_name}\")\n",
    "\n",
    "# Function to extract city name\n",
    "def extract_city_name(row):\n",
    "    match = re.match(r'^[A-Z]{2}, (.+)$', row)\n",
    "    return match.group(1) if match else row\n",
    "        \n",
    "def df_enduse_refactored(df_baseline, fuel_filter='Yes', tech_filter='Yes'):\n",
    "    # Initial check\n",
    "    if df_baseline.empty:\n",
    "        print(\"Warning: Input DataFrame is empty\")\n",
    "        return df_baseline\n",
    "\n",
    "    # Standardize fuel names in the base columns before creating the df_enduse\n",
    "    df_baseline = preprocess_fuel_data(df_baseline, 'in.clothes_dryer')\n",
    "    df_baseline = preprocess_fuel_data(df_baseline, 'in.cooking_range')\n",
    "\n",
    "    # Map standardized names to new columns\n",
    "    df_baseline['base_clothesDrying_fuel'] = df_baseline['in.clothes_dryer']\n",
    "    df_baseline['base_cooking_fuel'] = df_baseline['in.cooking_range']\n",
    "    \n",
    "    # Initialize df_enduse from df_baseline with all required columns\n",
    "    # (assuming columns are correctly listed here)\n",
    "    # Create a new DataFrame named df_enduse\n",
    "    # using pd.DataFrame constructor and initialize it with columns from df_baseline\n",
    "    df_enduse = pd.DataFrame({\n",
    "        'bldg_id': df_baseline['bldg_id'],\n",
    "        'square_footage': df_baseline['in.sqft'],\n",
    "        'census_region': df_baseline['in.census_region'],\n",
    "        'census_division': df_baseline['in.census_division'],\n",
    "        'census_division_recs': df_baseline['in.census_division_recs'],\n",
    "        'building_america_climate_zone': df_baseline['in.building_america_climate_zone'],\n",
    "        'reeds_balancing_area': df_baseline['in.reeds_balancing_area'],\n",
    "        'state': df_baseline['in.state'],\n",
    "        'city': df_baseline['in.city'].apply(extract_city_name),\n",
    "        'county': df_baseline['in.county'],\n",
    "        'puma': df_baseline['in.puma'],\n",
    "        'county_and_puma': df_baseline['in.county_and_puma'],\n",
    "        'weather_file_city': df_baseline['in.weather_file_city'],\n",
    "        'Longitude': df_baseline['in.weather_file_longitude'],\n",
    "        'Latitude': df_baseline['in.weather_file_latitude'],\n",
    "        'building_type': df_baseline['in.geometry_building_type_recs'],\n",
    "        'income': df_baseline['in.income'],\n",
    "        'federal_poverty_level': df_baseline['in.federal_poverty_level'],\n",
    "        'occupancy': df_baseline['in.occupants'],\n",
    "        'tenure': df_baseline['in.tenure'],\n",
    "        'vacancy_status': df_baseline['in.vacancy_status'],\n",
    "        'base_heating_fuel': df_baseline['in.heating_fuel'],\n",
    "        'heating_type': df_baseline['in.hvac_heating_type_and_fuel'],\n",
    "        'hvac_cooling_type': df_baseline['in.hvac_cooling_type'],\n",
    "        'vintage': df_baseline['in.vintage'],\n",
    "        'base_heating_efficiency': df_baseline['in.hvac_heating_efficiency'],\n",
    "        'base_electricity_heating_consumption': df_baseline['out.electricity.heating.energy_consumption.kwh'],\n",
    "        'base_fuelOil_heating_consumption': df_baseline['out.fuel_oil.heating.energy_consumption.kwh'],\n",
    "        'base_naturalGas_heating_consumption': df_baseline['out.natural_gas.heating.energy_consumption.kwh'],\n",
    "        'base_propane_heating_consumption': df_baseline['out.propane.heating.energy_consumption.kwh'],\n",
    "        'base_waterHeating_fuel': df_baseline['in.water_heater_fuel'],\n",
    "        'waterHeating_type': df_baseline['in.water_heater_efficiency'],\n",
    "        'base_electricity_waterHeating_consumption': df_baseline['out.electricity.hot_water.energy_consumption.kwh'],\n",
    "        'base_fuelOil_waterHeating_consumption': df_baseline['out.fuel_oil.hot_water.energy_consumption.kwh'],\n",
    "        'base_naturalGas_waterHeating_consumption': df_baseline['out.natural_gas.hot_water.energy_consumption.kwh'],\n",
    "        'base_propane_waterHeating_consumption': df_baseline['out.propane.hot_water.energy_consumption.kwh'],\n",
    "        'base_clothesDrying_fuel': df_baseline['in.clothes_dryer'],\n",
    "        'base_electricity_clothesDrying_consumption': df_baseline['out.electricity.clothes_dryer.energy_consumption.kwh'],\n",
    "        'base_naturalGas_clothesDrying_consumption': df_baseline['out.natural_gas.clothes_dryer.energy_consumption.kwh'],\n",
    "        'base_propane_clothesDrying_consumption': df_baseline['out.propane.clothes_dryer.energy_consumption.kwh'],\n",
    "        'base_cooking_fuel': df_baseline['in.cooking_range'],\n",
    "        'base_electricity_cooking_consumption': df_baseline['out.electricity.range_oven.energy_consumption.kwh'],\n",
    "        'base_naturalGas_cooking_consumption': df_baseline['out.natural_gas.range_oven.energy_consumption.kwh'],\n",
    "        'base_propane_cooking_consumption': df_baseline['out.propane.range_oven.energy_consumption.kwh']\n",
    "    })\n",
    "    \n",
    "    categories = ['heating', 'waterHeating', 'clothesDrying', 'cooking']\n",
    "    for category in categories:\n",
    "        if category == 'heating' or category == 'waterHeating':\n",
    "            fuel_types = ['electricity', 'fuelOil', 'naturalGas', 'propane']\n",
    "            # Calculate and update total consumption\n",
    "            total_consumption = sum(df_enduse.get(f'base_{fuel}_{category}_consumption', pd.Series([], dtype=float)).fillna(0) for fuel in fuel_types)\n",
    "            df_enduse[f'baseline_{category}_consumption'] = total_consumption.replace(0, np.nan)\n",
    "\n",
    "            debug_filters(df_enduse, f\"total {category} consumption calculation\")\n",
    "\n",
    "            # Apply filters\n",
    "            df_enduse = apply_fuel_filter(df_enduse, category, fuel_filter)\n",
    "            debug_filters(df_enduse, f\"{category} fuel filter\")\n",
    "\n",
    "            df_enduse = apply_technology_filter(df_enduse, category, tech_filter)\n",
    "            debug_filters(df_enduse, f\"{category} technology filter\")\n",
    "\n",
    "        else:\n",
    "            fuel_types = ['electricity', 'naturalGas', 'propane']\n",
    "            # Calculate and update total consumption\n",
    "            total_consumption = sum(df_enduse.get(f'base_{fuel}_{category}_consumption', pd.Series([], dtype=float)).fillna(0) for fuel in fuel_types)\n",
    "            df_enduse[f'baseline_{category}_consumption'] = total_consumption.replace(0, np.nan)\n",
    "\n",
    "            debug_filters(df_enduse, f\"total {category} consumption calculation\")\n",
    "\n",
    "            # Apply filters\n",
    "            df_enduse = apply_fuel_filter(df_enduse, category, fuel_filter)\n",
    "            debug_filters(df_enduse, f\"{category} fuel filter\")\n",
    "            \n",
    "    return df_enduse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81b01065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_future_consumption(df, hdd_factor_lookup, menu_mp):\n",
    "    \"\"\"\n",
    "    Projects future energy consumption based on baseline or upgraded equipment specifications.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing baseline consumption data.\n",
    "    hdd_factor_lookup (dict): A dictionary with Heating Degree Day (HDD) factors for different census divisions and years.\n",
    "    menu_mp (int): Indicates the measure package to apply. 0 for baseline, 8/9/10 for retrofit scenarios.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with projected future energy consumption and reductions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Equipment lifetime specifications in years\n",
    "    equipment_specs = {\n",
    "        'heating': 15,\n",
    "        # 'waterHeating': 12,\n",
    "        # 'clothesDrying': 13,\n",
    "        # 'cooking': 15\n",
    "    }\n",
    "\n",
    "    # Create a copy of the input DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Check if the 'census_division' column exists in the DataFrame\n",
    "    if 'census_division' not in df_copy.columns:\n",
    "        raise KeyError(\"'census_division' column is missing from the DataFrame\")\n",
    "\n",
    "    # Prepare a dictionary to hold new columns for projected consumption\n",
    "    new_columns = {}\n",
    "\n",
    "    # Baseline policy_scenario: Existing Equipment\n",
    "    if menu_mp == 0:\n",
    "        for category, lifetime in equipment_specs.items():\n",
    "            print(f\"Projecting Future Energy Consumption (Baseline Equipment): {category}\")\n",
    "            for year in range(1, lifetime + 1):\n",
    "                year_label = 2023 + year\n",
    "\n",
    "                # Adjust consumption based on HDD factors for heating and water heating\n",
    "                if category in ['heating', 'waterHeating']:\n",
    "                    hdd_factor = df_copy['census_division'].map(lambda x: hdd_factor_lookup.get(x, {}).get(year_label, hdd_factor_lookup['National'][year_label]))\n",
    "                    new_columns[f'baseline_{year_label}_{category}_consumption'] = (df_copy[f'baseline_{category}_consumption'] * hdd_factor).round(2)\n",
    "\n",
    "                else:\n",
    "                    new_columns[f'baseline_{year_label}_{category}_consumption'] = df_copy[f'baseline_{category}_consumption'].round(2)\n",
    "\n",
    "    # Retrofit policy_scenario: Upgraded Equipment (Measure Packages 8, 9, 10)\n",
    "    else:\n",
    "        for category, lifetime in equipment_specs.items():\n",
    "            print(f\"Projecting Future Energy Consumption (Upgraded Equipment): {category}\")\n",
    "            for year in range(1, lifetime + 1):\n",
    "                year_label = 2023 + year\n",
    "\n",
    "                # Adjust consumption based on HDD factors for heating and water heating\n",
    "                if category in ['heating', 'waterHeating']:\n",
    "                    hdd_factor = df_copy['census_division'].map(lambda x: hdd_factor_lookup.get(x, {}).get(year_label, hdd_factor_lookup['National'][year_label]))\n",
    "                    new_columns[f'mp{menu_mp}_{year_label}_{category}_consumption'] = (df_copy[f'mp{menu_mp}_{category}_consumption'] * hdd_factor).round(2)\n",
    "\n",
    "                    # Calculate the reduction in annual energy consumption\n",
    "                    new_columns[f'mp{menu_mp}_{year_label}_{category}_reduction_consumption'] = df_copy[f'baseline_{year_label}_{category}_consumption'].sub(\n",
    "                        new_columns[f'mp{menu_mp}_{year_label}_{category}_consumption'], axis=0, fill_value=0\n",
    "                    ).round(2)\n",
    "                else:\n",
    "                    new_columns[f'mp{menu_mp}_{year_label}_{category}_consumption'] = df_copy[f'mp{menu_mp}_{category}_consumption'].round(2)\n",
    "\n",
    "                    # Calculate the reduction in annual energy consumption\n",
    "                    new_columns[f'mp{menu_mp}_{year_label}_{category}_reduction_consumption'] = df_copy[f'baseline_{year_label}_{category}_consumption'].sub(\n",
    "                        new_columns[f'mp{menu_mp}_{year_label}_{category}_consumption'], axis=0, fill_value=0\n",
    "                    ).round(2)\n",
    "\n",
    "    # Calculate the new columns based on policy scenario and create dataframe based on df_copy index\n",
    "    df_new_columns = pd.DataFrame(new_columns, index=df_copy.index)\n",
    "\n",
    "    # Identify overlapping columns between the new and existing DataFrame.\n",
    "    overlapping_columns = df_new_columns.columns.intersection(df_copy.columns)\n",
    "\n",
    "    # Drop overlapping columns from df_copy.\n",
    "    if not overlapping_columns.empty:\n",
    "        df_copy.drop(columns=overlapping_columns, inplace=True)\n",
    "\n",
    "    # Merge new columns into df_copy, ensuring no duplicates or overwrites occur.\n",
    "    df_copy = df_copy.join(df_new_columns, how='left')\n",
    "\n",
    "    # Return the updated DataFrame.\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5eb4050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAST UPDATED SEPTEMBER 8, 2024 @ 5:26 PM\n",
    "# Constants and mappings\n",
    "td_losses = 0.06\n",
    "td_losses_multiplier = 1 / (1 - td_losses)\n",
    "\n",
    "equipment_specs = {\n",
    "    'heating': 15#, 'waterHeating': 12, 'clothesDrying': 13, 'cooking': 15\n",
    "}\n",
    "\n",
    "def calculate_marginal_damages(df, menu_mp, policy_scenario, df_summary):\n",
    "    \"\"\"\n",
    "    Calculate the marginal damages of different pollutants based on various conditions and mappings.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The primary data frame containing pollutant emissions data and other relevant attributes.\n",
    "    - menu_mp (int): Identifies measure package (retrofit being conducted)\n",
    "    - policy_scenario (str): Policy scenarios that determine electricity grid projections (No Inflation Reduction Act or AEO2023 Reference Case) \n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: The updated data frame with calculated marginal damages and potentially new columns.\n",
    "    \n",
    "    This function processes a given DataFrame 'df' to:\n",
    "    - Copy the DataFrame to avoid modification of the original data.\n",
    "    - Map regional identifiers to a subregion grid.\n",
    "    - Calculate the natural gas leakage factor based on state.\n",
    "    - Create and calculate damage factor columns if they do not exist.\n",
    "    - Depending on the flag 'grid_decarb', apply different damage calculation methods.\n",
    "    - Manage and merge newly created columns to avoid duplicates and ensure data integrity.\n",
    "\n",
    "    UPDATES:\n",
    "    - Aug/Sep 2024: Focus on CO2 (EPA SCC of $190USD-2020), Cambium Dataset for Emissions, Removed CEDM and EASIUR, Emissions Lookup/Projections instead of Damages\n",
    "    \"\"\"\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Determine scenario-specific settings\n",
    "    if menu_mp == 0:\n",
    "        scenario_prefix = \"baseline_\"\n",
    "        cambium_scenario = 'MidCase'\n",
    "        emis_electricity_lookup = emis_preIRA_cambium21_lookup\n",
    "    else:\n",
    "        if policy_scenario == 'No Inflation Reduction Act':\n",
    "            scenario_prefix = f\"preIRA_mp{menu_mp}_\"\n",
    "            cambium_scenario = 'MidCase'\n",
    "            emis_electricity_lookup = emis_preIRA_cambium21_lookup\n",
    "        elif policy_scenario == 'AEO2023 Reference Case':\n",
    "            scenario_prefix = f\"iraRef_mp{menu_mp}_\"\n",
    "            cambium_scenario = 'MidCase'\n",
    "            emis_electricity_lookup = emis_IRA_2025_2050_cambium23_lookup\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Policy Scenario! Please choose from 'No Inflation Reduction Act' or 'AEO2023 Reference Case'.\")\n",
    "\n",
    "    # Precompute hdd_factors for each region and year once\n",
    "    hdd_factors_per_year = {\n",
    "        year_label: df_copy['census_division'].map(\n",
    "            lambda x: hdd_factor_lookup.get(x, hdd_factor_lookup['National']).get(year_label, 1.0)\n",
    "        )\n",
    "        for year_label in range(2024, 2024 + max(equipment_specs.values()) + 1)\n",
    "    }\n",
    "\n",
    "    # Calculate the new columns based on grid scenarios.\n",
    "    df_new_columns = calculate_damages_grid_scenario(\n",
    "        df_copy, df_summary, menu_mp, td_losses_multiplier, emis_electricity_lookup,\n",
    "        policy_scenario, cambium_scenario, scenario_prefix, hdd_factors_per_year\n",
    "    )\n",
    "\n",
    "    # Drop overlapping columns and merge new data\n",
    "    overlapping_columns = df_new_columns.columns.intersection(df_copy.columns)\n",
    "    if not overlapping_columns.empty:\n",
    "        df_copy.drop(columns=overlapping_columns, inplace=True)\n",
    "\n",
    "    df_copy = df_copy.join(df_new_columns, how='left')\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def calculate_damages_grid_scenario(df_copy, df_summary, menu_mp, td_losses_multiplier, emis_electricity_lookup, policy_scenario, cambium_scenario, scenario_prefix, hdd_factors_per_year):\n",
    "    \"\"\"\n",
    "    Calculate damages for the specified electricity grid policy_scenario.\n",
    "\n",
    "    Parameters:\n",
    "        df_copy (DataFrame): The DataFrame containing consumption data.\n",
    "        menu_mp (int): The menu number for the measure package.\n",
    "        pollutants (list): List of pollutants.\n",
    "        td_losses (float): Transmission and distribution losses.\n",
    "        emis_electricity_lookup (dict): Lookup table for damages from CEDM or preIRA_damages_electricity_lookup.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with calculated damages.\n",
    "    \"\"\"\n",
    "\n",
    "    new_columns_data = {}\n",
    "\n",
    "    if menu_mp == 0:\n",
    "        for category, lifetime in equipment_specs.items():\n",
    "            print(f\"Calculating marginal emissions and marginal damages for {category}\")\n",
    "            lifetime_emissions = 0\n",
    "            lifetime_damages = 0\n",
    "            \n",
    "            for year in range(1, lifetime + 1):\n",
    "                year_label = year + 2023\n",
    "                emis_col = f'baseline_{year_label}_{category}_tons_co2e'\n",
    "                damage_col = f'baseline_{year_label}_{category}_damages_climate'\n",
    "                \n",
    "                # Get precomputed hdd_factor for the current year\n",
    "                # Adjust consumption based on HDD factors for heating and water heating\n",
    "                if category in ['heating', 'waterHeating']:\n",
    "                    hdd_factor = hdd_factors_per_year[year_label]\n",
    "                else:\n",
    "                    hdd_factor = 1.0\n",
    "\n",
    "                # Multiplied by HDD factor in Baseline Emissions Calculation but NOT IN POST-RETROFIT\n",
    "                # Baseline Energy Consumption Projections only performed on total end-use energy consumption (not by fuel_type)\n",
    "                # Consumption by fuel_type is needed for emissions calculations due to \n",
    "                emis_electricity = (\n",
    "                    df_copy[f'base_electricity_{category}_consumption'] *\n",
    "                    hdd_factor *\n",
    "                    td_losses_multiplier *\n",
    "                    df_copy.apply(\n",
    "                        lambda row: emis_electricity_lookup.get(\n",
    "                            (cambium_scenario, row['state'], row['reeds_balancing_area']), {}\n",
    "                        ).get(year_label, np.nan),\n",
    "                        axis=1\n",
    "                    ).fillna(0)\n",
    "                )\n",
    "                \n",
    "                # FOSSIL FUELS\n",
    "                # Natural Gas Emissions \n",
    "                emis_naturalGas = df_copy[f'base_naturalGas_{category}_consumption'] * hdd_factor * emis_factor_co2e_naturalGas_ton_perkWh\n",
    "\n",
    "                # Propane Emissions\n",
    "                emis_propane = df_copy[f'base_propane_{category}_consumption'] * hdd_factor * emis_factor_co2e_propane_ton_perkWh\n",
    "\n",
    "                if 'cooking' in category or 'clothesDrying' in category:\n",
    "                    fossilFuel_emissions = emis_naturalGas.fillna(0) + emis_propane.fillna(0)\n",
    "\n",
    "                else:\n",
    "                    emis_fuelOil = df_copy[f'base_fuelOil_{category}_consumption'] * hdd_factor * emis_factor_co2e_fuelOil_ton_perkWh\n",
    "\n",
    "                    fossilFuel_emissions = emis_naturalGas.fillna(0) + emis_propane.fillna(0) + emis_fuelOil.fillna(0)\n",
    "\n",
    "                total_emissions = fossilFuel_emissions + emis_electricity\n",
    "                total_damages = total_emissions * epa_scc_usd2023_per_ton\n",
    "\n",
    "                new_columns_data[emis_col] = np.round(total_emissions, 2)\n",
    "                new_columns_data[damage_col] = np.round(total_damages, 2)\n",
    "            \n",
    "                # Accumulate the emissions and damages\n",
    "                lifetime_emissions += total_emissions\n",
    "                lifetime_damages += total_damages\n",
    "\n",
    "            # Columns for Lifetime (Current Scenario Equipment) and Avoided (Reductions from Baseline) Emissions and Damages\n",
    "            lifetime_emissions_col = f'{scenario_prefix}{category}_lifetime_tons_co2e'\n",
    "            lifetime_damages_col = f'{scenario_prefix}{category}_lifetime_damages_climate'            \n",
    "\n",
    "            # Lifetime Emissions and Damages\n",
    "            new_columns_data[lifetime_emissions_col] = np.round(lifetime_emissions, 2)\n",
    "            new_columns_data[lifetime_damages_col] = np.round(lifetime_damages, 2)\n",
    "\n",
    "            # Summary Dataframe with main model results\n",
    "            df_summary[lifetime_emissions_col] = np.round(lifetime_emissions, 2)\n",
    "            df_summary[lifetime_damages_col] = np.round(lifetime_damages, 2)            \n",
    "\n",
    "    else:\n",
    "        for category, lifetime in equipment_specs.items():\n",
    "            print(f\"Calculating marginal emissions and marginal damages for {category}\")\n",
    "            lifetime_emissions = 0\n",
    "            lifetime_damages = 0\n",
    "\n",
    "            for year in range(1, lifetime + 1):\n",
    "                year_label = year + 2023\n",
    "                consumption_col = f'mp{menu_mp}_{year_label}_{category}_consumption' # Already includes HDD projection adjustment\n",
    "                emis_col = f'{scenario_prefix}{year_label}_{category}_tons_co2e'\n",
    "                damage_col = f'{scenario_prefix}{year_label}_{category}_damages_climate'\n",
    "\n",
    "                # ELECTRICITY\n",
    "                # The Electricity Lookup Dictionary Depends on the Current Year \n",
    "                # Cambium began in 2025 and there are no historical data available for the ReEDS Balancing Areas \n",
    "                if policy_scenario != 'No Inflation Reduction Act':\n",
    "                    if year_label == 2024:\n",
    "                        emis_electricity_lookup = emis_IRA_2024_cambium22_lookup\n",
    "                    else:\n",
    "                        emis_electricity_lookup = emis_IRA_2025_2050_cambium23_lookup\n",
    "\n",
    "                # POST-RETROFIT (MP) consumption includes HDD projection adjustment\n",
    "                emis_electricity = (\n",
    "                    df_copy[consumption_col] *\n",
    "                    td_losses_multiplier *\n",
    "                    df_copy.apply(\n",
    "                        lambda row: emis_electricity_lookup.get(\n",
    "                            (cambium_scenario, row['state'], row['reeds_balancing_area']), {}\n",
    "                        ).get(year_label, np.nan),\n",
    "                        axis=1\n",
    "                    ).fillna(0)\n",
    "                )\n",
    "\n",
    "                total_damages = emis_electricity * epa_scc_usd2023_per_ton\n",
    "\n",
    "                new_columns_data[emis_col] = np.round(emis_electricity, 2)\n",
    "                new_columns_data[damage_col] = np.round(total_damages, 2)\n",
    "            \n",
    "                # Accumulate the emissions and damages\n",
    "                lifetime_emissions += emis_electricity\n",
    "                lifetime_damages += total_damages\n",
    "\n",
    "            # Columns for Lifetime (Current Scenario Equipment) and Avoided (Reductions from Baseline) Emissions and Damages\n",
    "            lifetime_emissions_col = f'{scenario_prefix}{category}_lifetime_tons_co2e'\n",
    "            lifetime_damages_col = f'{scenario_prefix}{category}_lifetime_damages_climate'            \n",
    "            avoided_emissions_col = f'{scenario_prefix}{category}_avoided_tons_co2e'\n",
    "            avoided_damages_col = f'{scenario_prefix}{category}_avoided_damages_climate'\n",
    "\n",
    "            # Lifetime Emissions and Damages\n",
    "            new_columns_data[lifetime_emissions_col] = np.round(lifetime_emissions, 2)\n",
    "            new_columns_data[lifetime_damages_col] = np.round(lifetime_damages, 2)\n",
    "\n",
    "            # Avoided Emissions and Damages\n",
    "            new_columns_data[avoided_emissions_col] = np.round(df_copy[f'baseline_{category}_lifetime_tons_co2e'] - new_columns_data[lifetime_emissions_col], 2)\n",
    "            new_columns_data[avoided_damages_col] = np.round(df_copy[f'baseline_{category}_lifetime_damages_climate'] - new_columns_data[lifetime_damages_col], 2)\n",
    "\n",
    "            # Summary Dataframe with main model results\n",
    "            df_summary[lifetime_emissions_col] = np.round(lifetime_emissions, 2)\n",
    "            df_summary[lifetime_damages_col] = np.round(lifetime_damages, 2)            \n",
    "            df_summary[avoided_emissions_col] = np.round(df_copy[f'baseline_{category}_lifetime_tons_co2e'] - new_columns_data[lifetime_emissions_col], 2)\n",
    "            df_summary[avoided_damages_col] = np.round(df_copy[f'baseline_{category}_lifetime_damages_climate'] - new_columns_data[lifetime_damages_col], 2)\n",
    "\n",
    "    df_new_columns = pd.DataFrame(new_columns_data, index=df_copy.index)\n",
    "\n",
    "    return df_new_columns\n",
    "    return df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7c645bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define function to create a fuel price lookup dictionary without policy_scenario from row\n",
    "def create_fuel_price_lookup(df, policy_scenario):\n",
    "    lookup_dict = {}\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        location = row['location_map']\n",
    "        fuel_type = row['fuel_type']\n",
    "        \n",
    "        if location not in lookup_dict:\n",
    "            lookup_dict[location] = {}\n",
    "        \n",
    "        if fuel_type not in lookup_dict[location]:\n",
    "            lookup_dict[location][fuel_type] = {}\n",
    "        \n",
    "        if policy_scenario not in lookup_dict[location][fuel_type]:\n",
    "            lookup_dict[location][fuel_type][policy_scenario] = {}\n",
    "        \n",
    "        for year in range(2022, 2051):\n",
    "            column_name = f\"{year}_fuelPrice_perkWh\"\n",
    "            lookup_dict[location][fuel_type][policy_scenario][year] = row[column_name]\n",
    "    \n",
    "    return lookup_dict\n",
    "\n",
    "# Define function to project future prices with fallback to 'National'\n",
    "def project_future_prices(row, factor_dict, policy_scenario):\n",
    "    loc = row['census_division']\n",
    "    fuel = row['fuel_type']\n",
    "    price_2022 = row['2022_fuelPrice_perkWh']\n",
    "\n",
    "    print(f\"\\nProcessing location: {loc}, fuel: {fuel}, policy_scenario: {policy_scenario}\")\n",
    "    print(f\"Initial price for 2022: {price_2022}\")\n",
    "\n",
    "    # First, try to fetch the projection factors for the specific region\n",
    "    projection_factors = factor_dict.get((loc, fuel, policy_scenario))\n",
    "    \n",
    "    # If no factors are found for the specific region, default to 'National'\n",
    "    if not projection_factors:\n",
    "        print(f\"No projection factors found for {loc}, {fuel}, {policy_scenario}. Defaulting to 'National'.\")\n",
    "        projection_factors = factor_dict.get(('National', fuel, policy_scenario))\n",
    "        \n",
    "    if projection_factors:\n",
    "        print(f\"Using projection factors for {loc if projection_factors else 'National'}, {fuel}, {policy_scenario}: {projection_factors}\")\n",
    "    else:\n",
    "        print(f\"No projection factors found for 'National', {fuel}, {policy_scenario} either. Cannot project future prices.\")\n",
    "        return pd.Series()  # Return an empty Series if no factors are found\n",
    "\n",
    "    future_prices = {}\n",
    "    for year in range(2022, 2051):\n",
    "        if projection_factors and year in projection_factors:\n",
    "            factor = projection_factors[year]\n",
    "            future_price = price_2022 * factor\n",
    "            future_prices[f'{year}_fuelPrice_perkWh'] = future_price\n",
    "            print(f\"Year: {year}, Factor: {factor}, Future Price: {future_price}\")\n",
    "        else:\n",
    "            print(f\"Missing factor for year {year} in {loc if projection_factors else 'National'}, {fuel}, {policy_scenario}. Skipping this year.\")\n",
    "    \n",
    "    return pd.Series(future_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc7472ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAST UPDATED SEPTEMBER 5, 2024 @ 9:37 PM\n",
    "def calculate_annual_fuelCost(df, menu_mp, policy_scenario, drop_fuel_cost_columns):\n",
    "    \"\"\"\n",
    "    Calculate the annual fuel cost for baseline and measure packages.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing baseline fuel consumption data.\n",
    "    menu_mp (int): Measure package identifier\n",
    "    policy_scenario (str): Name of EIA AEO policy_scenario used to project fuel prices\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with additional columns for annual fuel costs, savings, and changes.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Determine the scenario prefix and fuel price lookup based on menu_mp and policy_scenario\n",
    "    if menu_mp == 0:\n",
    "        scenario_prefix = \"baseline_\"\n",
    "        fuel_price_lookup = preIRA_fuel_price_lookup\n",
    "    else:\n",
    "        if policy_scenario == 'No Inflation Reduction Act':\n",
    "            scenario_prefix = f\"preIRA_mp{menu_mp}_\"\n",
    "            fuel_price_lookup = preIRA_fuel_price_lookup\n",
    "        elif policy_scenario == 'AEO2023 Reference Case':\n",
    "            scenario_prefix = f\"iraRef_mp{menu_mp}_\"\n",
    "            fuel_price_lookup = iraRef_fuel_price_lookup\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Policy policy_scenario! Please choose from 'No Inflation Reduction Act' or 'AEO2023 Reference Case'.\")\n",
    "\n",
    "    # Fuel type mapping and equipment lifetime specifications\n",
    "    fuel_mapping = {'Electricity': 'electricity', 'Natural Gas': 'naturalGas', 'Fuel Oil': 'fuelOil', 'Propane': 'propane'}\n",
    "    equipment_specs = {'heating': 15}#, 'waterHeating': 12, 'clothesDrying': 13, 'cooking': 15}\n",
    "\n",
    "    # Initialize a dictionary to hold new columns\n",
    "    new_columns = {}\n",
    "\n",
    "    # If baseline calculations are required\n",
    "    if menu_mp == 0:\n",
    "        for category in equipment_specs:\n",
    "            df_copy[f'fuel_type_{category}'] = df_copy[f'base_{category}_fuel'].map(fuel_mapping)\n",
    "\n",
    "        for category, lifetime in equipment_specs.items():\n",
    "            print(f\"Calculating BASELINE (no retrofit) fuel costs from 2024 to {2024 + lifetime} for {category}\")\n",
    "            for year in range(1, lifetime + 1):\n",
    "                year_label = year + 2023\n",
    "\n",
    "                fuel_costs = df_copy.apply(lambda row: round(\n",
    "                    row[f'baseline_{year_label}_{category}_consumption'] *\n",
    "                    fuel_price_lookup.get(\n",
    "                        row['state'] if row[f'fuel_type_{category}'] in ['electricity', 'naturalGas'] else row['census_division'],\n",
    "                        {}\n",
    "                    ).get(row[f'fuel_type_{category}'], {}).get(policy_scenario, {}).get(year_label, 0), 2),\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "                new_columns[f'baseline_{year_label}_{category}_fuelCost'] = fuel_costs\n",
    "\n",
    "    else:\n",
    "        for category, lifetime in equipment_specs.items():\n",
    "            print(f\"Calculating POST-RETROFIT (MP{menu_mp}) fuel costs from 2024 to {2024 + lifetime} for {category}\")\n",
    "            for year in range(1, lifetime + 1):\n",
    "                year_label = year + 2023\n",
    "\n",
    "                fuel_costs = df_copy.apply(lambda row: round(\n",
    "                    row[f'mp{menu_mp}_{year_label}_{category}_consumption'] *\n",
    "                    fuel_price_lookup.get(row['state'], {}).get('electricity', {}).get(policy_scenario, {}).get(year_label, 0), 2),\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "                # Store all new columns in the dictionary first\n",
    "                new_columns[f'{scenario_prefix}{year_label}_{category}_fuelCost'] = fuel_costs\n",
    "                \n",
    "                new_columns[f'{scenario_prefix}{year_label}_{category}_savings_fuelCost'] = (\n",
    "                    df_copy[f'baseline_{year_label}_{category}_fuelCost'] - fuel_costs\n",
    "                )\n",
    "\n",
    "        # Only drop if annual fuel cost savings have already been calculated\n",
    "        # Drop fuel cost columns if the flag is True\n",
    "        if drop_fuel_cost_columns:\n",
    "            print(\"Dropping Annual Fuel Costs for Baseline Scenario and Retrofit. Storing Fuel Savings for Private NPV Calculation.\")\n",
    "            fuel_cost_columns = [col for col in df_copy.columns if '_fuelCost' in col and '_savings_fuelCost' not in col]\n",
    "            df_copy.drop(columns=fuel_cost_columns, inplace=True)\n",
    "\n",
    "    # Calculate the new columns based on policy scenario and create dataframe based on df_copy index\n",
    "    df_new_columns = pd.DataFrame(new_columns, index=df_copy.index)\n",
    "\n",
    "    # Identify overlapping columns between the new and existing DataFrame.\n",
    "    overlapping_columns = df_new_columns.columns.intersection(df_copy.columns)\n",
    "\n",
    "    # Drop overlapping columns from df_copy.\n",
    "    if not overlapping_columns.empty:\n",
    "        df_copy.drop(columns=overlapping_columns, inplace=True)\n",
    "\n",
    "    # Merge new columns into df_copy, ensuring no duplicates or overwrites occur.\n",
    "    df_copy = df_copy.join(df_new_columns, how='left')\n",
    "\n",
    "    # Return the updated DataFrame.\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca4223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_county_medianIncome(project_root, cpi_ratio_2023_2022):\n",
    "    # Collect Area Median Income Data at county-resolution\n",
    "    filename = \"nhgis0005_ds261_2022_county.csv\"\n",
    "    relative_path = os.path.join(r\"equity_data\", filename)\n",
    "    file_path = os.path.join(project_root, relative_path)\n",
    "\n",
    "    print(f\"Retrieved data for filename: {filename}\")\n",
    "    print(f\"Located at filepath: {file_path}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    df_county_medianIncome = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    # df_county_medianIncome = df_county_medianIncome.drop(0)\n",
    "    df_county_medianIncome = df_county_medianIncome.reset_index(drop=True)\n",
    "\n",
    "    cols_interest = ['GISJOIN', 'STUSAB', 'COUNTYA', 'NAME_E', 'AP2PE001', 'AP2PM001']\n",
    "    df_county_medianIncome = df_county_medianIncome[cols_interest]\n",
    "    df_county_medianIncome = df_county_medianIncome.rename(columns={\"GISJOIN\": \"gis_joinID_county\", \"STUSAB\": \"state_abbrev\", \"COUNTYA\": \"county_code\", \"NAME_E\": \"name_estimate\", \"AP2PE001\": \"median_income_USD2022\", \"AP2PM001\": \"median_income_USD2022_marginOfError\"})\n",
    "    df_county_medianIncome['median_income_USD2023'] = round((df_county_medianIncome['median_income_USD2022'] * cpi_ratio_2023_2022), 2)\n",
    "    return df_county_medianIncome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501783c1",
   "metadata": {},
   "source": [
    "# Retrofit Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613aa076",
   "metadata": {},
   "source": [
    "## Basic Retrofit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "46bd45e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_enduse_compare(df_mp, input_mp, menu_mp, df_baseline):\n",
    "    # Create a new DataFrame named df_compare\n",
    "    # using pd.DataFrame constructor and initialize it with columns from df_mp\n",
    "    df_compare = pd.DataFrame({\n",
    "        'bldg_id':df_mp['bldg_id'],\n",
    "        'hvac_has_ducts': df_mp['in.hvac_has_ducts'],\n",
    "        'baseline_heating_type': df_mp['in.hvac_heating_type_and_fuel'],\n",
    "        'hvac_heating_efficiency': df_mp['in.hvac_heating_efficiency'],\n",
    "        'hvac_heating_type_and_fuel': df_mp['in.hvac_heating_type_and_fuel'],\n",
    "        'size_heat_pump_backup_primary_k_btu_h': df_mp['out.params.size_heat_pump_backup_primary_k_btu_h'],\n",
    "        'size_heating_system_primary_k_btu_h': df_mp['out.params.size_heating_system_primary_k_btu_h'],\n",
    "        'size_heating_system_secondary_k_btu_h': df_mp['out.params.size_heating_system_secondary_k_btu_h'],\n",
    "        'upgrade_hvac_heating_efficiency': df_mp['upgrade.hvac_heating_efficiency'],\n",
    "        'water_heater_efficiency': df_mp['in.water_heater_efficiency'],\n",
    "        'water_heater_fuel': df_mp['in.water_heater_fuel'],\n",
    "        'water_heater_in_unit': df_mp['in.water_heater_in_unit'],\n",
    "        'size_water_heater_gal': df_mp['out.params.size_water_heater_gal'],\n",
    "        'upgrade_water_heater_efficiency': df_mp['upgrade.water_heater_efficiency'],\n",
    "        'clothes_dryer_in_unit': df_mp['in.clothes_dryer'],\n",
    "        'upgrade_clothes_dryer': df_mp['upgrade.clothes_dryer'],\n",
    "        'cooking_range_in_unit': df_euss_am_mp7['in.cooking_range'],\n",
    "        'upgrade_cooking_range': df_euss_am_mp7['upgrade.cooking_range']\n",
    "    })\n",
    "    \n",
    "    categories = ['heating', 'waterHeating', 'clothesDrying', 'cooking']\n",
    "    for category in categories:\n",
    "        if category == 'heating':\n",
    "            # Heating Dataframe\n",
    "            # MP9 = MP8 (Electrification, High Efficiency) + MP1 (Basic Enclosure)\n",
    "            if input_mp == 'upgrade09':\n",
    "                menu_mp = 9\n",
    "                df_compare[f'mp{menu_mp}_heating_consumption'] = df_mp['out.electricity.heating.energy_consumption.kwh'].round(2)\n",
    "\n",
    "                # Measure Package 1: Basic Enclosure Package\n",
    "                # Attic floor insulation (upgrade.insulation_ceiling)\n",
    "                df_compare['base_insulation_atticFloor'] = df_mp['in.insulation_ceiling']\n",
    "                df_compare['upgrade_insulation_atticFloor'] = df_mp['upgrade.insulation_ceiling']\n",
    "                df_compare['out_params_floor_area_attic_ft_2'] = df_mp['out.params.floor_area_attic_ft_2']\n",
    "\n",
    "                # Air leakage reduction (upgrade.infiltration_reduction == '30%')\n",
    "                df_compare['upgrade_infiltration_reduction'] = df_mp['upgrade.infiltration_reduction']\n",
    "\n",
    "                # Duct sealing (upgrade.ducts == '10% Leakage, R-8')            \n",
    "                df_compare['base_ducts'] = df_mp['in.ducts']\n",
    "                df_compare['upgrade_duct_sealing'] = df_mp['upgrade.ducts']\n",
    "                df_compare['out_params_duct_unconditioned_surface_area_ft_2'] = df_mp['out.params.duct_unconditioned_surface_area_ft_2']\n",
    "\n",
    "                # Drill-and-fill wall insulation (upgrade.insulation_wall == 'Wood Stud, R-13')\n",
    "                df_compare['base_insulation_wall'] = df_mp['in.insulation_wall']\n",
    "                df_compare['upgrade_insulation_wall'] = df_mp['upgrade.insulation_wall']\n",
    "                df_compare['out_params_wall_area_above_grade_exterior_ft_2'] = df_mp['out.params.wall_area_above_grade_exterior_ft_2']\n",
    "\n",
    "            # MP8 = MP8 (Electrification, High Efficiency) + MP2 (Enhanced Enclosure)\n",
    "            elif input_mp == 'upgrade10':\n",
    "                menu_mp = 10\n",
    "                df_compare[f'mp{menu_mp}_heating_consumption'] = df_mp['out.electricity.heating.energy_consumption.kwh'].round(2)\n",
    "\n",
    "                # Measure Package 1: Basic Enclosure Package\n",
    "                # Attic floor insulation (upgrade.insulation_ceiling)\n",
    "                df_compare['base_insulation_atticFloor'] = df_mp['in.insulation_ceiling']\n",
    "                df_compare['upgrade_insulation_atticFloor'] = df_mp['upgrade.insulation_ceiling']\n",
    "                df_compare['out_params_floor_area_attic_ft_2'] = df_mp['out.params.floor_area_attic_ft_2']\n",
    "\n",
    "                # Air leakage reduction (upgrade.infiltration_reduction == '30%')\n",
    "                df_compare['upgrade_infiltration_reduction'] = df_mp['upgrade.infiltration_reduction']\n",
    "\n",
    "                # Duct sealing (upgrade.ducts == '10% Leakage, R-8')                        \n",
    "                df_compare['base_ducts'] = df_mp['in.ducts']\n",
    "                df_compare['upgrade_duct_sealing'] = df_mp['upgrade.ducts']\n",
    "                df_compare['out_params_duct_unconditioned_surface_area_ft_2'] = df_mp['out.params.duct_unconditioned_surface_area_ft_2']\n",
    "\n",
    "                # Drill-and-fill wall insulation (upgrade.insulation_wall == 'Wood Stud, R-13')\n",
    "                df_compare['base_insulation_wall'] = df_mp['in.insulation_wall']\n",
    "                df_compare['upgrade_insulation_wall'] = df_mp['upgrade.insulation_wall']\n",
    "                df_compare['out_params_wall_area_above_grade_exterior_ft_2'] = df_mp['out.params.wall_area_above_grade_exterior_ft_2']\n",
    "\n",
    "                # Measure Package 2: Enhanced Enclosure Package\n",
    "                # Foundation wall insulation and rim joist insulation\n",
    "                df_compare['base_foundation_type'] = df_mp['in.geometry_foundation_type']\n",
    "                df_compare['base_insulation_foundation_wall'] = df_mp['in.insulation_foundation_wall']\n",
    "                df_compare['base_insulation_rim_joist'] = df_mp['in.insulation_rim_joist']\n",
    "\n",
    "                # Only upgrade column for foundation wall insulation, but we will assume technical documentation and modeling consistent\n",
    "                df_compare['upgrade_insulation_foundation_wall'] = df_mp['upgrade.insulation_foundation_wall']\n",
    "                df_compare['out_params_floor_area_foundation_ft_2'] = df_mp['out.params.floor_area_foundation_ft_2']\n",
    "                df_compare['out_params_rim_joist_area_above_grade_exterior_ft_2'] = df_mp['out.params.rim_joist_area_above_grade_exterior_ft_2']                        \n",
    "\n",
    "                # Seal Vented Crawl Space\n",
    "                df_compare['upgrade_seal_crawlspace'] = df_mp['upgrade.geometry_foundation_type']\n",
    "\n",
    "                # Insulate finished attics and cathedral ceilings\n",
    "                df_compare['base_insulation_roof'] = df_mp['in.insulation_roof']\n",
    "                df_compare['upgrade_insulation_roof'] = df_mp['upgrade.insulation_roof']\n",
    "                df_compare['out_params_roof_area_ft_2'] = df_mp['out.params.roof_area_ft_2']\n",
    "            \n",
    "            else:\n",
    "                df_compare[f'mp{menu_mp}_heating_consumption'] = df_mp['out.electricity.heating.energy_consumption.kwh'].round(2)\n",
    "        # Water Heating Dataframe    \n",
    "        elif category == 'waterHeating':\n",
    "            df_compare[f'mp{menu_mp}_waterHeating_consumption'] = df_mp['out.electricity.hot_water.energy_consumption.kwh'].round(2)\n",
    "\n",
    "        # Clothes Drying Dataframe\n",
    "        elif category == 'clothesDrying':\n",
    "            df_compare[f'mp{menu_mp}_clothesDrying_consumption'] = df_mp['out.electricity.clothes_dryer.energy_consumption.kwh'].round(2)\n",
    "\n",
    "        # Cooking Dataframe\n",
    "        elif category == 'cooking':\n",
    "            df_compare[f'mp{menu_mp}_cooking_consumption'] = df_euss_am_mp7['out.electricity.range_oven.energy_consumption.kwh'].round(2)\n",
    "            \n",
    "    # Merge dataframes on bldg id column so everything is lined up\n",
    "    df_compare = pd.merge(df_baseline, df_compare, how='inner', on = 'bldg_id')\n",
    "    # calculate_consumption_reduction(df_compare, category)    \n",
    "        \n",
    "    return df_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc5d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def summarize_stats_table(df, category, data_columns, column_name_mapping, number_formatting, include_zero=True):\n",
    "#     \"\"\"\n",
    "#     Generate a formatted summary statistics table for specified columns in a DataFrame, grouped by 'base_fuel' and 'lowModerateIncome_designation'.\n",
    "\n",
    "#     Parameters:\n",
    "#     - df (DataFrame): The input DataFrame from which to compute statistics.\n",
    "#     - data_columns (list of str): The columns to include in the summary statistics.\n",
    "#     - column_name_mapping (dict): A dictionary to rename the columns in the summary statistics output.\n",
    "#     - number_formatting (str): The format string to use for numeric values in the output.\n",
    "#     - include_zero (bool, optional): Whether to include zero values in the statistics. Defaults to True.\n",
    "#       If False, zeros are replaced with NaN, which are then ignored in the computations.\n",
    "\n",
    "#     Returns:\n",
    "#     - DataFrame: A DataFrame containing the summary statistics, with formatted numeric values\n",
    "#       and renamed columns according to the input specifications, grouped by 'base_fuel' and 'lowModerateIncome_designation'.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Ensure 'lowModerateIncome_designation' is treated as a categorical variable with a specific order\n",
    "#     income_categories = ['Low-Income', 'Moderate-Income', 'Middle-to-Upper-Income']\n",
    "#     df['lowModerateIncome_designation'] = pd.Categorical(df['lowModerateIncome_designation'], categories=income_categories, ordered=True)\n",
    "\n",
    "#     # Filter out the 'Middle-to-Upper-Income' rows if needed (similar to your earlier function)\n",
    "#     df_filtered = df[df['lowModerateIncome_designation'] != 'Middle-to-Upper-Income']\n",
    "\n",
    "#     # Replace 0 values with NaN in the selected columns if include_zero is set to False\n",
    "#     if not include_zero:\n",
    "#         df_filtered[data_columns] = df_filtered[data_columns].replace(0, np.nan)\n",
    "\n",
    "#     # Group by 'base_fuel' and 'lowModerateIncome_designation' and calculate summary statistics\n",
    "#     summary_stats = df_filtered.groupby(by=[f'base_{category}_fuel', 'lowModerateIncome_designation'], observed=False)[data_columns].describe().unstack()\n",
    "\n",
    "#     # # Apply formatting to each number in these statistics according to the given format\n",
    "#     # summary_stats = summary_stats.applymap(lambda x: f\"{x:{number_formatting}}\" if pd.notnull(x) else \"\")\n",
    "\n",
    "#     # Rename the columns in the summary statistics DataFrame according to the provided mapping\n",
    "#     summary_stats.rename(columns=column_name_mapping, inplace=True)\n",
    "\n",
    "#     return summary_stats\n",
    "\n",
    "# # Example usage of the function:\n",
    "# # Assume 'df' is a DataFrame with relevant data and columns:\n",
    "# df_multiIndex_summary = summarize_stats_table(df_basic_summary_heating, category='heating', data_columns=['iraRef_heating_usd2023_per_mtCO2e'], \n",
    "#                                                column_name_mapping={'iraRef_heating_usd2023_per_mtCO2e': 'CO2 Abatement Cost (USD/mtCO2e)'},\n",
    "#                                                number_formatting=\".2f\", include_zero=True)\n",
    "# df_multiIndex_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58c1f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stats_table(df, data_columns, column_name_mapping, number_formatting, include_zero=True):\n",
    "    \"\"\"\n",
    "    Generate a formatted summary statistics table for specified columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame from which to compute statistics.\n",
    "    - data_columns (list of str): The columns to include in the summary statistics.\n",
    "    - column_name_mapping (dict): A dictionary to rename the columns in the summary statistics output.\n",
    "    - number_formatting (str): The format string to use for numeric values in the output.\n",
    "    - include_zero (bool, optional): Whether to include zero values in the statistics. Defaults to True.\n",
    "      If False, zeros are replaced with NaN, which are then ignored in the computations.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing the summary statistics, with formatted numeric values\n",
    "      and renamed columns according to the input specifications.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the DataFrame to avoid modifying the original data\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Replace 0 values with NaN in the selected columns if include_zero is set to False\n",
    "    if not include_zero:\n",
    "        df_copy[data_columns] = df_copy[data_columns].replace(0, np.nan)\n",
    "\n",
    "    # Compute summary statistics for the selected columns\n",
    "    # The 'describe' function returns summary statistics including count, mean, std, min, 25%, 50%, 75%, max\n",
    "    # Apply formatting to each number in these statistics according to the given format\n",
    "    summary_stats = df_copy[data_columns].describe().apply(lambda col: col.map(lambda x: f\"{x:{number_formatting}}\"))\n",
    "\n",
    "    # Rename the columns in the summary statistics DataFrame according to the provided mapping\n",
    "    summary_stats.rename(columns=column_name_mapping, inplace=True)\n",
    "\n",
    "    return summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e31f1fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LAST UPDATE AUGUST 21, 2024 at 10:54 PM\n",
    "# def calculate_public_npv(df, df_damages, menu_mp, policy_scenario, interest_rate=0.02):\n",
    "#     \"\"\"\n",
    "#     Calculate the public Net Present Value (NPV) for specific categories of damages,\n",
    "#     considering different policy scenarios related to grid decarbonization.\n",
    "\n",
    "#     Parameters:\n",
    "#     - df (DataFrame): A pandas DataFrame containing the relevant data.\n",
    "#     - menu_mp (str): Menu identifier used in column names.\n",
    "#     - policy_scenario (str): Policy policy_scenario that determines electricity grid projections. \n",
    "#                              Accepted values: 'AEO2023 Reference Case', 'High Uptake of Inflation Reduction Act'.\n",
    "#     - interest_rate (float): The discount rate used in the NPV calculation. Default is 2% for Social Discount Rate.\n",
    "\n",
    "#     Returns:\n",
    "#     - DataFrame: The input DataFrame with additional columns containing the calculated public NPVs for each category.\n",
    "#     \"\"\"\n",
    "#     equipment_specs = {\n",
    "#         'heating': 15,\n",
    "#         'waterHeating': 12,\n",
    "#         'clothesDrying': 13,\n",
    "#         'cooking': 15\n",
    "#     }\n",
    "    \n",
    "#     df_copy = df.copy()\n",
    "#     df_damages_copy = df_damages.copy()\n",
    "\n",
    "#     # Calculate the lifetime damages and corresponding NPV based on the policy policy_scenario\n",
    "#     df_new_columns = calculate_lifetime_damages_grid_scenario(df_copy, df_damages_copy, menu_mp, equipment_specs, policy_scenario, interest_rate)\n",
    "\n",
    "#     # Drop any overlapping columns from df_copy\n",
    "#     overlapping_columns = df_new_columns.columns.intersection(df_copy.columns)\n",
    "#     if not overlapping_columns.empty:\n",
    "#         df_copy.drop(columns=overlapping_columns, inplace=True)\n",
    "\n",
    "#     # Merge new columns into the original DataFrame\n",
    "#     df_copy = df_copy.join(df_new_columns, how='left')\n",
    "\n",
    "#     return df_copy\n",
    "\n",
    "# def calculate_lifetime_damages_grid_scenario(df_copy, df_damages_copy, menu_mp, equipment_specs, policy_scenario, interest_rate):\n",
    "#     \"\"\"\n",
    "#     Calculate the NPV of climate, health, and public damages over the equipment's lifetime\n",
    "#     under different grid decarbonization scenarios.\n",
    "\n",
    "#     Parameters:\n",
    "#     - df_copy (DataFrame): A copy of the original DataFrame to store NPV calculations.\n",
    "#     - menu_mp (str): Menu identifier used in column names.\n",
    "#     - equipment_specs (dict): Dictionary containing lifetimes for each equipment category.\n",
    "#     - policy_scenario (str): Specifies the grid policy_scenario ('No Inflation Reduction Act', 'AEO2023 Reference Case', 'High Uptake of Inflation Reduction Act').\n",
    "#     - interest_rate (float): Discount rate for NPV calculation.\n",
    "\n",
    "#     Returns:\n",
    "#     - DataFrame: A DataFrame containing the calculated NPV values for each category.\n",
    "#     \"\"\"\n",
    "#     # Determine the policy_scenario prefix based on the policy policy_scenario\n",
    "#     if policy_scenario == 'No Inflation Reduction Act':\n",
    "#         scenario_prefix = f\"preIRA_mp{menu_mp}_\"\n",
    "#     elif policy_scenario == 'AEO2023 Reference Case':\n",
    "#         scenario_prefix = f\"iraRef_mp{menu_mp}_\"\n",
    "#     elif policy_scenario == 'High Uptake of Inflation Reduction Act':\n",
    "#         scenario_prefix = f\"iraHigh_mp{menu_mp}_\"\n",
    "#     else:\n",
    "#         raise ValueError(\"Invalid Policy policy_scenario! Please choose from 'No Inflation Reduction Act', 'AEO2023 Reference Case', or 'High Uptake of Inflation Reduction Act'.\")\n",
    "    \n",
    "#     # Create a DataFrame to hold the NPV calculations\n",
    "#     npv_columns = {}\n",
    "    \n",
    "#     for category, lifetime in equipment_specs.items():\n",
    "#         print(f\"\"\"\\nCalculating Public NPV for {category}...\n",
    "#               lifetime: {lifetime}, interest_rate: {interest_rate}, policy_scenario: {policy_scenario}\"\"\")\n",
    "#         # Initialize NPV columns for each category\n",
    "#         climate_npv_key = f'{scenario_prefix}{category}_climate_npv'\n",
    "#         health_npv_key = f'{scenario_prefix}{category}_health_npv'\n",
    "#         public_npv_key = f'{scenario_prefix}{category}_public_npv'\n",
    "        \n",
    "#         # Initialize NPV columns in the dictionary if they don't exist\n",
    "#         npv_columns[climate_npv_key] = npv_columns.get(climate_npv_key, 0)\n",
    "#         npv_columns[health_npv_key] = npv_columns.get(health_npv_key, 0)\n",
    "#         npv_columns[public_npv_key] = npv_columns.get(public_npv_key, 0)\n",
    "            \n",
    "#         for year in range(1, lifetime + 1):\n",
    "#             year_label = year + 2021\n",
    "            \n",
    "#             base_climate = df_damages_copy[f'baseline_{year_label}_{category}_damages_climate']\n",
    "#             base_health = df_damages_copy[f'baseline_{year_label}_{category}_damages_health']\n",
    "            \n",
    "#             retrofit_climate = df_damages_copy[f'{scenario_prefix}{year_label}_{category}_damages_climate']\n",
    "#             retrofit_health = df_damages_copy[f'{scenario_prefix}{year_label}_{category}_damages_health']\n",
    "            \n",
    "#             base_damages = base_climate + base_health\n",
    "#             retrofit_damages = retrofit_climate + retrofit_health\n",
    "            \n",
    "#             # Apply the discount factor to each year's damages\n",
    "#             discount_factor = 1 / ((1 + interest_rate) ** year)\n",
    "                \n",
    "#             npv_columns[climate_npv_key] += ((base_climate - retrofit_climate) * discount_factor).round(2)\n",
    "#             npv_columns[health_npv_key] += ((base_health - retrofit_health) * discount_factor).round(2)\n",
    "#             npv_columns[public_npv_key] += ((base_damages - retrofit_damages) * discount_factor).round(2)\n",
    "    \n",
    "#     # Convert the dictionary to a DataFrame and return it\n",
    "#     npv_df = pd.DataFrame(npv_columns, index=df_copy.index)\n",
    "#     return npv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e8d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAST UPDATED SEPTEMBER 5, 2024 @ 9:27 PM\n",
    "def calculate_public_npv(df, df_damages, menu_mp, policy_scenario, interest_rate=0.02):\n",
    "    \"\"\"\n",
    "    Calculate the public Net Present Value (NPV) for specific categories of damages,\n",
    "    considering different policy scenarios related to grid decarbonization.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): A pandas DataFrame containing the relevant data.\n",
    "    - menu_mp (str): Menu identifier used in column names.\n",
    "    - policy_scenario (str): Policy policy_scenario that determines electricity grid projections. \n",
    "                             Accepted values: 'AEO2023 Reference Case'.\n",
    "    - interest_rate (float): The discount rate used in the NPV calculation. Default is 2% for Social Discount Rate.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: The input DataFrame with additional columns containing the calculated public NPVs for each enduse.\n",
    "    \"\"\"\n",
    "    equipment_specs = {\n",
    "        'heating': 15,\n",
    "        # 'waterHeating': 12,\n",
    "        # 'clothesDrying': 13,\n",
    "        # 'cooking': 15\n",
    "    }\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    df_damages_copy = df_damages.copy()\n",
    "\n",
    "    # Calculate the lifetime damages and corresponding NPV based on the policy policy_scenario\n",
    "    df_new_columns = calculate_lifetime_damages_grid_scenario(df_copy, df_damages_copy, menu_mp, equipment_specs, policy_scenario, interest_rate)\n",
    "\n",
    "    # Drop any overlapping columns from df_copy\n",
    "    overlapping_columns = df_new_columns.columns.intersection(df_copy.columns)\n",
    "    if not overlapping_columns.empty:\n",
    "        df_copy.drop(columns=overlapping_columns, inplace=True)\n",
    "\n",
    "    # Merge new columns into the original DataFrame\n",
    "    df_copy = df_copy.join(df_new_columns, how='left')\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "def calculate_lifetime_damages_grid_scenario(df_copy, df_damages_copy, menu_mp, equipment_specs, policy_scenario, interest_rate):\n",
    "    \"\"\"\n",
    "    Calculate the NPV of climate, health, and public damages over the equipment's lifetime\n",
    "    under different grid decarbonization scenarios.\n",
    "\n",
    "    Parameters:\n",
    "    - df_copy (DataFrame): A copy of the original DataFrame to store NPV calculations.\n",
    "    - menu_mp (str): Menu identifier used in column names.\n",
    "    - equipment_specs (dict): Dictionary containing lifetimes for each equipment category.\n",
    "    - policy_scenario (str): Specifies the grid policy_scenario ('No Inflation Reduction Act', 'AEO2023 Reference Case').\n",
    "    - interest_rate (float): Discount rate for NPV calculation.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing the calculated NPV values for each category.\n",
    "    \"\"\"\n",
    "    # Determine the policy_scenario prefix based on the policy policy_scenario\n",
    "    if policy_scenario == 'No Inflation Reduction Act':\n",
    "        scenario_prefix = f\"preIRA_mp{menu_mp}_\"\n",
    "    elif policy_scenario == 'AEO2023 Reference Case':\n",
    "        scenario_prefix = f\"iraRef_mp{menu_mp}_\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid Policy policy_scenario! Please choose from 'No Inflation Reduction Act' or 'AEO2023 Reference Case'.\")\n",
    "    \n",
    "    # Create a DataFrame to hold the NPV calculations\n",
    "    npv_columns = {}\n",
    "    \n",
    "    for category, lifetime in equipment_specs.items():\n",
    "        print(f\"\"\"\\nCalculating Public NPV for {category}...\n",
    "              lifetime: {lifetime}, interest_rate: {interest_rate}, policy_scenario: {policy_scenario}\"\"\")\n",
    "        \n",
    "        # Initialize NPV columns for each category\n",
    "        public_npv_key = f'{scenario_prefix}{category}_public_npv'\n",
    "        \n",
    "        # Initialize NPV columns in the dictionary if they don't exist\n",
    "        npv_columns[public_npv_key] = npv_columns.get(public_npv_key, 0)\n",
    "            \n",
    "        for year in range(1, lifetime + 1):\n",
    "            year_label = year + 2023\n",
    "            \n",
    "            base_climate_damages = df_damages_copy[f'baseline_{year_label}_{category}_damages_climate']\n",
    "            \n",
    "            retrofit_climate_damages = df_damages_copy[f'{scenario_prefix}{year_label}_{category}_damages_climate']\n",
    "            \n",
    "            # Apply the discount factor to each year's damages\n",
    "            discount_factor = 1 / ((1 + interest_rate) ** year)\n",
    "                \n",
    "            npv_columns[public_npv_key] += ((base_climate_damages - retrofit_climate_damages) * discount_factor).round(2)\n",
    "    \n",
    "    # Convert the dictionary to a DataFrame and return it\n",
    "    npv_df = pd.DataFrame(npv_columns, index=df_copy.index)\n",
    "    return npv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2549f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CCI to adjust for cost differences when compared to the national average\n",
    "# Function to map city to its average cost\n",
    "def map_average_cost(city):\n",
    "    if city in average_cost_map:\n",
    "        return average_cost_map[city]\n",
    "    elif city == 'Not in a census Place' or city == 'In another census Place':\n",
    "        return average_cost_map.get('+30 City Average')\n",
    "    else:\n",
    "        return average_cost_map.get('+30 City Average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f3b2993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_heating_system_specs(df):\n",
    "    # Check if necessary columns are in the DataFrame\n",
    "    necessary_columns = ['size_heating_system_primary_k_btu_h', 'size_heat_pump_backup_primary_k_btu_h',\n",
    "                         'size_heating_system_secondary_k_btu_h', 'baseline_heating_type']\n",
    "    if not all(column in df.columns for column in necessary_columns):\n",
    "        raise ValueError(\"DataFrame does not contain all necessary columns.\")\n",
    "\n",
    "    # Total heating load in kBtuh\n",
    "    df['total_heating_load_kBtuh'] = df['size_heating_system_primary_k_btu_h'] + df['size_heat_pump_backup_primary_k_btu_h'] + df['size_heating_system_secondary_k_btu_h']\n",
    "    \n",
    "#     # Total heating load in kW\n",
    "#     df['total_heating_load_kW'] = df['total_heating_load_kBtuh'] * 1000 / 3412.142\n",
    "   \n",
    "    # Use regex to remove the fuel and leave only the heating type:\n",
    "    df['baseline_heating_type'] = df['baseline_heating_type'].str.extract(r'^(?:\\d+\\s+)?(?:Natural Gas|Electricity|Propane|Fuel Oil|Fuel)\\s+(?:Fuel\\s+)?(?:Electric\\s+)?(.+)$')\n",
    "    \n",
    "    # AFUE extraction for existing, baseline equipment (Replacement Costs)\n",
    "    df['baseline_AFUE'] = df['hvac_heating_efficiency'].str.extract(r'([\\d.]+)%').astype(float)\n",
    "    \n",
    "    # SEER extraction for existing, baseline equipment (Replacement Costs)\n",
    "    df['baseline_SEER'] = df['hvac_heating_efficiency'].str.extract(r'SEER ([\\d.]+)').astype(float)\n",
    "    \n",
    "    # HSPF extraction for existing, baseline equipment (Replacement Costs)\n",
    "    df['baseline_HSPF'] = df['hvac_heating_efficiency'].str.extract(r'([\\d.]+) HSPF').astype(float)\n",
    "\n",
    "    # HSPF extraction for upgraded equipment (New Install Costs)\n",
    "    # df['ugrade_newInstall_HSPF'] = df['upgrade_hvac_heating_efficiency'].str.extract(r'(\\d+\\.\\d+)')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39eff356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_heating_installation_premium(df, menu_mp, rsMeans_national_avg, cpi_ratio_2023_2013):\n",
    "    necessary_columns = ['hvac_cooling_type', 'heating_type', 'rsMeans_CCI_avg']\n",
    "    if not all(column in df.columns for column in necessary_columns):\n",
    "        raise ValueError(\"DataFrame does not contain all necessary columns.\")\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # Initialization to zero\n",
    "        premium_cost = 0\n",
    "        \n",
    "        # Installation cost for homes with existing AC\n",
    "        # Deetjen: Replace SEER 15, 8.5 HSPF ASHP with SEER 15, 8.5 HSPF ASHP: NREL REMDB 50th Percentile Cost is $3300 USD-2013        \n",
    "        if row['hvac_cooling_type'] != 'None':\n",
    "            premium_cost = 0\n",
    "        \n",
    "        # Installation cost for homes without central AC, but an existing furnace or baseboard\n",
    "        # Deetjen: Install SEER 15, 8.5 HSPF ASHP: NREL REMDB 50th Percentile Cost is $3700 USD-2013        \n",
    "        elif 'Furnace' in row['heating_type'] or 'Baseboard' in row['heating_type']:\n",
    "            premium_cost = 400 * cpi_ratio_2023_2013\n",
    "        \n",
    "        # Installation cost for homes without central AC and an existing boiler as heating system\n",
    "        # Deetjen: Install SEER 15, 8.5 HSPF ASHP: NREL REMDB High Cost is $4800 USD-2013        \n",
    "        elif 'Boiler' in row['heating_type']:\n",
    "            premium_cost = 1500 * cpi_ratio_2023_2013\n",
    "        \n",
    "        # Apply CPI adjustment above and regional cost index adjustment below\n",
    "        adjusted_cost = round(premium_cost * (row['rsMeans_CCI_avg'] / rsMeans_national_avg), 2)\n",
    "        df.at[index, f'mp{menu_mp}_heating_installation_premium'] = adjusted_cost\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cec5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_county_medianIncome(project_root, cpi_ratio_2023_2022):\n",
    "    # Collect Area Median Income Data at county-resolution\n",
    "    filename = \"nhgis0005_ds261_2022_county.csv\"\n",
    "    relative_path = os.path.join(r\"equity_data\", filename)\n",
    "    file_path = os.path.join(project_root, relative_path)\n",
    "\n",
    "    print(f\"Retrieved data for filename: {filename}\")\n",
    "    print(f\"Located at filepath: {file_path}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    df_county_medianIncome = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    # df_county_medianIncome = df_county_medianIncome.drop(0)\n",
    "    df_county_medianIncome = df_county_medianIncome.reset_index(drop=True)\n",
    "\n",
    "    cols_interest = ['GISJOIN', 'STUSAB', 'COUNTYA', 'NAME_E', 'AP2PE001', 'AP2PM001']\n",
    "    df_county_medianIncome = df_county_medianIncome[cols_interest]\n",
    "    df_county_medianIncome = df_county_medianIncome.rename(columns={\"GISJOIN\": \"gis_joinID_county\", \"STUSAB\": \"state_abbrev\", \"COUNTYA\": \"county_code\", \"NAME_E\": \"name_estimate\", \"AP2PE001\": \"median_income_USD2022\", \"AP2PM001\": \"median_income_USD2022_marginOfError\"})\n",
    "    df_county_medianIncome['median_income_USD2023'] = round((df_county_medianIncome['median_income_USD2022'] * cpi_ratio_2023_2022), 2)\n",
    "    return df_county_medianIncome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e18bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATED AUGUST 22, 2024 @ 9:40 PM (~ENSURE COLS UPDATE WHEN FUNCTION RE-RUN. DROP OLD OVERLAPPING COLS~)\n",
    "# Replacement Cost Function and Helper Functions (Parametes, Formula)\n",
    "\n",
    "# Helper function to get parameters based on end use\n",
    "def get_end_use_replacement_parameters(df, end_use):\n",
    "    parameters = {\n",
    "        'heating': {\n",
    "            'conditions': [\n",
    "                (df['base_heating_fuel'] == 'Propane'),\n",
    "                (df['base_heating_fuel'] == 'Fuel Oil'),\n",
    "                (df['base_heating_fuel'] == 'Natural Gas'),\n",
    "                (df['base_heating_fuel'] == 'Electricity') & (df['heating_type'] == 'Electricity ASHP'),\n",
    "                (df['base_heating_fuel'] == 'Electricity')\n",
    "            ],\n",
    "            'tech_eff_pairs': [\n",
    "                ('Propane Furnace', '94 AFUE'),\n",
    "                ('Fuel Oil Furnace', '95 AFUE'),\n",
    "                ('Natural Gas Furnace', '95 AFUE'),\n",
    "                ('Electric ASHP', 'SEER 18, 9.3 HSPF'),\n",
    "                ('Electric Furnace', '100 AFUE')\n",
    "            ],\n",
    "            'cost_components': ['unitCost', 'otherCost', 'cost_per_kBtuh']\n",
    "        },\n",
    "        # 'waterHeating': {\n",
    "        #     'conditions': [\n",
    "        #         (df['base_waterHeating_fuel'] == 'Fuel Oil'),\n",
    "        #         (df['base_waterHeating_fuel'] == 'Natural Gas'),\n",
    "        #         (df['base_waterHeating_fuel'] == 'Propane'),\n",
    "        #         (df['water_heater_efficiency'].isin(['Electric Standard', 'Electric Premium'])),\n",
    "        #         (df['water_heater_efficiency'] == 'Electric Heat Pump, 80 gal')\n",
    "        #     ],\n",
    "        #     'tech_eff_pairs': [\n",
    "        #         ('Fuel Oil Water Heater', 0.68),\n",
    "        #         ('Natural Gas Water Heater', 0.67),\n",
    "        #         ('Propane Water Heater', 0.67),\n",
    "        #         ('Electric Water Heater', 0.95),\n",
    "        #         ('Electric Heat Pump Water Heater, 80 gal', 2.35)\n",
    "        #     ],\n",
    "        #     'cost_components': ['unitCost', 'cost_per_gallon']\n",
    "        # },\n",
    "        # 'clothesDrying': {\n",
    "        #     'conditions': [\n",
    "        #         (df['base_clothesDrying_fuel'] == 'Electricity'),\n",
    "        #         (df['base_clothesDrying_fuel'] == 'Natural Gas'),\n",
    "        #         (df['base_clothesDrying_fuel'] == 'Propane')\n",
    "        #     ],\n",
    "        #     'tech_eff_pairs': [\n",
    "        #         ('Electric Clothes Dryer', 3.1),\n",
    "        #         ('Natural Gas Clothes Dryer', 2.75),\n",
    "        #         ('Propane Clothes Dryer', 2.75)\n",
    "        #     ],\n",
    "        #     'cost_components': ['unitCost']\n",
    "        # },\n",
    "        # 'cooking': {\n",
    "        #     'conditions': [\n",
    "        #         (df['base_cooking_fuel'] == 'Electricity'),\n",
    "        #         (df['base_cooking_fuel'] == 'Natural Gas'),\n",
    "        #         (df['base_cooking_fuel'] == 'Propane')\n",
    "        #     ],\n",
    "        #     'tech_eff_pairs': [\n",
    "        #         ('Electric Range', 0.74),\n",
    "        #         ('Natural Gas Range', 0.4),\n",
    "        #         ('Propane Range', 0.4)\n",
    "        #     ],\n",
    "        #     'cost_components': ['unitCost']\n",
    "        # }\n",
    "    }\n",
    "    if end_use not in parameters:\n",
    "        raise ValueError(f\"Invalid end_use specified: {end_use}\")\n",
    "    return parameters[end_use]\n",
    "\n",
    "# UPDATED AUGUST 22, 2024 @ 9:40 PM (~ENSURE COLS UPDATE WHEN FUNCTION RE-RUN. DROP OLD OVERLAPPING COLS~)\n",
    "def calculate_replacement_cost_per_row(df_valid, sampled_costs_dict, rsMeans_national_avg, menu_mp, end_use):\n",
    "    \"\"\"\n",
    "    Helper function to calculate the replacement cost for each row based on the end use.\n",
    "\n",
    "    Parameters:\n",
    "    df_valid (pd.DataFrame): Filtered DataFrame containing valid rows.\n",
    "    sampled_costs_dict (dict): Dictionary with sampled costs for each component.\n",
    "    rsMeans_national_avg (float): National average value for cost adjustment.\n",
    "    menu_mp (int): Menu option identifier.\n",
    "    end_use (str): Type of end-use to calculate replacement cost for ('heating', 'waterHeating', 'clothesDrying', 'cooking').\n",
    "\n",
    "    Returns:\n",
    "    tuple: Tuple containing the calculated replacement costs and the cost column name.\n",
    "    \"\"\"\n",
    "    if end_use == 'heating':\n",
    "        replacement_cost = (\n",
    "            sampled_costs_dict['unitCost'] +\n",
    "            sampled_costs_dict['otherCost'] +\n",
    "            (df_valid['total_heating_load_kBtuh'] * sampled_costs_dict['cost_per_kBtuh'])\n",
    "        ) * (df_valid['rsMeans_CCI_avg'] / rsMeans_national_avg)\n",
    "        cost_column_name = f'mp{menu_mp}_heating_replacementCost'\n",
    "    elif end_use == 'waterHeating':\n",
    "        replacement_cost = (\n",
    "            sampled_costs_dict['unitCost'] +\n",
    "            (sampled_costs_dict['cost_per_gallon'] * df_valid['size_water_heater_gal'])\n",
    "        ) * (df_valid['rsMeans_CCI_avg'] / rsMeans_national_avg)\n",
    "        cost_column_name = f'mp{menu_mp}_waterHeating_replacementCost'\n",
    "    else:\n",
    "        replacement_cost = sampled_costs_dict['unitCost'] * (df_valid['rsMeans_CCI_avg'] / rsMeans_national_avg)\n",
    "        cost_column_name = f'mp{menu_mp}_{end_use}_replacementCost'\n",
    "    \n",
    "    return replacement_cost, cost_column_name\n",
    "\n",
    "# UPDATED AUGUST 22, 2024 @ 9:40 PM (~ENSURE COLS UPDATE WHEN FUNCTION RE-RUN. DROP OLD OVERLAPPING COLS~)\n",
    "def calculate_replacement_cost(df, cost_dict, rsMeans_national_avg, menu_mp, end_use):\n",
    "    \"\"\"\n",
    "    General function to calculate replacement costs for various end-uses based on fuel types, costs, and efficiency.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing data for different scenarios.\n",
    "    cost_dict (dict): Dictionary with cost information for different technology and efficiency combinations.\n",
    "    rsMeans_national_avg (float): National average value for cost adjustment.\n",
    "    menu_mp (int): Menu option identifier.\n",
    "    end_use (str): Type of end-use to calculate replacement cost for ('heating', 'waterHeating', 'clothesDrying', 'cooking').\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Updated DataFrame with calculated replacement costs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate menu_mp\n",
    "    valid_menu_mps = [7, 8, 9, 10]\n",
    "    if menu_mp not in valid_menu_mps:\n",
    "        raise ValueError(\"Please enter a valid measure package number for menu_mp. Should be 7, 8, 9, or 10.\")\n",
    "    \n",
    "    # Get conditions, technology-efficiency pairs, and cost components for the specified end_use\n",
    "    params = get_end_use_replacement_parameters(df, end_use)\n",
    "    conditions = params['conditions']\n",
    "    tech_eff_pairs = params['tech_eff_pairs']\n",
    "    cost_components = params['cost_components']\n",
    "   \n",
    "    # Map each condition to its tech and efficiency\n",
    "    tech = np.select(conditions, [pair[0] for pair in tech_eff_pairs], default='unknown')\n",
    "    eff = np.select(conditions, [pair[1] for pair in tech_eff_pairs], default=np.nan)\n",
    "\n",
    "    # Convert efficiency values to appropriate types\n",
    "    if end_use == 'heating':\n",
    "        eff = np.array([str(e) if e != 'unknown' else np.nan for e in eff])\n",
    "    else:\n",
    "        eff = np.array([float(e) if e != 'unknown' else np.nan for e in eff])\n",
    "\n",
    "    # Filter out rows with unknown technology and NaN efficiency\n",
    "    valid_indices = tech != 'unknown'\n",
    "    tech = tech[valid_indices]\n",
    "    eff = eff[valid_indices]\n",
    "    df_valid = df.loc[valid_indices].copy()\n",
    "\n",
    "    # Initialize dictionaries to store sampled costs\n",
    "    sampled_costs_dict = {}\n",
    "\n",
    "    # Calculate costs for each component\n",
    "    for cost_component in cost_components:\n",
    "        progressive_costs = np.array([cost_dict.get((t, e), {}).get(f'{cost_component}_progressive', np.nan) for t, e in zip(tech, eff)])\n",
    "        reference_costs = np.array([cost_dict.get((t, e), {}).get(f'{cost_component}_reference', np.nan) for t, e in zip(tech, eff)])\n",
    "        conservative_costs = np.array([cost_dict.get((t, e), {}).get(f'{cost_component}_conservative', np.nan) for t, e in zip(tech, eff)])\n",
    "\n",
    "        # Handle missing cost data\n",
    "        if np.isnan(progressive_costs).any() or np.isnan(reference_costs).any() or np.isnan(conservative_costs).any():\n",
    "            missing_indices = np.where(np.isnan(progressive_costs) | np.isnan(reference_costs) | np.isnan(conservative_costs))\n",
    "            print(f\"Missing data at indices: {missing_indices}\")\n",
    "            print(f\"Tech with missing data: {tech[missing_indices]}\")\n",
    "            print(f\"Efficiencies with missing data: {eff[missing_indices]}\")\n",
    "            \n",
    "            raise ValueError(f\"Missing cost data for some technology and efficiency combinations in cost_component {cost_component}\")\n",
    "\n",
    "        # Calculate mean and standard deviation assuming the costs represent the 10th, 50th, and 90th percentiles of a normal distribution\n",
    "        mean_costs = reference_costs\n",
    "        std_costs = (conservative_costs - progressive_costs) / (norm.ppf(0.90) - norm.ppf(0.10))\n",
    "\n",
    "        # Sample from the normal distribution for each row\n",
    "        sampled_costs = np.random.normal(loc=mean_costs, scale=std_costs)\n",
    "        sampled_costs_dict[cost_component] = sampled_costs\n",
    "\n",
    "    # Calculate the replacement cost for each row\n",
    "    replacement_cost, cost_column_name = calculate_replacement_cost_per_row(df_valid, sampled_costs_dict, rsMeans_national_avg, menu_mp, end_use)\n",
    "\n",
    "    # Add the calculated costs to a new DataFrame, rounded to 2 decimal places\n",
    "    df_new_columns = pd.DataFrame({cost_column_name: np.round(replacement_cost, 2)}, index=df_valid.index)\n",
    "\n",
    "    # Identify overlapping columns between the new and existing DataFrame\n",
    "    overlapping_columns = df_new_columns.columns.intersection(df.columns)\n",
    "\n",
    "    # Drop overlapping columns from the original DataFrame\n",
    "    if not overlapping_columns.empty:\n",
    "        df.drop(columns=overlapping_columns, inplace=True)\n",
    "\n",
    "    # Merge new columns into the original DataFrame, ensuring no duplicates or overwrites occur\n",
    "    df = df.join(df_new_columns, how='left')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26e83772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATED AUGUST 22, 2024 @ 9:30 PM (~ENSURE COLS UPDATE WHEN FUNCTION RE-RUN. DROP OLD OVERLAPPING COLS~)\n",
    "\n",
    "# Installation Cost Function and Helper Functions (Parametes, Formula)\n",
    "# Helper function to get parameters based on end use\n",
    "def get_end_use_installation_parameters(df, end_use, menu_mp):\n",
    "    parameters = {\n",
    "        'heating': {\n",
    "            'conditions': [\n",
    "                (df['hvac_has_ducts'] == 'Yes') & (menu_mp == 7),\n",
    "                (df['hvac_has_ducts'] == 'No') & (menu_mp == 7),\n",
    "                (df['hvac_has_ducts'] == 'Yes') & (menu_mp != 7),\n",
    "                (df['hvac_has_ducts'] == 'No') & (menu_mp != 7)\n",
    "            ],\n",
    "            'tech_eff_pairs': [\n",
    "                ('Electric ASHP', 'SEER 18, 9.3 HSPF'),\n",
    "                ('Electric MSHP', 'SEER 18, 9.6 HSPF'),\n",
    "                ('Electric MSHP - Ducted', 'SEER 15.5, 10 HSPF'),\n",
    "                ('Electric MSHP', 'SEER 29.3, 14 HSPF')\n",
    "            ],\n",
    "            'cost_components': ['unitCost', 'otherCost', 'cost_per_kBtuh']\n",
    "        },\n",
    "        # 'waterHeating': {\n",
    "        #     'conditions': [\n",
    "        #         (df['upgrade_water_heater_efficiency'] == 'Electric Heat Pump, 50 gal, 3.45 UEF'),\n",
    "        #         (df['upgrade_water_heater_efficiency'] == 'Electric Heat Pump, 66 gal, 3.35 UEF'),\n",
    "        #         (df['upgrade_water_heater_efficiency'] == 'Electric Heat Pump, 80 gal, 3.45 UEF')\n",
    "        #     ],\n",
    "        #     'tech_eff_pairs': [\n",
    "        #         ('Electric Heat Pump Water Heater, 50 gal', 3.45),\n",
    "        #         ('Electric Heat Pump Water Heater, 66 gal', 3.35),\n",
    "        #         ('Electric Heat Pump Water Heater, 80 gal', 3.45),\n",
    "        #     ],\n",
    "        #     'cost_components': ['unitCost', 'cost_per_gallon']\n",
    "        # },\n",
    "        # 'clothesDrying': {\n",
    "        #     'conditions': [\n",
    "        #         df['upgrade_clothes_dryer'].str.contains('Electric, Premium, Heat Pump, Ventless', na=False),\n",
    "        #         ~df['upgrade_clothes_dryer'].str.contains('Electric, Premium, Heat Pump, Ventless', na=False),\n",
    "        #     ],\n",
    "        #     'tech_eff_pairs': [\n",
    "        #         ('Electric HP Clothes Dryer', 5.2),\n",
    "        #         ('Electric Clothes Dryer', 3.1),\n",
    "        #     ],\n",
    "        #     'cost_components': ['unitCost']\n",
    "        # },\n",
    "        # 'cooking': {\n",
    "        #     'conditions': [\n",
    "        #         df['upgrade_cooking_range'].str.contains('Electric, Induction', na=False),\n",
    "        #         ~df['upgrade_cooking_range'].str.contains('Electric, Induction', na=False),\n",
    "        #     ],\n",
    "        #     'tech_eff_pairs': [\n",
    "        #         ('Electric Induction Range', 0.84),\n",
    "        #         ('Electric Range, Modern', 0.74),\n",
    "        #     ],\n",
    "        #     'cost_components': ['unitCost']\n",
    "        # }\n",
    "    }\n",
    "    if end_use not in parameters:\n",
    "        raise ValueError(f\"Invalid end_use specified: {end_use}\")\n",
    "    return parameters[end_use]\n",
    "\n",
    "# UPDATED AUGUST 22, 2024 @ 9:30 PM (~ENSURE COLS UPDATE WHEN FUNCTION RE-RUN. DROP OLD OVERLAPPING COLS~)\n",
    "def calculate_installation_cost_per_row(df_valid, sampled_costs_dict, rsMeans_national_avg, menu_mp, end_use):\n",
    "    \"\"\"\n",
    "    Helper function to calculate the installation cost for each row based on the end use.\n",
    "\n",
    "    Parameters:\n",
    "    df_valid (pd.DataFrame): Filtered DataFrame containing valid rows.\n",
    "    sampled_costs_dict (dict): Dictionary with sampled costs for each component.\n",
    "    rsMeans_national_avg (float): National average value for cost adjustment.\n",
    "    menu_mp (int): Menu option identifier.\n",
    "    end_use (str): Type of end-use to calculate installation cost for ('heating', 'waterHeating', 'clothesDrying', 'cooking').\n",
    "\n",
    "    Returns:\n",
    "    tuple: Tuple containing the calculated installation costs and the cost column name.\n",
    "    \"\"\"\n",
    "    if end_use == 'heating':\n",
    "        installation_cost = (\n",
    "            sampled_costs_dict['unitCost'] +\n",
    "            sampled_costs_dict['otherCost'] +\n",
    "            (df_valid['total_heating_load_kBtuh'] * sampled_costs_dict['cost_per_kBtuh'])\n",
    "        ) * (df_valid['rsMeans_CCI_avg'] / rsMeans_national_avg)\n",
    "        cost_column_name = f'mp{menu_mp}_heating_installationCost'\n",
    "    elif end_use == 'waterHeating':\n",
    "        installation_cost = (\n",
    "            sampled_costs_dict['unitCost'] +\n",
    "            (sampled_costs_dict['cost_per_gallon'] * df_valid['size_water_heater_gal'])\n",
    "        ) * (df_valid['rsMeans_CCI_avg'] / rsMeans_national_avg)\n",
    "        cost_column_name = f'mp{menu_mp}_waterHeating_installationCost'\n",
    "    else:\n",
    "        installation_cost = sampled_costs_dict['unitCost'] * (df_valid['rsMeans_CCI_avg'] / rsMeans_national_avg)\n",
    "        cost_column_name = f'mp{menu_mp}_{end_use}_installationCost'\n",
    "    \n",
    "    return installation_cost, cost_column_name\n",
    "\n",
    "# UPDATED AUGUST 22, 2024 @ 9:30 PM (~ENSURE COLS UPDATE WHEN FUNCTION RE-RUN. DROP OLD OVERLAPPING COLS~)\n",
    "def calculate_installation_cost(df, cost_dict, rsMeans_national_avg, menu_mp, end_use):\n",
    "    \"\"\"\n",
    "    General function to calculate installation costs for various end-uses based on fuel types, costs, and efficiency.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing data for different scenarios.\n",
    "    cost_dict (dict): Dictionary with cost information for different technology and efficiency combinations.\n",
    "    rsMeans_national_avg (float): National average value for cost adjustment.\n",
    "    menu_mp (int): Menu option identifier.\n",
    "    end_use (str): Type of end-use to calculate installation cost for ('heating', 'waterHeating', 'clothesDrying', 'cooking').\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Updated DataFrame with calculated installation costs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate menu_mp \n",
    "    valid_menu_mps = [7, 8, 9, 10]\n",
    "    if menu_mp not in valid_menu_mps:\n",
    "        raise ValueError(\"Please enter a valid measure package number for menu_mp. Should be 7, 8, 9, or 10.\")\n",
    "    \n",
    "    # Get conditions, technology-efficiency pairs, and cost components for the specified end_use\n",
    "    params = get_end_use_installation_parameters(df, end_use, menu_mp)\n",
    "    conditions = params['conditions']\n",
    "    tech_eff_pairs = params['tech_eff_pairs']\n",
    "    cost_components = params['cost_components']\n",
    "   \n",
    "    # Map each condition to its tech and efficiency\n",
    "    tech = np.select(conditions, [pair[0] for pair in tech_eff_pairs], default='unknown')\n",
    "    eff = np.select(conditions, [pair[1] for pair in tech_eff_pairs], default=np.nan)\n",
    "\n",
    "    # Convert efficiency values to appropriate types\n",
    "    if end_use == 'heating':\n",
    "        eff = np.array([str(e) if e != 'unknown' else np.nan for e in eff])\n",
    "    else:\n",
    "        eff = np.array([float(e) if e != 'unknown' else np.nan for e in eff])\n",
    "\n",
    "    # Filter out rows with unknown technology and NaN efficiency\n",
    "    valid_indices = tech != 'unknown'\n",
    "    tech = tech[valid_indices]\n",
    "    eff = eff[valid_indices]\n",
    "    df_valid = df.loc[valid_indices].copy()\n",
    "\n",
    "    # Initialize dictionaries to store sampled costs\n",
    "    sampled_costs_dict = {}\n",
    "\n",
    "    # Calculate costs for each component\n",
    "    for cost_component in cost_components:\n",
    "        progressive_costs = np.array([cost_dict.get((t, e), {}).get(f'{cost_component}_progressive', np.nan) for t, e in zip(tech, eff)])\n",
    "        reference_costs = np.array([cost_dict.get((t, e), {}).get(f'{cost_component}_reference', np.nan) for t, e in zip(tech, eff)])\n",
    "        conservative_costs = np.array([cost_dict.get((t, e), {}).get(f'{cost_component}_conservative', np.nan) for t, e in zip(tech, eff)])\n",
    "        \n",
    "        print(f\"progressive_costs is {progressive_costs}\")\n",
    "        \n",
    "        print(f\"reference_costs is {reference_costs}\")\n",
    "        \n",
    "        print(f\"conservative_costs is {conservative_costs}\")\n",
    "\n",
    "        # Handle missing cost data\n",
    "        if np.isnan(progressive_costs).any() or np.isnan(reference_costs).any() or np.isnan(conservative_costs).any():\n",
    "            missing_indices = np.where(np.isnan(progressive_costs) | np.isnan(reference_costs) | np.isnan(conservative_costs))\n",
    "            print(f\"Missing data at indices: {missing_indices}\")\n",
    "            print(f\"Tech with missing data: {tech[missing_indices]}\")\n",
    "            print(f\"Efficiencies with missing data: {eff[missing_indices]}\")\n",
    "            \n",
    "            raise ValueError(f\"Missing cost data for some technology and efficiency combinations in cost_component {cost_component}\")\n",
    "\n",
    "        # Calculate mean and standard deviation assuming the costs represent the 10th, 50th, and 90th percentiles of a normal distribution\n",
    "        mean_costs = reference_costs\n",
    "        std_costs = (conservative_costs - progressive_costs) / (norm.ppf(0.90) - norm.ppf(0.10))\n",
    "\n",
    "        # Sample from the normal distribution for each row\n",
    "        sampled_costs = np.random.normal(loc=mean_costs, scale=std_costs)\n",
    "        sampled_costs_dict[cost_component] = sampled_costs\n",
    "\n",
    "    # Calculate the installation cost for each row\n",
    "    installation_cost, cost_column_name = calculate_installation_cost_per_row(df_valid, sampled_costs_dict, rsMeans_national_avg, menu_mp, end_use)\n",
    "\n",
    "    # Add the calculated costs to a new DataFrame, rounded to 2 decimal places\n",
    "    df_new_columns = pd.DataFrame({cost_column_name: np.round(installation_cost, 2)}, index=df_valid.index)\n",
    "\n",
    "    # Identify overlapping columns between the new and existing DataFrame\n",
    "    overlapping_columns = df_new_columns.columns.intersection(df.columns)\n",
    "\n",
    "    # Drop overlapping columns from the original DataFrame\n",
    "    if not overlapping_columns.empty:\n",
    "        df.drop(columns=overlapping_columns, inplace=True)\n",
    "\n",
    "    # Merge new columns into the original DataFrame, ensuring no duplicates or overwrites occur\n",
    "    df = df.join(df_new_columns, how='left')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea126288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAST UPDATE AUGUST 21, 2024 @ 11:40 PM\n",
    "\n",
    "# POTENTIALLY UPDATE CODE IN THE FUTURE TO ACCOUNT FOR CHANGES IN CAPITAL COSTS BASED ON SCENARIOS (BESIDES IRA REBATES)\n",
    "# Note: CURRENT MODELING ASSUMES EQUIPMENT PRICES ARE THE SAME UNDER IRA REF AND IRA HIGH\n",
    "# THIS MAY BE UPDATED IN THE FUTURE, SO WE STILL USE policy_scenario PREFIXES FOR TOTAL AND NET CAPITAL COSTS\n",
    "# COSTS ARE DIFFERENT FOR PRE-IRA BECAUSE NO REBATES ARE APPLIED\n",
    "def calculate_private_NPV(df, df_fuelCosts, interest_rate, input_mp, menu_mp, policy_scenario):\n",
    "    \"\"\"\n",
    "    Calculate the private net present value (NPV) for various equipment categories,\n",
    "    considering different cost assumptions and potential IRA rebates. The function adjusts\n",
    "    equipment costs for inflation and regional cost differences, and calculates NPV based\n",
    "    on cost savings between baseline and retrofit scenarios.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): Input DataFrame with installation costs, fuel savings, and potential rebates.\n",
    "        interest_rate (float): Annual discount rate used for NPV calculation.\n",
    "        menu_mp (str): Prefix for columns in the DataFrame.\n",
    "        input_mp (str): Input policy_scenario for calculating costs.\n",
    "        policy_scenario (str): Policy policy_scenario that determines electricity grid projections. \n",
    "                               Accepted values: 'AEO2023 Reference Case'.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The input DataFrame updated with calculated private NPV and adjusted equipment costs.\n",
    "    \"\"\"\n",
    "    # POTENTIALLY UPDATE CODE IN THE FUTURE TO ACCOUNT FOR CHANGES IN CAPITAL COSTS BASED ON SCENARIOS (BESIDES IRA REBATES)\n",
    "    # Note: CURRENT MODELING ASSUMES EQUIPMENT PRICES ARE THE SAME UNDER IRA REF AND IRA HIGH\n",
    "    # THIS MAY BE UPDATED IN THE FUTURE, SO WE STILL USE policy_scenario PREFIXES FOR TOTAL AND NET CAPITAL COSTS\n",
    "    # COSTS ARE DIFFERENT FOR PRE-IRA BECAUSE NO REBATES ARE APPLIED   \n",
    "    equipment_specs = {\n",
    "        'heating': 15,\n",
    "        # 'waterHeating': 12,\n",
    "        # 'clothesDrying': 13,\n",
    "        # 'cooking': 15\n",
    "    }\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "\n",
    "    df_fuelCosts_copy = df_fuelCosts.copy()\n",
    "\n",
    "    df_new_columns = pd.DataFrame(index=df_copy.index)\n",
    "    \n",
    "    for category, lifetime in equipment_specs.items():\n",
    "        # print(f\"\\nCalculating for category: {category} with lifetime: {lifetime}\")\n",
    "        \n",
    "        total_capital_cost, net_capital_cost = calculate_costs(df_copy, category, input_mp, menu_mp, policy_scenario)\n",
    "        \n",
    "        # print(f\"Total capital cost for {category}: {total_capital_cost}\")\n",
    "        # print(f\"Net capital cost for {category}: {net_capital_cost}\")\n",
    "        \n",
    "        calculate_and_update_npv(df_new_columns, df_fuelCosts_copy, category, menu_mp, interest_rate, lifetime, total_capital_cost, net_capital_cost, policy_scenario)\n",
    "      \n",
    "    overlapping_columns = df_new_columns.columns.intersection(df_copy.columns)\n",
    "    if not overlapping_columns.empty:\n",
    "        df_copy.drop(columns=overlapping_columns, inplace=True)\n",
    "\n",
    "    df_copy = df_copy.join(df_new_columns, how='left')\n",
    "    # print(\"Final DataFrame after NPV calculations:\\n\", df_copy.head())\n",
    "    return df_copy\n",
    "\n",
    "def calculate_costs(df_copy, category, input_mp, menu_mp, policy_scenario):\n",
    "    \"\"\"\n",
    "    Calculate total and net capital costs based on the equipment category and cost assumptions.\n",
    "\n",
    "    Parameters:\n",
    "        df_copy (DataFrame): DataFrame containing cost data.\n",
    "        category (str): Equipment category.\n",
    "        menu_mp (str): Prefix for columns in the DataFrame.\n",
    "        input_mp (str): Input policy_scenario for calculating costs.\n",
    "        ira_rebates (bool): Flag indicating whether IRA rebates are applied.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Total and net capital costs.\n",
    "    \"\"\"\n",
    "    print(f\"\"\"\\nCalculating costs for {category}...\n",
    "          input_mp: {input_mp}, menu_mp: {menu_mp}, policy_scenario: {policy_scenario}\"\"\")\n",
    "\n",
    "\n",
    "    # POTENTIALLY UPDATE CODE IN THE FUTURE TO ACCOUNT FOR CHANGES IN CAPITAL COSTS BASED ON SCENARIOS (BESIDES IRA REBATES)\n",
    "    # Note: CURRENT MODELING ASSUMES EQUIPMENT PRICES ARE THE SAME UNDER IRA REF AND IRA HIGH\n",
    "    # THIS MAY BE UPDATED IN THE FUTURE, SO WE STILL USE policy_scenario PREFIXES FOR TOTAL AND NET CAPITAL COSTS\n",
    "    # COSTS ARE DIFFERENT FOR PRE-IRA BECAUSE NO REBATES ARE APPLIED\n",
    "    if policy_scenario == 'No Inflation Reduction Act':\n",
    "        if category == 'heating':\n",
    "            if input_mp == 'upgrade09':            \n",
    "                weatherization_cost = df_copy[f'mp9_enclosure_upgradeCost'].fillna(0)\n",
    "            elif input_mp == 'upgrade10':\n",
    "                weatherization_cost = df_copy[f'mp10_enclosure_upgradeCost'].fillna(0)\n",
    "            else:\n",
    "                weatherization_cost = 0.0\n",
    "            # print(f\"Weatherization cost (no IRA rebates): {weatherization_cost}\")\n",
    "            \n",
    "            total_capital_cost = (df_copy[f'mp{menu_mp}_{category}_installationCost'].fillna(0) + \n",
    "                                  weatherization_cost + \n",
    "                                  df_copy[f'mp{menu_mp}_heating_installation_premium'].fillna(0))\n",
    "            net_capital_cost = total_capital_cost - df_copy[f'mp{menu_mp}_{category}_replacementCost'].fillna(0)\n",
    "            \n",
    "        else:\n",
    "            total_capital_cost = df_copy[f'mp{menu_mp}_{category}_installationCost'].fillna(0)\n",
    "            net_capital_cost = total_capital_cost - df_copy[f'mp{menu_mp}_{category}_replacementCost'].fillna(0)\n",
    "    \n",
    "    # POTENTIALLY UPDATE CODE IN THE FUTURE TO ACCOUNT FOR CHANGES IN CAPITAL COSTS BASED ON SCENARIOS (BESIDES IRA REBATES)\n",
    "    # Note: CURRENT MODELING ASSUMES EQUIPMENT PRICES ARE THE SAME UNDER IRA REF AND IRA HIGH\n",
    "    # THIS MAY BE UPDATED IN THE FUTURE, SO WE STILL USE policy_scenario PREFIXES FOR TOTAL AND NET CAPITAL COSTS\n",
    "    # COSTS ARE DIFFERENT FOR PRE-IRA BECAUSE NO REBATES ARE APPLIED\n",
    "    else:\n",
    "        if category == 'heating':\n",
    "            if input_mp == 'upgrade09':            \n",
    "                weatherization_cost = df_copy[f'mp9_enclosure_upgradeCost'].fillna(0) - df_copy[f'weatherization_rebate_amount'].fillna(0)\n",
    "            elif input_mp == 'upgrade10':\n",
    "                weatherization_cost = df_copy[f'mp10_enclosure_upgradeCost'].fillna(0) - df_copy[f'weatherization_rebate_amount'].fillna(0)\n",
    "            else:\n",
    "                weatherization_cost = 0.0       \n",
    "            # print(f\"Weatherization cost (with IRA rebates): {weatherization_cost}\")\n",
    "            \n",
    "            installation_cost = (df_copy[f'mp{menu_mp}_{category}_installationCost'].fillna(0) + \n",
    "                                 weatherization_cost + \n",
    "                                 df_copy[f'mp{menu_mp}_{category}_installation_premium'].fillna(0))\n",
    "            \n",
    "            rebate_amount = df_copy[f'mp{menu_mp}_{category}_rebate_amount'].fillna(0)\n",
    "            total_capital_cost = installation_cost - rebate_amount\n",
    "            net_capital_cost = total_capital_cost - df_copy[f'mp{menu_mp}_{category}_replacementCost'].fillna(0)\n",
    "        \n",
    "        else:\n",
    "            installation_cost = df_copy[f'mp{menu_mp}_{category}_installationCost'].fillna(0)\n",
    "            rebate_amount = df_copy[f'mp{menu_mp}_{category}_rebate_amount'].fillna(0)\n",
    "            total_capital_cost = installation_cost - rebate_amount\n",
    "            net_capital_cost = total_capital_cost - df_copy[f'mp{menu_mp}_{category}_replacementCost'].fillna(0)\n",
    "\n",
    "    # print(f\"Calculated total_capital_cost: {total_capital_cost}, net_capital_cost: {net_capital_cost}\")\n",
    "    return total_capital_cost, net_capital_cost\n",
    "\n",
    "def calculate_and_update_npv(df_new_columns, df_fuelCosts_copy, category, menu_mp, interest_rate, lifetime, total_capital_cost, net_capital_cost, policy_scenario):\n",
    "    \"\"\"\n",
    "    Calculate and update the NPV values in the DataFrame based on provided capital costs.\n",
    "\n",
    "    Parameters:\n",
    "        df_new_columns (DataFrame): DataFrame to update.\n",
    "        df_fuelCosts_copy (DataFrame): Original DataFrame containing savings data.\n",
    "        category (str): Equipment category.\n",
    "        menu_mp (str): Prefix for columns in the DataFrame.\n",
    "        interest_rate (float): Discount rate for NPV calculation.\n",
    "        lifetime (int): Expected lifetime of the equipment.\n",
    "        total_capital_cost (float): Total capital cost of the equipment.\n",
    "        net_capital_cost (float): Net capital cost after considering replacements.\n",
    "        ira_rebates (bool): Flag to consider IRA rebates in calculations.\n",
    "    \"\"\"\n",
    "    # Determine the policy_scenario prefix based on the policy policy_scenario\n",
    "    if policy_scenario == 'No Inflation Reduction Act':\n",
    "        scenario_prefix = f\"preIRA_mp{menu_mp}_\"\n",
    "    elif policy_scenario == 'AEO2023 Reference Case':\n",
    "        scenario_prefix = f\"iraRef_mp{menu_mp}_\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid Policy policy_scenario! Please choose from 'AEO2023 Reference Case'.\")\n",
    "        \n",
    "    print(f\"\"\"\\nCalculating Private NPV for {category}...\n",
    "          lifetime: {lifetime}, interest_rate: {interest_rate}, policy_scenario: {policy_scenario}\n",
    "          \"\"\")\n",
    "\n",
    "    # Calculate the discounted savings for each year\n",
    "    discounted_savings = []\n",
    "    for year in range(1, lifetime + 1):\n",
    "        year_label = year + 2023  # Adjust the start year as necessary\n",
    "        annual_savings = df_fuelCosts_copy[f'{scenario_prefix}{year_label}_{category}_savings_fuelCost'].fillna(0)\n",
    "        discount_factor = (1 / ((1 + interest_rate) ** year))\n",
    "        discounted_savings.append(annual_savings * discount_factor)\n",
    "        # print(f\"Year {year_label} savings for {category}: {annual_savings}, discounted: {annual_savings * discount_factor}\")\n",
    "    \n",
    "    # Sum up the discounted savings over the lifetime\n",
    "    total_discounted_savings = sum(discounted_savings)\n",
    "    # print(f\"Total discounted savings over {lifetime} years for {category}: {total_discounted_savings}\")\n",
    "    \n",
    "    # Calculate NPV for less WTP and more WTP scenarios\n",
    "    npv_lessWTP = round(total_discounted_savings - total_capital_cost, 2)\n",
    "    npv_moreWTP = round(total_discounted_savings - net_capital_cost, 2)\n",
    "    \n",
    "    # POTENTIALLY UPDATE CODE IN THE FUTURE TO ACCOUNT FOR CHANGES IN CAPITAL COSTS BASED ON SCENARIOS (BESIDES IRA REBATES)\n",
    "    # Note: CURRENT MODELING ASSUMES EQUIPMENT PRICES ARE THE SAME UNDER IRA REF AND IRA HIGH\n",
    "    # THIS MAY BE UPDATED IN THE FUTURE, SO WE STILL USE policy_scenario PREFIXES FOR TOTAL AND NET CAPITAL COSTS\n",
    "    # COSTS ARE DIFFERENT FOR PRE-IRA BECAUSE NO REBATES ARE APPLIED\n",
    "    df_new_columns[f'{scenario_prefix}{category}_total_capitalCost'] = total_capital_cost\n",
    "    df_new_columns[f'{scenario_prefix}{category}_net_capitalCost'] = net_capital_cost\n",
    "        \n",
    "    df_new_columns[f'{scenario_prefix}{category}_private_npv_lessWTP'] = npv_lessWTP\n",
    "    df_new_columns[f'{scenario_prefix}{category}_private_npv_moreWTP'] = npv_moreWTP\n",
    "        \n",
    "    # print(f\"Updated df_new_columns with NPV for {category}:\\n\", df_new_columns[[col for col in df_new_columns.columns if category in col]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cce0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UPDATED AUGUST 21, 2024 @ 11:40 PM\n",
    "# def adoption_decision(df, policy_scenario):\n",
    "#     \"\"\"\n",
    "#     Updates the provided DataFrame with new columns that reflect decisions about equipment adoption\n",
    "#     and public impacts based on net present values (NPV). The function handles different scenarios\n",
    "#     based on input flags for incentives and grid decarbonization.\n",
    "\n",
    "#     Parameters:\n",
    "#         df (pandas.DataFrame): The DataFrame containing home equipment data.\n",
    "#         policy_scenario (str): Policy policy_scenario that determines electricity grid projections. \n",
    "#                                Accepted values: 'AEO2023 Reference Case'.\n",
    "\n",
    "#     Returns:\n",
    "#         pandas.DataFrame: The modified DataFrame with additional columns for decisions and impacts.\n",
    "\n",
    "#     Notes:\n",
    "#         - It adds columns for both individual and public economic evaluations.\n",
    "#         - Adoption decisions and public impacts are dynamically calculated based on the input parameters.\n",
    "#     \"\"\"\n",
    "#     df_copy = df.copy()\n",
    "    \n",
    "#     # Define the lifetimes of different equipment categories\n",
    "#     upgrade_columns = {\n",
    "#         'heating': 'upgrade_hvac_heating_efficiency',\n",
    "#         'waterHeating': 'upgrade_water_heater_efficiency',\n",
    "#         'clothesDrying': 'upgrade_clothes_dryer',\n",
    "#         'cooking': 'upgrade_cooking_range'\n",
    "#     }\n",
    "    \n",
    "#     df_new_columns = pd.DataFrame(index=df_copy.index)  # DataFrame to hold new or modified columns\n",
    "\n",
    "#     # Determine the policy_scenario prefix based on the policy policy_scenario\n",
    "#     if policy_scenario == 'No Inflation Reduction Act':\n",
    "#         scenario_prefix = f\"preIRA_mp{menu_mp}_\"\n",
    "#     elif policy_scenario == 'AEO2023 Reference Case':\n",
    "#         scenario_prefix = f\"iraRef_mp{menu_mp}_\"\n",
    "#     else:\n",
    "#         raise ValueError(\"Invalid Policy Scenario! Please choose from 'No Inflation Reduction Act' or 'AEO2023 Reference Case'.\")\n",
    "\n",
    "#     # Iterate over each equipment category and its respective upgrade column\n",
    "#     for category, upgrade_column in upgrade_columns.items():\n",
    "#         # Column names for net NPV, private NPV, and public NPV\n",
    "#         lessWTP_total_npv_col = f'{scenario_prefix}{category}_total_npv_lessWTP' # LESS WTP: BREAK EVEN ON TOTAL CAPITAL COSTS\n",
    "#         moreWTP_total_npv_col = f'{scenario_prefix}{category}_total_npv_moreWTP' # MORE WTP: BREAK EVEN ON NET CAPITAL COSTS (BETTER THAN ALTERNATIVE)\n",
    "\n",
    "#         lessWTP_private_npv_col = f'{scenario_prefix}{category}_private_npv_lessWTP' # LESS WTP: BREAK EVEN ON TOTAL CAPITAL COSTS\n",
    "#         moreWTP_private_npv_col = f'{scenario_prefix}{category}_private_npv_moreWTP' # MORE WTP: BREAK EVEN ON NET CAPITAL COSTS (BETTER THAN ALTERNATIVE)\n",
    "\n",
    "#         public_npv_col = f'{scenario_prefix}{category}_public_npv'\n",
    "\n",
    "#         # Ensure columns are numeric if they exist and convert them\n",
    "#         for col in [lessWTP_private_npv_col, moreWTP_private_npv_col, public_npv_col]:\n",
    "#             if col in df.columns:\n",
    "#                 df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "#             else:\n",
    "#                 print(f\"Warning: {col} does not exist in the DataFrame.\")\n",
    "\n",
    "#         # Ensure the columns are present after conversion\n",
    "#         if lessWTP_private_npv_col in df.columns and moreWTP_private_npv_col in df.columns and public_npv_col in df.columns:\n",
    "#             # \n",
    "            \n",
    "#             # Calculate net NPV by summing private and public NPVs\n",
    "#             df_new_columns[lessWTP_total_npv_col] = df[lessWTP_private_npv_col] + df[public_npv_col] # LESS WTP: BREAK EVEN ON TOTAL CAPITAL COSTS\n",
    "#             df_new_columns[moreWTP_total_npv_col] = df[moreWTP_private_npv_col] + df[public_npv_col] # MORE WTP: BREAK EVEN ON NET CAPITAL COSTS (BETTER THAN ALTERNATIVE)\n",
    "\n",
    "#             # Initialize columns for adoption decisions and public impact\n",
    "#             adoption_col_name = f'{scenario_prefix}{category}_adoption'\n",
    "#             retrofit_col_name = f'{scenario_prefix}{category}_retrofit_publicImpact'\n",
    "#             df_new_columns[adoption_col_name] = 'Tier 4: Averse'  # Default value for all rows\n",
    "#             df_new_columns[retrofit_col_name] = 'No Retrofit'  # Default public impact\n",
    "\n",
    "#             # Conditions for determining adoption decisions\n",
    "#             conditions = [\n",
    "#                 df[upgrade_column].isna(),\n",
    "#                 df[lessWTP_private_npv_col] > 0,\n",
    "#                 (df[lessWTP_private_npv_col] < 0) & (df[moreWTP_private_npv_col] > 0),\n",
    "#                 (df[lessWTP_private_npv_col] < 0) & (df[moreWTP_private_npv_col] <= 0) & (df_new_columns[moreWTP_total_npv_col] > 0),\n",
    "#             ]\n",
    "\n",
    "#             choices = ['Existing Equipment', 'Tier 1: Feasible', 'Tier 2: Feasible vs. Alternative', 'Tier 3: Subsidy-Dependent Feasibility']\n",
    "#             df_new_columns[adoption_col_name] = np.select(conditions, choices, default='Tier 4: Averse')\n",
    "\n",
    "#             # Conditions and choices for public impacts\n",
    "#             public_conditions = [\n",
    "#                 df[public_npv_col] > 0,\n",
    "#                 df[public_npv_col] < 0\n",
    "#             ]\n",
    "            \n",
    "#             public_choices = ['Public Benefit', 'Public Detriment']\n",
    "#             df_new_columns[retrofit_col_name] = np.select(public_conditions, public_choices, default='No Retrofit')\n",
    "#         else:\n",
    "#             print(f\"Warning: One or more columns ({lessWTP_private_npv_col}, {moreWTP_private_npv_col}, {public_npv_col}) are missing or not numeric.\")\n",
    "    \n",
    "#     # Identify overlapping columns between the new and existing DataFrame.\n",
    "#     overlapping_columns = df_new_columns.columns.intersection(df_copy.columns)\n",
    "\n",
    "#     # Drop overlapping columns from df_copy.\n",
    "#     if not overlapping_columns.empty:\n",
    "#         df_copy.drop(columns=overlapping_columns, inplace=True)\n",
    "\n",
    "#     # Merge new columns into df_copy, ensuring no duplicates or overwrites occur.\n",
    "#     df_copy = df_copy.join(df_new_columns, how='left')\n",
    "\n",
    "#     # Return the updated DataFrame.\n",
    "#     return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e08039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATED SEPTEMBER 14, 2024 @ 4:23 PM\n",
    "def adoption_decision(df, policy_scenario, menu_mp):\n",
    "    \"\"\"\n",
    "    Updates the provided DataFrame with new columns that reflect decisions about equipment adoption\n",
    "    and public impacts based on net present values (NPV). The function handles different scenarios\n",
    "    based on input flags for incentives and grid decarbonization.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The DataFrame containing home equipment data.\n",
    "        policy_scenario (str): Policy policy_scenario that determines electricity grid projections. \n",
    "                               Accepted values: 'AEO2023 Reference Case'.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The modified DataFrame with additional columns for decisions and impacts.\n",
    "\n",
    "    Notes:\n",
    "        - It adds columns for both individual and public economic evaluations.\n",
    "        - Adoption decisions and public impacts are dynamically calculated based on the input parameters.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Define the lifetimes of different equipment categories\n",
    "    upgrade_columns = {\n",
    "        'heating': 'upgrade_hvac_heating_efficiency',\n",
    "        # 'waterHeating': 'upgrade_water_heater_efficiency',\n",
    "        # 'clothesDrying': 'upgrade_clothes_dryer',\n",
    "        # 'cooking': 'upgrade_cooking_range'\n",
    "    }\n",
    "    \n",
    "    df_new_columns = pd.DataFrame(index=df_copy.index)  # DataFrame to hold new or modified columns\n",
    "\n",
    "    # Determine the policy_scenario prefix based on the policy policy_scenario\n",
    "    if policy_scenario == 'No Inflation Reduction Act':\n",
    "        scenario_prefix = f\"preIRA_mp{menu_mp}_\"\n",
    "    elif policy_scenario == 'AEO2023 Reference Case':\n",
    "        scenario_prefix = f\"iraRef_mp{menu_mp}_\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid Policy Scenario! Please choose from 'No Inflation Reduction Act' or 'AEO2023 Reference Case'.\")\n",
    "\n",
    "    # Iterate over each equipment category and its respective upgrade column\n",
    "    for category, upgrade_column in upgrade_columns.items():\n",
    "        # Column names for net NPV, private NPV, and public NPV\n",
    "        lessWTP_private_npv_col = f'{scenario_prefix}{category}_private_npv_lessWTP' # LESS WTP: BREAK EVEN ON TOTAL CAPITAL COSTS\n",
    "        moreWTP_private_npv_col = f'{scenario_prefix}{category}_private_npv_moreWTP' # MORE WTP: BREAK EVEN ON NET CAPITAL COSTS (BETTER THAN ALTERNATIVE)\n",
    "\n",
    "        public_npv_col = f'{scenario_prefix}{category}_public_npv'\n",
    "        rebate_col = f'mp{menu_mp}_{category}_rebate_amount'\n",
    "        addition_public_benefit = f'{scenario_prefix}{category}_additional_public_benefit'\n",
    "\n",
    "        lessWTP_total_npv_col = f'{scenario_prefix}{category}_total_npv_lessWTP' # LESS WTP: BREAK EVEN ON TOTAL CAPITAL COSTS\n",
    "        moreWTP_total_npv_col = f'{scenario_prefix}{category}_total_npv_moreWTP' # MORE WTP: BREAK EVEN ON NET CAPITAL COSTS (BETTER THAN ALTERNATIVE)\n",
    "        # Ensure columns are numeric if they exist and convert them\n",
    "        for col in [lessWTP_private_npv_col, moreWTP_private_npv_col, public_npv_col, rebate_col]:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            else:\n",
    "                print(f\"Warning: {col} does not exist in the DataFrame.\")\n",
    "\n",
    "        # Ensure the columns are present after conversion\n",
    "        if lessWTP_private_npv_col in df.columns and moreWTP_private_npv_col in df.columns and public_npv_col in df.columns:\n",
    "            # No IRA Rebate so no \"Additional Public Benefit\"\n",
    "            if policy_scenario == 'No Inflation Reduction Act':\n",
    "                df_new_columns[addition_public_benefit] = 0.0\n",
    "            else:\n",
    "                # Calculate Additional Public Benefit with IRA Rebates Accounted For and clip at 0\n",
    "                df_new_columns[addition_public_benefit] = (df[public_npv_col] - df[rebate_col]).clip(lower=0)\n",
    "            \n",
    "            # Calculate Total NPV by summing private and public NPVs\n",
    "            df_new_columns[lessWTP_total_npv_col] = df[lessWTP_private_npv_col] + df_new_columns[addition_public_benefit] # LESS WTP: BREAK EVEN ON TOTAL CAPITAL COSTS\n",
    "            df_new_columns[moreWTP_total_npv_col] = df[moreWTP_private_npv_col] + df_new_columns[addition_public_benefit] # MORE WTP: BREAK EVEN ON NET CAPITAL COSTS (BETTER THAN ALTERNATIVE)\n",
    "\n",
    "            # Initialize columns for adoption decisions and public impact\n",
    "            adoption_col_name = f'{scenario_prefix}{category}_adoption'\n",
    "            retrofit_col_name = f'{scenario_prefix}{category}_retrofit_publicImpact'\n",
    "            df_new_columns[adoption_col_name] = 'Tier 4: Averse'  # Default value for all rows\n",
    "            df_new_columns[retrofit_col_name] = 'No Retrofit'  # Default public impact\n",
    "\n",
    "            # Conditions for determining adoption decisions\n",
    "            conditions = [\n",
    "                df[upgrade_column].isna(),\n",
    "                df[lessWTP_private_npv_col] > 0,\n",
    "                (df[lessWTP_private_npv_col] < 0) & (df[moreWTP_private_npv_col] > 0),\n",
    "                (df[lessWTP_private_npv_col] < 0) & (df[moreWTP_private_npv_col] <= 0) & (df_new_columns[moreWTP_total_npv_col] > 0) & (df_new_columns[addition_public_benefit] > 0), # Ensures only Tier 3 for IRA Scenario\n",
    "            ]\n",
    "\n",
    "            choices = ['Existing Equipment', 'Tier 1: Feasible', 'Tier 2: Feasible vs. Alternative', 'Tier 3: Subsidy-Dependent Feasibility']\n",
    "            df_new_columns[adoption_col_name] = np.select(conditions, choices, default='Tier 4: Averse')\n",
    "\n",
    "            # Conditions and choices for public impacts\n",
    "            public_conditions = [\n",
    "                df[public_npv_col] > 0,\n",
    "                df[public_npv_col] < 0\n",
    "            ]\n",
    "            \n",
    "            public_choices = ['Public Benefit', 'Public Detriment']\n",
    "            df_new_columns[retrofit_col_name] = np.select(public_conditions, public_choices, default='No Retrofit')\n",
    "        else:\n",
    "            print(f\"Warning: One or more columns ({lessWTP_private_npv_col}, {moreWTP_private_npv_col}, {public_npv_col}) are missing or not numeric.\")\n",
    "    \n",
    "    # Identify overlapping columns between the new and existing DataFrame.\n",
    "    overlapping_columns = df_new_columns.columns.intersection(df_copy.columns)\n",
    "\n",
    "    # Drop overlapping columns from df_copy.\n",
    "    if not overlapping_columns.empty:\n",
    "        df_copy.drop(columns=overlapping_columns, inplace=True)\n",
    "\n",
    "    # Merge new columns into df_copy, ensuring no duplicates or overwrites occur.\n",
    "    df_copy = df_copy.join(df_new_columns, how='left')\n",
    "\n",
    "    # Return the updated DataFrame.\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b219b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_adoption_consistency(df, category, upgrade_column):\n",
    "#     df_copy = df.copy()\n",
    "    \n",
    "#     cols_to_display = ['bldg_id',\n",
    "#                        f'base_{category}_fuel',\n",
    "#                        f'{upgrade_column}',\n",
    "#                        f'baseline_{category}_consumption',\n",
    "#                        f'mp{menu_mp}_{category}_consumption',\n",
    "#                        f'mp{menu_mp}_{category}_reduction_consumption',\n",
    "#                        f'baseline_{category}_fuelCost',\n",
    "#                        f'mp{menu_mp}_{category}_fuelCost',        \n",
    "#                        f'mp{menu_mp}_{category}_savings_fuelCost',\n",
    "#                        f'mp{menu_mp}_{category}_net_capitalCost',\n",
    "#                        f'mp{menu_mp}_{category}_private_npv',\n",
    "#                        f'baseline_{category}_damages_health',\n",
    "#                        f'baseline_{category}_damages_climate',\n",
    "#                        f'mp{menu_mp}_{category}_damages_health',\n",
    "#                        f'mp{menu_mp}_{category}_damages_climate',\n",
    "#                        f'mp{menu_mp}_{category}_reduction_damages_health',\n",
    "#                        f'mp{menu_mp}_{category}_reduction_damages_climate',\n",
    "#                        f'mp{menu_mp}_{category}_public_npv',\n",
    "#                        f'mp{menu_mp}_{category}_retrofit_publicImpact',\n",
    "#                        f'mp{menu_mp}_{category}_total_npv',\n",
    "#                        f'mp{menu_mp}_{category}_adoption',  \n",
    "#                        ]    \n",
    "        \n",
    "#     # Filter the dataframe to show only the columns relevant for the current cost_type\n",
    "#     df_filtered = df_copy[cols_to_display]\n",
    "    \n",
    "#     return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dd7b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATED SEPTEMBER 6, 2024 @ 12:10 AM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def generate_household_medianIncome_2023(row):\n",
    "    # Inflate the income bins to USD 2023 first\n",
    "    low = row['income_low'] * cpi_ratio_2023_2022\n",
    "    high = row['income_high'] * cpi_ratio_2023_2022\n",
    "    mean = row['income'] * cpi_ratio_2023_2022\n",
    "    \n",
    "    # Calculate std assuming 10th and 90th percentiles\n",
    "    std = (high - low) / (norm.ppf(0.90) - norm.ppf(0.10))\n",
    "    \n",
    "    # Sample from the normal distribution\n",
    "    ami_2023 = np.random.normal(loc=mean, scale=std)\n",
    "    \n",
    "    # Ensure the generated income is within the bounds\n",
    "    ami_2023 = max(low, min(high, ami_2023))\n",
    "    return ami_2023\n",
    "\n",
    "def fill_na_with_hierarchy(df, df_puma, df_county, df_state):\n",
    "    \"\"\"\n",
    "    Fills NaN values in 'census_area_medianIncome' using a hierarchical lookup:\n",
    "    first using the Puma level, then county, and finally state level median incomes.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The main DataFrame with NaNs to fill.\n",
    "        df_puma (DataFrame): DataFrame with median incomes at the Puma level.\n",
    "        df_county (DataFrame): DataFrame with median incomes at the county level.\n",
    "        df_state (DataFrame): DataFrame with median incomes at the state level.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Modified DataFrame with NaNs filled in 'census_area_medianIncome'.\n",
    "    \"\"\"\n",
    "\n",
    "    # First, attempt to fill using Puma-level median incomes\n",
    "    df['census_area_medianIncome'] = df['puma'].map(\n",
    "        df_puma.set_index('gis_joinID_puma')['median_income_USD2023']\n",
    "    )\n",
    "\n",
    "    # Find the rows where 'census_area_medianIncome' is NaN\n",
    "    nan_mask = df['census_area_medianIncome'].isna()\n",
    "\n",
    "    # Attempt to fill NaNs using county-level median incomes\n",
    "    df.loc[nan_mask, 'census_area_medianIncome'] = df.loc[nan_mask, 'county'].map(\n",
    "        df_county.set_index('gis_joinID_county')['median_income_USD2023']\n",
    "    )\n",
    "\n",
    "    # Update the NaN mask after attempting to fill with county-level data\n",
    "    nan_mask = df['census_area_medianIncome'].isna()\n",
    "\n",
    "    # Attempt to fill remaining NaNs using state-level median incomes\n",
    "    df.loc[nan_mask, 'census_area_medianIncome'] = df.loc[nan_mask, 'state'].map(\n",
    "        df_state.set_index('state_abbrev')['median_income_USD2023']\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_percent_AMI(df_results_IRA):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of Area Median Income (AMI) and assigns a designation based on the income level.\n",
    "\n",
    "    Parameters:\n",
    "        df_results_IRA (DataFrame): Input DataFrame containing income information.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Modified DataFrame with additional columns for income calculations and designation.\n",
    "    \"\"\"\n",
    "    # Create a mapping for income ranges\n",
    "    income_map = {\n",
    "        '<10000': (9999.0, 9999.0),\n",
    "        '200000+': (200000.0, 200000.0)\n",
    "    }\n",
    "\n",
    "    # Split the income ranges and map values\n",
    "    def split_income_range(income):\n",
    "        if isinstance(income, float):  # Handle float income directly\n",
    "            return income, income\n",
    "        if income in income_map:\n",
    "            return income_map[income]\n",
    "        try:\n",
    "            low, high = map(float, income.split('-'))\n",
    "            return low, high\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Unexpected income format: {income}\") from e\n",
    "\n",
    "    # Apply the income range split\n",
    "    income_ranges = df_results_IRA['income'].apply(split_income_range)\n",
    "    df_results_IRA['income_low'], df_results_IRA['income_high'] = zip(*income_ranges)\n",
    "    df_results_IRA['income'] = (df_results_IRA['income_low'] + df_results_IRA['income_high']) / 2\n",
    "    \n",
    "    # Apply the generate_household_medianIncome_2023 function\n",
    "    df_results_IRA['household_income'] = df_results_IRA.apply(generate_household_medianIncome_2023, axis=1)\n",
    "\n",
    "    # Drop the intermediate columns\n",
    "    df_results_IRA.drop(['income_low', 'income_high'], axis=1, inplace=True)\n",
    "\n",
    "    # Fill NaNs in 'census_area_medianIncome' with the hierarchical lookup\n",
    "    # Attempt to match median income for puma, then county, then state\n",
    "    df_results_IRA = fill_na_with_hierarchy(df_results_IRA, df_puma=df_puma_medianIncome, df_county=df_county_medianIncome, df_state=df_state_medianIncome)\n",
    "\n",
    "    # Ensure income and census_area_medianIncome columns are float\n",
    "    df_results_IRA['household_income'] = df_results_IRA['household_income'].astype(float).round(2)\n",
    "    df_results_IRA['census_area_medianIncome'] = df_results_IRA['census_area_medianIncome'].astype(float).round(2)\n",
    "\n",
    "    # Calculate percent_AMI\n",
    "    df_results_IRA['percent_AMI'] = ((df_results_IRA['household_income'] / df_results_IRA['census_area_medianIncome']) * 100).round(2)\n",
    "\n",
    "    # Categorize the income level based on percent_AMI\n",
    "    conditions_lmi = [\n",
    "        df_results_IRA['percent_AMI'] <= 80.0,\n",
    "        (df_results_IRA['percent_AMI'] > 80.0) & (df_results_IRA['percent_AMI'] <= 150.0)\n",
    "    ]\n",
    "    choices_lmi = ['Low-Income', 'Moderate-Income']\n",
    "\n",
    "    df_results_IRA['lowModerateIncome_designation'] = np.select(\n",
    "        conditions_lmi, choices_lmi, default='Middle-to-Upper-Income'\n",
    "    )\n",
    "\n",
    "    # Output the modified DataFrame\n",
    "    return df_results_IRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e93789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATED AUGUST 20, 2024 @ 3:08 AM\n",
    "# Mapping for categories and their corresponding conditions\n",
    "rebate_mapping = {\n",
    "    'heating': ('upgrade_hvac_heating_efficiency', ['ASHP', 'MSHP'], 8000.00),\n",
    "    'waterHeating': ('upgrade_water_heater_efficiency', ['Electric Heat Pump'], 1750.00),\n",
    "    'clothesDrying': ('upgrade_clothes_dryer', ['Electric, Premium, Heat Pump, Ventless'], 840.00),\n",
    "    'cooking': ('upgrade_cooking_range', ['Electric, '], 840.00)\n",
    "}\n",
    "\n",
    "def get_max_rebate_amount(row, category):\n",
    "    \"\"\"\n",
    "    Determine the maximum rebate amounts based on the category and row data.\n",
    "    \"\"\"\n",
    "    if category in rebate_mapping:\n",
    "        column, conditions, rebate_amount = rebate_mapping[category]\n",
    "        max_rebate_amount = rebate_amount if any(cond in str(row[column]) for cond in conditions) else 0.00\n",
    "    else:\n",
    "        max_rebate_amount = 0.00\n",
    "\n",
    "    max_weatherization_rebate_amount = 1600.00\n",
    "    return max_rebate_amount, max_weatherization_rebate_amount\n",
    "\n",
    "def calculate_rebate(df_results_IRA, row, category, menu_mp, coverage_rate):\n",
    "    \"\"\"\n",
    "    Calculate and assign the rebate amounts.\n",
    "    \"\"\"\n",
    "    max_rebate_amount, max_weatherization_rebate_amount = get_max_rebate_amount(row, category)\n",
    "    \n",
    "    project_coverage = round(row[f'mp{menu_mp}_{category}_installationCost'] * coverage_rate, 2)\n",
    "    df_results_IRA.at[row.name, f'mp{menu_mp}_{category}_rebate_amount'] = min(project_coverage, max_rebate_amount)\n",
    "    \n",
    "    if f'mp{menu_mp}_enclosure_upgradeCost' in df_results_IRA.columns:\n",
    "        weatherization_project_coverage = round(row[f'mp{menu_mp}_enclosure_upgradeCost'] * coverage_rate, 2)\n",
    "        df_results_IRA.at[row.name, 'weatherization_rebate_amount'] = min(weatherization_project_coverage, max_weatherization_rebate_amount)\n",
    "\n",
    "def calculate_rebateIRA(df_results_IRA, category, menu_mp):\n",
    "    \"\"\"\n",
    "    Calculates rebate amounts for different end-uses based on income designation.\n",
    "    \"\"\"\n",
    "    def apply_rebate(row):\n",
    "        income_designation = row['lowModerateIncome_designation']\n",
    "        if income_designation == 'Low-Income':\n",
    "            calculate_rebate(df_results_IRA, row, category, menu_mp, 1.00)\n",
    "        elif income_designation == 'Moderate-Income':\n",
    "            calculate_rebate(df_results_IRA, row, category, menu_mp, 0.50)\n",
    "        else:\n",
    "            df_results_IRA.at[row.name, f'mp{menu_mp}_{category}_rebate_amount'] = 0.00\n",
    "            if menu_mp in [9, 10]:\n",
    "                df_results_IRA.at[row.name, 'weatherization_rebate_amount'] = 0.00\n",
    "\n",
    "    df_results_IRA.apply(apply_rebate, axis=1)\n",
    "    return df_results_IRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f74d5bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_ira_adoption_consistency(df, category, upgrade_column):\n",
    "#     df_copy = df.copy()\n",
    "    \n",
    "#     cols_to_display = ['bldg_id',\n",
    "#                        f'base_{category}_fuel',\n",
    "#                        f'{upgrade_column}',\n",
    "#                        f'baseline_{category}_consumption',\n",
    "#                        f'mp{menu_mp}_{category}_consumption',\n",
    "#                        f'mp{menu_mp}_{category}_reduction_consumption',\n",
    "#                        f'baseline_{category}_fuelCost',\n",
    "#                        f'mp{menu_mp}_{category}_fuelCost',        \n",
    "#                        f'mp{menu_mp}_{category}_savings_fuelCost',\n",
    "#                        f'mp{menu_mp}_{category}_net_capitalCost',\n",
    "#                        f'mp{menu_mp}_{category}_private_npv',\n",
    "#                        f'baseline_{category}_damages_health',\n",
    "#                        f'baseline_{category}_damages_climate',\n",
    "#                        f'mp{menu_mp}_{category}_damages_health',\n",
    "#                        f'mp{menu_mp}_{category}_damages_climate',\n",
    "#                        f'mp{menu_mp}_{category}_reduction_damages_health',\n",
    "#                        f'mp{menu_mp}_{category}_reduction_damages_climate',\n",
    "#                        f'mp{menu_mp}_{category}_public_npv',\n",
    "#                        f'mp{menu_mp}_{category}_retrofit_publicImpact',\n",
    "#                        f'mp{menu_mp}_{category}_total_npv',\n",
    "#                        f'mp{menu_mp}_{category}_adoption',\n",
    "#                        f'ira_mp{menu_mp}_{category}_net_capitalCost',\n",
    "#                        f'ira_mp{menu_mp}_{category}_private_npv',\n",
    "#                        f'ira_mp{menu_mp}_{category}_total_npv',\n",
    "#                        f'ira_mp{menu_mp}_{category}_adoption',\n",
    "#                        ]    \n",
    "\n",
    "#     # Filter the dataframe to show only the relevant columns\n",
    "#     df_filtered = df_copy[cols_to_display]\n",
    "    \n",
    "#     return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ae094b",
   "metadata": {},
   "source": [
    "## Moderate Retrofit (MP9): MP8 + Basic Enclosure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1929a10d",
   "metadata": {},
   "source": [
    "## Advanced Retrofit (MP10): MP8 + Enhanced Enclosure\n",
    "**Notes**\n",
    "- There are some inconsistencies for variable names and syntax for calculations\n",
    "- The calculations should still end up the same regardless because of order of operations\n",
    "- Plan to update for consistency to avoid user confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25f82f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATED AUGUST 22, 2024 @ 7:00 PM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Helper function to get conditions and tech-efficiency pairs for enclosure retrofit\n",
    "def get_enclosure_parameters(df, retrofit_col):\n",
    "    if retrofit_col == 'insulation_atticFloor_upgradeCost':\n",
    "        conditions = [\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-30') & (df['base_insulation_atticFloor'] == 'R-13'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-30') & (df['base_insulation_atticFloor'] == 'R-7'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-30') & (df['base_insulation_atticFloor'] == 'Uninsulated'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-49') & (df['base_insulation_atticFloor'] == 'R-30'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-49') & (df['base_insulation_atticFloor'] == 'R-19'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-49') & (df['base_insulation_atticFloor'] == 'R-13'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-49') & (df['base_insulation_atticFloor'] == 'R-7'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-49') & (df['base_insulation_atticFloor'] == 'Uninsulated'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-60') & (df['base_insulation_atticFloor'] == 'R-38'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-60') & (df['base_insulation_atticFloor'] == 'R-30'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-60') & (df['base_insulation_atticFloor'] == 'R-19'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-60') & (df['base_insulation_atticFloor'] == 'R-13'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-60') & (df['base_insulation_atticFloor'] == 'R-7'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-60') & (df['base_insulation_atticFloor'] == 'Uninsulated')\n",
    "        ]\n",
    "        tech_eff_pairs = [\n",
    "            ('Attic Floor Insulation: R-30', 'R-13'),\n",
    "            ('Attic Floor Insulation: R-30', 'R-7'),\n",
    "            ('Attic Floor Insulation: R-30', 'Uninsulated'),\n",
    "            ('Attic Floor Insulation: R-49', 'R-30'),\n",
    "            ('Attic Floor Insulation: R-49', 'R-19'),\n",
    "            ('Attic Floor Insulation: R-49', 'R-13'),\n",
    "            ('Attic Floor Insulation: R-49', 'R-7'),\n",
    "            ('Attic Floor Insulation: R-49', 'Uninsulated'),\n",
    "            ('Attic Floor Insulation: R-60', 'R-38'),\n",
    "            ('Attic Floor Insulation: R-60', 'R-30'),\n",
    "            ('Attic Floor Insulation: R-60', 'R-19'),\n",
    "            ('Attic Floor Insulation: R-60', 'R-13'),\n",
    "            ('Attic Floor Insulation: R-60', 'R-7'),\n",
    "            ('Attic Floor Insulation: R-60', 'Uninsulated')\n",
    "        ]\n",
    "    elif retrofit_col == 'infiltration_reduction_upgradeCost':\n",
    "        conditions = [\n",
    "            (df['upgrade_infiltration_reduction'] == '30%')\n",
    "        ]\n",
    "        tech_eff_pairs = [\n",
    "            ('Air Leakage Reduction: 30% Reduction', 'Varies')\n",
    "        ]\n",
    "    elif retrofit_col == 'duct_sealing_upgradeCost':\n",
    "        conditions = [\n",
    "            (df['upgrade_duct_sealing'] == '10% Leakage, R-8') & (df['base_ducts'].str.contains('10% Leakage')),\n",
    "            (df['upgrade_duct_sealing'] == '10% Leakage, R-8') & (df['base_ducts'].str.contains('20% Leakage')),\n",
    "            (df['upgrade_duct_sealing'] == '10% Leakage, R-8') & (df['base_ducts'].str.contains('30% Leakage')),\n",
    "        ]\n",
    "        tech_eff_pairs = [\n",
    "            ('Duct Sealing: 10% Leakage, R-8', '10% Leakage'),\n",
    "            ('Duct Sealing: 10% Leakage, R-8', '20% Leakage'),\n",
    "            ('Duct Sealing: 10% Leakage, R-8', '30% Leakage'),\n",
    "        ]\n",
    "    elif retrofit_col == 'insulation_wall_upgradeCost':\n",
    "        conditions = [\n",
    "            (df['upgrade_insulation_wall'] == 'Wood Stud, R-13')\n",
    "        ]\n",
    "        tech_eff_pairs = [\n",
    "            ('Drill-and-fill Wall Insulation: Wood Stud, R-13', 'Wood Stud, Uninsulated')\n",
    "        ]\n",
    "    elif retrofit_col == 'insulation_foundation_wall_upgradeCost':\n",
    "        conditions = [\n",
    "            (df['upgrade_insulation_foundation_wall'] == 'Wall R-10, Interior')\n",
    "        ]\n",
    "        tech_eff_pairs = [\n",
    "            ('Foundation Wall Insulation: Wall R-10, Interior', 'Uninsulated')\n",
    "        ]\n",
    "    elif retrofit_col == 'insulation_rim_joist_upgradeCost':\n",
    "        conditions = [\n",
    "            (df['base_insulation_foundation_wall'] == 'Uninsulated') & (df['base_foundation_type'].isin(['Unvented Crawlspace', 'Vented Crawlspace', 'Heated Basement']))\n",
    "        ]\n",
    "        tech_eff_pairs = [\n",
    "            ('Rim Joist Insulation: Wall R-10, Exterior', 'Uninsulated')\n",
    "        ]\n",
    "    elif retrofit_col == 'seal_crawlspace_upgradeCost':\n",
    "        conditions = [\n",
    "            (df['upgrade_seal_crawlspace'] == 'Unvented Crawlspace')\n",
    "        ]\n",
    "        tech_eff_pairs = [\n",
    "            ('Seal Vented Crawlspace: Unvented Crawlspace', 'Vented Crawlspace')\n",
    "        ]\n",
    "    elif retrofit_col == 'insulation_roof_upgradeCost':\n",
    "        conditions = [\n",
    "            (df['upgrade_insulation_roof'] == 'Finished, R-30')\n",
    "        ]\n",
    "        tech_eff_pairs = [\n",
    "            ('Insulate Finished Attics and Cathedral Ceilings: Finished, R-30', 'R-30')\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid retrofit_col specified: {retrofit_col}\")\n",
    "    \n",
    "    return {'conditions': conditions, 'tech_eff_pairs': tech_eff_pairs}\n",
    "\n",
    "# UPDATED AUGUST 22, 2024 @ 7:00 PM\n",
    "def calculate_enclosure_retrofit_upgradeCosts(df, cost_dict, retrofit_col, params_col, rsMeans_national_avg):\n",
    "    \"\"\"\n",
    "    Calculate the enclosure retrofit upgrade costs based on given parameters and conditions.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing data for different scenarios.\n",
    "    cost_dict (dict): Dictionary with cost information for different technology and efficiency combinations.\n",
    "    retrofit_col (str): Column name for the retrofit cost.\n",
    "        - NaN value indicates that the retrofit was not performed.\n",
    "    params_col (str): Column name for the parameter to use in the cost calculation.\n",
    "    rsMeans_national_avg (float): National average value for cost adjustment.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Updated DataFrame with calculated retrofit costs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy of the original DataFrame to avoid modifying it directly\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Get conditions and tech-efficiency pairs for the specified retrofit\n",
    "    params = get_enclosure_parameters(df_copy, retrofit_col)\n",
    "    conditions = params['conditions']\n",
    "    tech_eff_pairs = params['tech_eff_pairs']\n",
    "\n",
    "    # # Debug: Print the extracted parameters\n",
    "    # print(\"Extracted Parameters:\", params)\n",
    "\n",
    "    # Map each condition to its tech and efficiency\n",
    "    tech = np.select(conditions, [pair[0] for pair in tech_eff_pairs], default='unknown')\n",
    "    eff = np.select(conditions, [pair[1] for pair in tech_eff_pairs], default='unknown')\n",
    "\n",
    "    # # Debug: Print the mapped tech and efficiency pairs\n",
    "    # print(\"Mapped Tech:\", tech)\n",
    "    # print(\"Mapped Efficiency:\", eff)\n",
    "\n",
    "    # Filter out rows with unknown technology and efficiency\n",
    "    valid_indices = tech != 'unknown'\n",
    "    tech = tech[valid_indices]\n",
    "    eff = eff[valid_indices]\n",
    "    df_valid = df_copy.loc[valid_indices].copy()\n",
    "\n",
    "    # # Debug: Print the valid indices and corresponding tech-efficiency pairs\n",
    "    # print(\"Valid Indices:\", valid_indices)\n",
    "    # print(\"Valid Tech:\", tech)\n",
    "    # print(\"Valid Efficiency:\", eff)\n",
    "\n",
    "    # Initialize dictionary to store sampled costs\n",
    "    sampled_costs_dict = {}\n",
    "\n",
    "    # Calculate costs for each component (normalized_cost)\n",
    "    for cost_component in ['normalized_cost']:\n",
    "        progressive_costs = np.array([cost_dict.get((t, e), {}).get(f'{cost_component}_progressive', np.nan) for t, e in zip(tech, eff)])\n",
    "        reference_costs = np.array([cost_dict.get((t, e), {}).get(f'{cost_component}_reference', np.nan) for t, e in zip(tech, eff)])\n",
    "        conservative_costs = np.array([cost_dict.get((t, e), {}).get(f'{cost_component}_conservative', np.nan) for t, e in zip(tech, eff)])\n",
    "\n",
    "        # Handle missing cost data\n",
    "        if np.isnan(progressive_costs).any() or np.isnan(reference_costs).any() or np.isnan(conservative_costs).any():\n",
    "            missing_indices = np.where(np.isnan(progressive_costs) | np.isnan(reference_costs) | np.isnan(conservative_costs))\n",
    "            print(f\"Missing data at indices: {missing_indices}\")\n",
    "            print(f\"Tech with missing data: {tech[missing_indices]}\")\n",
    "            print(f\"Efficiencies with missing data: {eff[missing_indices]}\")\n",
    "            \n",
    "            raise ValueError(f\"Missing cost data for some technology and efficiency combinations in cost_component {cost_component}\")\n",
    "\n",
    "        # Calculate mean and standard deviation assuming the costs represent the 10th, 50th, and 90th percentiles of a normal distribution\n",
    "        mean_costs = reference_costs\n",
    "        std_costs = (conservative_costs - progressive_costs) / (norm.ppf(0.90) - norm.ppf(0.10))\n",
    "\n",
    "        # Sample from the normal distribution for each row\n",
    "        sampled_costs = np.random.normal(loc=mean_costs, scale=std_costs)\n",
    "        sampled_costs_dict[cost_component] = sampled_costs\n",
    "\n",
    "    # Calculate the retrofit cost for each row\n",
    "    retrofit_cost = (\n",
    "        sampled_costs_dict['normalized_cost'] * df_valid[params_col]\n",
    "    ) * (df_valid['rsMeans_CCI_avg'] / rsMeans_national_avg)\n",
    "\n",
    "    # Add the calculated costs to a new DataFrame, rounded to 2 decimal places\n",
    "    df_new_columns = pd.DataFrame({retrofit_col: np.round(retrofit_cost, 2)}, index=df_valid.index)\n",
    "\n",
    "    # Identify overlapping columns between the new and existing DataFrame\n",
    "    overlapping_columns = df_new_columns.columns.intersection(df_copy.columns)\n",
    "\n",
    "    # Drop overlapping columns from the original DataFrame\n",
    "    if not overlapping_columns.empty:\n",
    "        df_copy.drop(columns=overlapping_columns, inplace=True)\n",
    "\n",
    "    # Merge new columns into the original DataFrame, ensuring no duplicates or overwrites occur\n",
    "    df_copy = df_copy.join(df_new_columns, how='left')\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75b9aba",
   "metadata": {},
   "source": [
    "# Storing Output Results and Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9df5eb",
   "metadata": {},
   "source": [
    "## Save Results: Merge DFs and Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "454a5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df_merge(df_compare, df_results_IRA, df_results_IRA_gridDecarb):\n",
    "    # Identify common columns (excluding 'bldg_id' which is the merging key)\n",
    "    common_columns_IRA = set(df_compare.columns) & set(df_results_IRA.columns)\n",
    "    common_columns_IRA.discard('bldg_id')\n",
    "        \n",
    "    # Drop duplicate columns in df_results_IRA and merge\n",
    "    df_results_IRA = df_results_IRA.drop(columns=common_columns_IRA)\n",
    "    print(f\"\"\"Dropped the following duplicate columns before merge: \n",
    "    {common_columns_IRA}\n",
    "    \"\"\")\n",
    "    merged_df = pd.merge(df_compare, df_results_IRA, on='bldg_id', how='inner')\n",
    "\n",
    "    # Repeat the steps above for the merged_df and df_results_IRA_gridDecarb\n",
    "    common_columnsIRA_gridDecarb = set(merged_df.columns) & set(df_results_IRA_gridDecarb.columns)\n",
    "    common_columnsIRA_gridDecarb.discard('bldg_id')\n",
    "    df_results_IRA_gridDecarb = df_results_IRA_gridDecarb.drop(columns=common_columnsIRA_gridDecarb)\n",
    "    print(f\"\"\"Dropped the following duplicate columns before merge: \n",
    "    {common_columnsIRA_gridDecarb}\n",
    "    \"\"\")\n",
    "        \n",
    "    # Create cleaned, merged results df with no duplicate columns\n",
    "    df_results_export = pd.merge(merged_df, df_results_IRA_gridDecarb, on='bldg_id', how='inner')\n",
    "    print(\"Dataframes have been cleaned of duplicate columns and merged successfully. Ready to export!\")\n",
    "    return df_results_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d09ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model_run_output(df_results_export, results_category, menu_mp):\n",
    "    \"\"\"\n",
    "    Exports data for results summaries (npv, adoption, impact) and supplemental info (consumption, damages, fuel costs)\n",
    "\n",
    "    Parameters:\n",
    "    df_results_export (pd.DataFrame): DataFrame containing data for different scenarios.\n",
    "    results_category (str): Determines the type of info being exported.\n",
    "        - Accepted: 'summary', 'consumption', 'damages', 'fuelCost'\n",
    "    menu_mp (int or str): Determines the measure package or retrofit being conducted\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    # Baseline model run results\n",
    "    if results_category == 'summary':\n",
    "        if menu_mp == '0' or menu_mp==0:\n",
    "            results_filename = f\"baseline_results_{location_id}_{results_export_formatted_date}.csv\"\n",
    "            print(f\"BASELINE RESULTS:\")\n",
    "            print(f\"Dataframe results will be saved in this csv file: {results_filename}\")\n",
    "\n",
    "            # Change the directory to the upload folder and export the file\n",
    "            results_change_directory = \"baseline_summary\"\n",
    "\n",
    "        # Measure Package model run results\n",
    "        else:\n",
    "            if menu_mp == '8' or menu_mp==8:\n",
    "                print(f\"MEASURE PACKAGE {menu_mp} (MP{menu_mp}) RESULTS:\")\n",
    "                results_filename = f\"mp{menu_mp}_results_{location_id}_{results_export_formatted_date}.csv\"\n",
    "                print(f\"Dataframe results will be saved in this csv file: {results_filename}\")\n",
    "\n",
    "                # Change the directory to the upload folder and export the file\n",
    "                results_change_directory = \"retrofit_basic_summary\"\n",
    "\n",
    "            elif menu_mp == '9' or menu_mp==9:\n",
    "                results_filename = f\"mp{menu_mp}_results_{location_id}_{results_export_formatted_date}.csv\"\n",
    "                print(f\"MEASURE PACKAGE {menu_mp} (MP{menu_mp}) RESULTS:\")\n",
    "                print(f\"Dataframe results will be saved in this csv file: {results_filename}\")\n",
    "\n",
    "                # Change the directory to the upload folder and export the file\n",
    "                results_change_directory = \"retrofit_moderate_summary\"\n",
    "\n",
    "            elif menu_mp == '10' or menu_mp==10:\n",
    "                results_filename = f\"mp{menu_mp}_results_{location_id}_{results_export_formatted_date}.csv\"\n",
    "                print(f\"MEASURE PACKAGE {menu_mp} (MP{menu_mp}) RESULTS:\")\n",
    "                print(f\"Dataframe results will be saved in this csv file: {results_filename}\")\n",
    "\n",
    "                # Change the directory to the upload folder and export the file\n",
    "                results_change_directory = \"retrofit_advanced_summary\"\n",
    "\n",
    "            else:\n",
    "                print(\"No matching scenarios for this Measure Package (MP)\")\n",
    "\n",
    "    # This includes exported dataframes for calculated consumption, damages, and fuel costs\n",
    "    else:\n",
    "        results_filename = f\"mp{menu_mp}_data_{results_category}_{location_id}_{results_export_formatted_date}.csv\"\n",
    "        print(f\"SUPPLEMENTAL INFORMATION DATAFRAME: {results_category}\")\n",
    "        print(f\"Dataframe results will be saved in this csv file: {results_filename}\")\n",
    "\n",
    "        # Change the directory to the upload folder and export the file\n",
    "        results_change_directory = f\"supplemental_data_{results_category}\"\n",
    "\n",
    "    # Export dataframe results as a csv to the specified filepath\n",
    "    results_export_filepath = os.path.join(output_folder_path, results_change_directory, results_filename)\n",
    "    os.makedirs(results_export_filepath, exist_ok=True)\n",
    "    df_results_export.to_csv(results_export_filepath)\n",
    "    print(f\"Dataframe for MP{menu_mp} {results_category} results were exported here: {results_export_filepath}\")\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b40e7",
   "metadata": {},
   "source": [
    "## Convert Results Output CSVs to Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6ca945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scenario_data(end_use, output_folder_path, scenario_string, model_run_date_time, columns_to_string):\n",
    "    # Construct the output folder path with the policy_scenario of interest\n",
    "    scenario_folder_path = os.path.join(output_folder_path, scenario_string)\n",
    "    print(f\"Output Results Folder Path: {scenario_folder_path}\")\n",
    "\n",
    "    # List all files in the specified folder with the specified date in the filename\n",
    "    files = [f for f in os.listdir(scenario_folder_path) if os.path.isfile(os.path.join(scenario_folder_path, f)) and model_run_date_time in f]\n",
    "\n",
    "    # Initialize dataframe as None\n",
    "    df_outputs = None\n",
    "\n",
    "    # Assume there is one main file per policy_scenario that includes all necessary data\n",
    "    if files:\n",
    "        file_path = os.path.join(scenario_folder_path, files[0])  # Assumes the first file is the correct one\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            df_outputs = pd.read_csv(file_path, index_col=0, dtype=columns_to_string)\n",
    "            print(f\"Loaded {end_use} data for policy_scenario '{scenario_string}'\", \"\\n\")\n",
    "        else:\n",
    "            print(\"File not found for the specified policy_scenario\", \"\\n\")\n",
    "\n",
    "    if df_outputs is None:\n",
    "        print(f\"No {end_use} data found for policy_scenario '{scenario_string}'\")\n",
    "\n",
    "    return df_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6b00a",
   "metadata": {},
   "source": [
    "## Visuals for Public and Private Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "629494a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added base fuel color-coded legend\n",
    "# Possibly update colors to make more color blind accessible\n",
    "color_map_fuel = {\n",
    "    'Electricity': 'seagreen',\n",
    "    'Natural Gas': 'steelblue',\n",
    "    'Propane': 'orange',\n",
    "    'Fuel Oil': 'firebrick',\n",
    "}\n",
    "\n",
    "# Define a function to plot the histogram and percentile subplot\n",
    "def create_subplot_histogram(ax, df, x_col, bin_number, x_label=None, y_label=None, lower_percentile=2.5, upper_percentile=97.5, color_code='base_fuel', statistic='count', include_zero=False, show_legend=False):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    if not include_zero:\n",
    "        df_copy[x_col] = df_copy[x_col].replace(0, np.nan)\n",
    "\n",
    "    lower_limit = df_copy[x_col].quantile(lower_percentile / 100)\n",
    "    upper_limit = df_copy[x_col].quantile(upper_percentile / 100)\n",
    "\n",
    "    valid_data = df_copy[x_col][(df_copy[x_col] >= lower_limit) & (df_copy[x_col] <= upper_limit)]\n",
    "\n",
    "    # Get the corresponding color for each fuel category\n",
    "    colors = [color_map_fuel.get(fuel, 'gray') for fuel in df_copy[color_code].unique()]\n",
    "\n",
    "    # Set the hue_order to match the unique fuel categories and their corresponding colors\n",
    "    hue_order = [fuel for fuel in df_copy[color_code].unique() if fuel in color_map_fuel]\n",
    "\n",
    "    ax = sns.histplot(data=df_copy, x=valid_data, kde=False, bins=bin_number, hue=color_code, hue_order=hue_order, stat=statistic, multiple=\"stack\", palette=colors, ax=ax, legend=show_legend)\n",
    "\n",
    "    if x_label is not None:\n",
    "        ax.set_xlabel(x_label, fontsize=22)  # Set font size for x-axis label\n",
    "\n",
    "    if y_label is not None:\n",
    "        ax.set_ylabel(y_label, fontsize=22)  # Set font size for y-axis label\n",
    "\n",
    "    ax.set_xlim(left=lower_limit, right=upper_limit)\n",
    "\n",
    "    # Set font size for tick labels\n",
    "    ax.tick_params(axis='both', labelsize=22)\n",
    "\n",
    "    sns.despine()\n",
    "\n",
    "def create_subplot_grid_histogram(df, subplot_positions, x_cols, x_labels, y_label=None, bin_number=20, lower_percentile=2.5, upper_percentile=97.5, statistic='count', color_code='base_fuel', include_zero=False, suptitle=None, sharex=False, sharey=False, column_titles=None, show_legend=True, figure_size=(12, 10), export_filename=None, export_format='png', dpi=300):\n",
    "    num_subplots = len(subplot_positions)\n",
    "    num_cols = max(pos[1] for pos in subplot_positions) + 1\n",
    "    num_rows = max(pos[0] for pos in subplot_positions) + 1\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=figure_size, sharex=sharex, sharey=sharey)\n",
    "\n",
    "    # Create a dictionary to map subplot positions to their respective axes\n",
    "    subplot_axes = {(pos[0], pos[1]): axes[pos[0], pos[1]] for pos in subplot_positions}\n",
    "\n",
    "    # Define the parameters for each histogram subplot\n",
    "    plot_params = [{'ax': subplot_axes[pos], 'x_col': col, 'x_label': label, 'y_label': y_label, 'bin_number': bin_number, 'lower_percentile': lower_percentile, 'upper_percentile': upper_percentile, 'statistic': statistic, 'color_code': color_code, 'include_zero': include_zero, 'show_legend': show_legend}\n",
    "                   for pos, col, label in zip(subplot_positions, x_cols, x_labels)]\n",
    "\n",
    "    # Plot each histogram subplot using the defined parameters\n",
    "    for params in plot_params:\n",
    "        create_subplot_histogram(df=df, **params)\n",
    "\n",
    "    # Add a super title to the entire figure if suptitle is provided\n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle, fontweight='bold')\n",
    "\n",
    "    # Add titles over the columns\n",
    "    if column_titles:\n",
    "        for col_index, title in enumerate(column_titles):\n",
    "            axes[0, col_index].set_title(title, fontsize=22, fontweight='bold')\n",
    "    \n",
    "    # If sharey is True, remove y-axis labels on all subplots except the leftmost ones in each row\n",
    "    if sharey:\n",
    "        for row_index in range(num_rows):\n",
    "            for col_index in range(num_cols):\n",
    "                if col_index > 0:\n",
    "                    axes[row_index, col_index].set_yticklabels([])\n",
    "\n",
    "    # Add a legend for the color mapping at the bottom of the entire figure\n",
    "    legend_labels = list(color_map_fuel.keys())\n",
    "    legend_handles = [plt.Rectangle((0, 0), 1, 1, color=color_map_fuel[label]) for label in legend_labels]\n",
    "    fig.legend(legend_handles, legend_labels, loc='lower center', ncol=len(legend_labels), prop={'size': 22}, labelspacing=0.5, bbox_to_anchor=(0.5, -0.05))             \n",
    "    \n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Export the figure if export_filename is provided\n",
    "    if export_filename:\n",
    "        save_figure_path = os.path.join(save_figure_directory, export_filename)\n",
    "        plt.savefig(save_figure_path, format=export_format, dpi=dpi)\n",
    "    # Otherwise show the plot in Jupyter Notebook\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a177b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LAST UPDATED SEPTEMBER 12, 2024\n",
    "# def subplot_grid_co2_abatement(dataframes, subplot_positions, epa_scc_values, x_cols, y_cols, hues, plot_titles=None, x_labels=None, y_labels=None, suptitle=None, figure_size=(12, 10), sharex=False, sharey=False):\n",
    "#     \"\"\"\n",
    "#     Creates a grid of subplots to visualize CO2 abatement cost effectiveness across different datasets and scenarios.\n",
    "#     \"\"\"\n",
    "#     num_subplots = len(subplot_positions)\n",
    "#     num_cols = max(pos[1] for pos in subplot_positions) + 1\n",
    "#     num_rows = max(pos[0] for pos in subplot_positions) + 1\n",
    "\n",
    "#     fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=figure_size, sharex=sharex, sharey=sharey)\n",
    "#     axes = np.array(axes).reshape(num_rows, num_cols)  # Ensure axes is always 2D\n",
    "\n",
    "#     for idx, (df, epa_scc, x_col, y_col, hue) in enumerate(zip(dataframes, epa_scc_values, x_cols, y_cols, hues)):\n",
    "#         pos = subplot_positions[idx]\n",
    "#         ax = axes[pos[0], pos[1]]\n",
    "#         title = plot_titles[idx] if plot_titles else \"\"\n",
    "#         x_label = x_labels[idx] if x_labels else \"\"\n",
    "#         y_label = y_labels[idx] if y_labels else \"\"\n",
    "\n",
    "#         # Plot using the plot_co2_abatement function, passing the current axis to it\n",
    "#         plot_co2_abatement(df, x_col, y_col, hue, epa_scc, ax=ax)\n",
    "\n",
    "#         # Set custom labels and title if provided\n",
    "#         ax.set_xlabel(x_label, fontweight='bold', fontsize=18)\n",
    "#         ax.set_ylabel(y_label, fontweight='bold', fontsize=18)\n",
    "#         ax.set_title(title, fontweight='bold', fontsize=18)\n",
    "\n",
    "#         # Set font size for tick labels on the x-axis\n",
    "#         ax.tick_params(axis='x', labelsize=18)\n",
    "\n",
    "#         # Set font size for tick labels on the y-axis\n",
    "#         ax.tick_params(axis='y', labelsize=18)\n",
    "\n",
    "#     if suptitle:\n",
    "#         plt.suptitle(suptitle, fontweight='bold')\n",
    "\n",
    "#     # Create a consolidated legend by grabbing handles and labels from all subplots\n",
    "#     handles, labels = [], []\n",
    "#     for ax in axes.flatten():\n",
    "#         for handle, label in zip(*ax.get_legend_handles_labels()):\n",
    "#             if label not in labels:  # Avoid duplicates\n",
    "#                 handles.append(handle)\n",
    "#                 labels.append(label)\n",
    "\n",
    "#     # # Add the consolidated legend outside the plots\n",
    "#     # fig.legend(handles, labels, loc='lower center', ncol=5, prop={'size': 18}, labelspacing=0.25, bbox_to_anchor=(0.5, -0.01))\n",
    "\n",
    "#     # # Adjust the layout\n",
    "#     # plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout to leave space for the suptitle\n",
    "\n",
    "#     # Add the consolidated legend outside the plots\n",
    "#     fig.legend(handles, labels, loc='lower center', ncol=5, prop={'size': 16}, labelspacing=0.25, handletextpad=1, columnspacing=1, bbox_to_anchor=(0.5, -0.01), bbox_transform=fig.transFigure)\n",
    "\n",
    "#     # Fine-tune the layout adjustment if needed\n",
    "#     plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjusted the rect to leave space for the suptitle and legend\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# def plot_co2_abatement(df, x_col, y_col, hue, epa_scc_usd2023_per_ton, ax=None):\n",
    "#     \"\"\"\n",
    "#     Plots a boxplot of CO2 abatement cost effectiveness.\n",
    "\n",
    "#     Parameters:\n",
    "#     - df: DataFrame containing the data.\n",
    "#     - x_col: Column name for the x-axis.\n",
    "#     - y_col: Column name for the y-axis.\n",
    "#     - hue: Column name for the hue (categorical variable for color).\n",
    "#     - epa_scc_usd2023_per_ton: Value for the red dashed line indicating SCC.\n",
    "#     - ax: Axis object to plot on. If None, creates a new plot.\n",
    "    \n",
    "#     Returns:\n",
    "#     - None: Displays the plot.\n",
    "#     \"\"\"\n",
    "#     # Filter out the 'Middle-to-Upper-Income' rows\n",
    "#     df_filtered = df[df[x_col] != 'Middle-to-Upper-Income']\n",
    "\n",
    "#     # Color map for fuel types\n",
    "#     color_map_fuel = {\n",
    "#         'Electricity': 'seagreen',\n",
    "#         'Natural Gas': 'steelblue',\n",
    "#         'Propane': 'orange',\n",
    "#         'Fuel Oil': 'firebrick',\n",
    "#     }\n",
    "\n",
    "#     if ax is None:\n",
    "#         ax = plt.gca()\n",
    "\n",
    "#     # Create the boxplot\n",
    "#     sns.boxplot(\n",
    "#         data=df_filtered,\n",
    "#         x=x_col, \n",
    "#         y=y_col, \n",
    "#         hue=hue, \n",
    "#         palette=color_map_fuel, \n",
    "#         showfliers=False,\n",
    "#         width=0.8,\n",
    "#         ax=ax\n",
    "#     )\n",
    "\n",
    "#     # Add a red dashed line at the value of epa_scc_usd2023_per_ton\n",
    "#     ax.axhline(y=epa_scc_usd2023_per_ton, color='red', linestyle='--', linewidth=2, label=f'SCC (USD2023): ${int(round((epa_scc_usd2023_per_ton), 0))}/mtCO2e')\n",
    "\n",
    "#     # Remove the individual legend for each subplot\n",
    "#     ax.legend_.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7441c5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAST UPDATED SEPTEMBER 19, 2024\n",
    "def subplot_grid_co2_abatement(dataframes, subplot_positions, epa_scc_values, x_cols, y_cols, hues, plot_titles=None, x_labels=None, y_labels=None, suptitle=None, figure_size=(12, 10), sharex=False, sharey=False):\n",
    "    \"\"\"\n",
    "    Creates a grid of subplots to visualize CO2 abatement cost effectiveness across different datasets and scenarios.\n",
    "    \"\"\"\n",
    "    num_cols = max(pos[1] for pos in subplot_positions) + 1\n",
    "    num_rows = max(pos[0] for pos in subplot_positions) + 1\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=figure_size, sharex=sharex, sharey=sharey)\n",
    "    axes = np.array(axes).reshape(num_rows, num_cols)  # Ensure axes is always 2D\n",
    "\n",
    "    for idx, (df, epa_scc, x_col, y_col, hue) in enumerate(zip(dataframes, epa_scc_values, x_cols, y_cols, hues)):\n",
    "        pos = subplot_positions[idx]\n",
    "        ax = axes[pos[0], pos[1]]\n",
    "        title = plot_titles[idx] if plot_titles else \"\"\n",
    "        x_label = x_labels[idx] if x_labels else \"\"\n",
    "        y_label = y_labels[idx] if y_labels else \"\"\n",
    "\n",
    "        # Plot using the plot_co2_abatement function, passing the current axis to it\n",
    "        plot_co2_abatement(df, x_col, y_col, hue, epa_scc, ax=ax)\n",
    "\n",
    "        # Set custom labels and title if provided\n",
    "        ax.set_xlabel(x_label, fontweight='bold', fontsize=18)\n",
    "        ax.set_ylabel(y_label, fontweight='bold', fontsize=18)\n",
    "        ax.set_title(title, fontweight='bold', fontsize=18)\n",
    "\n",
    "        # Set font size for tick labels on the x-axis\n",
    "        ax.tick_params(axis='x', labelsize=18)\n",
    "\n",
    "        # Set font size for tick labels on the y-axis\n",
    "        ax.tick_params(axis='y', labelsize=18)\n",
    "\n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle, fontweight='bold')\n",
    "\n",
    "    # Create a consolidated legend by grabbing handles and labels from all subplots\n",
    "    handles, labels = [], []\n",
    "    for ax in axes.flatten():\n",
    "        for handle, label in zip(*ax.get_legend_handles_labels()):\n",
    "            if label not in labels:  # Avoid duplicates\n",
    "                handles.append(handle)\n",
    "                labels.append(label)\n",
    "\n",
    "    # # Add the consolidated legend outside the plots\n",
    "    # fig.legend(handles, labels, loc='lower center', ncol=5, prop={'size': 18}, labelspacing=0.25, bbox_to_anchor=(0.5, -0.01))\n",
    "\n",
    "    # # Adjust the layout\n",
    "    # plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout to leave space for the suptitle\n",
    "\n",
    "    # Add the consolidated legend outside the plots\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=5, prop={'size': 16}, labelspacing=0.25, handletextpad=1, columnspacing=1, bbox_to_anchor=(0.5, -0.05), bbox_transform=fig.transFigure)\n",
    "\n",
    "    # Fine-tune the layout adjustment if needed\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjusted the rect to leave space for the suptitle and legend\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_co2_abatement(df, x_col, y_col, hue, epa_scc_usd2023_per_ton, ax=None):\n",
    "    \"\"\"\n",
    "    Plots a boxplot of CO2 abatement cost effectiveness.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - x_col: Column name for the x-axis.\n",
    "    - y_col: Column name for the y-axis.\n",
    "    - hue: Column name for the hue (categorical variable for color).\n",
    "    - epa_scc_usd2023_per_ton: Value for the red dashed line indicating SCC.\n",
    "    - ax: Axis object to plot on. If None, creates a new plot.\n",
    "    \n",
    "    Returns:\n",
    "    - None: Displays the plot.\n",
    "    \"\"\"\n",
    "    # Filter out the 'Middle-to-Upper-Income' rows and create a copy to avoid SettingWithCopyWarning\n",
    "    df_copy = df.copy()\n",
    "    df_filtered = df_copy[df_copy[x_col] != 'Middle-to-Upper-Income']\n",
    "\n",
    "    # If x_col is categorical, remove unused categories\n",
    "    if df_filtered[x_col].dtype.name == 'category':\n",
    "        df_filtered.loc[:, x_col] = df_filtered[x_col].cat.remove_unused_categories()\n",
    "\n",
    "    # Color map for fuel types\n",
    "    color_map_fuel = {\n",
    "        'Electricity': 'seagreen',\n",
    "        'Natural Gas': 'steelblue',\n",
    "        'Propane': 'orange',\n",
    "        'Fuel Oil': 'firebrick',\n",
    "    }\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # Create the boxplot\n",
    "    sns.boxplot(\n",
    "        data=df_filtered,\n",
    "        x=x_col, \n",
    "        y=y_col, \n",
    "        hue=hue, \n",
    "        palette=color_map_fuel, \n",
    "        showfliers=False,\n",
    "        width=0.8,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    # Add a red dashed line at the value of epa_scc_usd2023_per_ton\n",
    "    ax.axhline(y=epa_scc_usd2023_per_ton, color='red', linestyle='--', linewidth=2, label=f'SCC (USD2023): ${int(round((epa_scc_usd2023_per_ton), 0))}/mtCO2e')\n",
    "\n",
    "    # Remove the individual legend for each subplot\n",
    "    ax.legend_.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7991dd9f",
   "metadata": {},
   "source": [
    "# Adoption Rate Scenario Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea827eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LAST UPDATED SEPTEMBER 9, 2024 @ 12:45 AM\n",
    "# def create_df_adoption(df, menu_mp):\n",
    "#     \"\"\"\n",
    "#     Generates a new DataFrame with specific adoption columns based on provided parameters.\n",
    "    \n",
    "#     Args:\n",
    "#     df (pd.DataFrame): Original DataFrame.\n",
    "#     menu_mp (int): Measure package identifier.\n",
    "\n",
    "#     Returns:\n",
    "#     pd.DataFrame: A DataFrame with the selected columns.\n",
    "#     \"\"\"    \n",
    "#     # Create a copy of the dataframe\n",
    "#     df_copy = df.copy()\n",
    "\n",
    "#     # Begin df with these cols\n",
    "#     df_copy['scc_usd2023_per_ton'] = np.round(epa_scc_usd2023_per_ton, 2)\n",
    "\n",
    "#     summary_cols = ['bldg_id', 'state', 'city', 'county', 'puma', 'percent_AMI', 'lowModerateIncome_designation', 'scc_usd2023_per_ton']\n",
    "\n",
    "#     # for category in ['heating', 'waterHeating', 'clothesDrying', 'cooking']:\n",
    "#     for category in ['heating', 'waterHeating']:\n",
    "#         df_copy[f'iraRef_{category}_usd2023_per_mtCO2e'] = round((df_copy[f'mp{menu_mp}_{category}_rebate_amount'] / df_copy[f'iraRef_mp{menu_mp}_{category}_avoided_tons_co2e']), 2)\n",
    "        \n",
    "#         cols_to_add = [\n",
    "#             f'base_{category}_fuel',\n",
    "#             f'mp{menu_mp}_{category}_rebate_amount', \n",
    "#             f'iraRef_mp{menu_mp}_{category}_avoided_tons_co2e', \n",
    "#             f'iraRef_{category}_usd2023_per_mtCO2e',\n",
    "#             f'iraRef_mp{menu_mp}_{category}_public_npv',\n",
    "#             f'iraRef_mp{menu_mp}_{category}_private_npv_lessWTP', \n",
    "#             f'iraRef_mp{menu_mp}_{category}_total_capitalCost', \n",
    "#             f'iraRef_mp{menu_mp}_{category}_private_npv_moreWTP', \n",
    "#             f'iraRef_mp{menu_mp}_{category}_net_capitalCost',\n",
    "#             f'iraRef_mp{menu_mp}_{category}_adoption'\n",
    "#         ]\n",
    "        \n",
    "#         # Use extend instead of append to add each element of cols_to_add to summary_cols\n",
    "#         summary_cols.extend(cols_to_add)\n",
    "        \n",
    "#     # Select the relevant columns\n",
    "#     df_copy = df_copy[summary_cols]\n",
    "\n",
    "#     return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAST UPDATED SEPTEMBER 14, 2024 @ 1:00 AM\n",
    "def create_df_adoption(df, menu_mp, category):\n",
    "    \"\"\"\n",
    "    Generates a new DataFrame with specific adoption columns based on provided parameters.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): Original DataFrame.\n",
    "    menu_mp (int): Measure package identifier.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with the selected columns.\n",
    "    \"\"\"    \n",
    "    # Create a copy of the dataframe\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Begin df with these cols\n",
    "    df_copy['scc_usd2023_per_ton'] = np.round(epa_scc_usd2023_per_ton, 2)\n",
    "\n",
    "    summary_cols = ['bldg_id', 'state', 'city', 'county', 'puma', 'percent_AMI', 'lowModerateIncome_designation', 'scc_usd2023_per_ton']\n",
    "\n",
    "    df_copy[f'iraRef_{category}_usd2023_per_mtCO2e'] = round((df_copy[f'mp{menu_mp}_{category}_rebate_amount'] / df_copy[f'iraRef_mp{menu_mp}_{category}_avoided_tons_co2e']), 2)\n",
    "            \n",
    "    cols_to_add = [f'base_{category}_fuel',\n",
    "                   f'preIRA_mp{menu_mp}_{category}_avoided_tons_co2e', \n",
    "                   f'preIRA_mp{menu_mp}_{category}_public_npv',\n",
    "                   f'preIRA_mp{menu_mp}_{category}_private_npv_lessWTP', \n",
    "                   f'preIRA_mp{menu_mp}_{category}_total_capitalCost', \n",
    "                   f'preIRA_mp{menu_mp}_{category}_private_npv_moreWTP', \n",
    "                   f'preIRA_mp{menu_mp}_{category}_net_capitalCost',\n",
    "                   f'preIRA_mp{menu_mp}_{category}_adoption',\n",
    "                   f'mp{menu_mp}_{category}_rebate_amount', \n",
    "                   f'iraRef_mp{menu_mp}_{category}_avoided_tons_co2e', \n",
    "                   f'iraRef_{category}_usd2023_per_mtCO2e',\n",
    "                   f'iraRef_mp{menu_mp}_{category}_public_npv',\n",
    "                   f'iraRef_mp{menu_mp}_{category}_additional_public_benefit',\n",
    "                   f'iraRef_mp{menu_mp}_{category}_private_npv_lessWTP', \n",
    "                   f'iraRef_mp{menu_mp}_{category}_total_capitalCost', \n",
    "                   f'iraRef_mp{menu_mp}_{category}_private_npv_moreWTP', \n",
    "                   f'iraRef_mp{menu_mp}_{category}_net_capitalCost',\n",
    "                   f'iraRef_mp{menu_mp}_{category}_adoption'\n",
    "                   ]\n",
    "            \n",
    "    # Use extend instead of append to add each element of cols_to_add to summary_cols\n",
    "    summary_cols.extend(cols_to_add)\n",
    "\n",
    "    # Select the relevant columns\n",
    "    df_copy = df_copy[summary_cols]\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cbd92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATED SEPTEMBER 14, 2024 @ 5:00 PM\n",
    "import pandas as pd\n",
    "\n",
    "def filter_columns(df):\n",
    "    keep_columns = [col for col in df.columns if 'Tier 1: Feasible' in col[1] or \n",
    "                    'Tier 2: Feasible vs. Alternative' in col[1] or \n",
    "                    'Tier 3: Subsidy-Dependent Feasibility' in col[1] or \n",
    "                    'Total Adoption Potential' in col[1] or \n",
    "                    'Total Adoption Potential (Additional Subsidy)' in col[1]]    \n",
    "    \n",
    "    return df.loc[:, keep_columns]\n",
    "\n",
    "def create_multiIndex_adoption_df(df, menu_mp, category):\n",
    "    # Explicitly set 'lowModerateIncome_designation' as a categorical type with order\n",
    "    income_categories = ['Low-Income', 'Moderate-Income', 'Middle-to-Upper-Income']\n",
    "\n",
    "    df['lowModerateIncome_designation'] = pd.Categorical(df['lowModerateIncome_designation'], categories=income_categories, ordered=True)\n",
    "    \n",
    "    # Define the columns for adoption data\n",
    "    adoption_cols = [f'preIRA_mp{menu_mp}_{category}_adoption', \n",
    "                     f'iraRef_mp{menu_mp}_{category}_adoption']\n",
    "\n",
    "    # Group by f'base_{category}_fuel' and 'lowModerateIncome_designation', calculate normalized counts\n",
    "    percentages_df = df.groupby([f'base_{category}_fuel', 'lowModerateIncome_designation'], observed=False)[adoption_cols].apply(\n",
    "        lambda x: x.apply(lambda y: y.value_counts(normalize=True))).unstack().fillna(0) * 100\n",
    "    percentages_df = percentages_df.round(0)\n",
    "\n",
    "    # Ensure 'Tier 1: Feasible' columns exist, set to 0 if they don't\n",
    "    for column in adoption_cols:\n",
    "        if (column, 'Tier 1: Feasible') not in percentages_df.columns:\n",
    "            percentages_df[(column, 'Tier 1: Feasible')] = 0\n",
    "        if (column, 'Tier 2: Feasible vs. Alternative') not in percentages_df.columns:\n",
    "            percentages_df[(column, 'Tier 2: Feasible vs. Alternative')] = 0\n",
    "        if (column, 'Tier 3: Subsidy-Dependent Feasibility') not in percentages_df.columns:\n",
    "            percentages_df[(column, 'Tier 3: Subsidy-Dependent Feasibility')] = 0\n",
    "\n",
    "        percentages_df[(column, 'Total Adoption Potential')] = (\n",
    "            percentages_df[(column, 'Tier 1: Feasible')] + \n",
    "            percentages_df[(column, 'Tier 2: Feasible vs. Alternative')]\n",
    "        )\n",
    "\n",
    "        percentages_df[(column, 'Total Adoption Potential (Additional Subsidy)')] = (\n",
    "            percentages_df[(column, 'Tier 1: Feasible')] + \n",
    "            percentages_df[(column, 'Tier 2: Feasible vs. Alternative')] + \n",
    "            percentages_df[(column, 'Tier 3: Subsidy-Dependent Feasibility')]\n",
    "        )\n",
    "\n",
    "    # Rebuild the column MultiIndex\n",
    "    percentages_df.columns = pd.MultiIndex.from_tuples(percentages_df.columns)\n",
    "    \n",
    "    # Filter DataFrame to keep relevant columns only\n",
    "    filtered_df = filter_columns(percentages_df)\n",
    "\n",
    "    new_order = []\n",
    "    for prefix in ['preIRA_mp', 'iraRef_mp']:\n",
    "        for suffix in ['Tier 1: Feasible', 'Tier 2: Feasible vs. Alternative', 'Tier 3: Subsidy-Dependent Feasibility', 'Total Adoption Potential', 'Total Adoption Potential (Additional Subsidy)']:\n",
    "            col = (f'{prefix}{menu_mp}_{category}_adoption', suffix)\n",
    "            if col in filtered_df.columns:\n",
    "                new_order.append(col)\n",
    "\n",
    "    # Check if new_order is empty before reordering columns\n",
    "    if new_order:\n",
    "        # Reorder columns based on new_order\n",
    "        filtered_df = filtered_df.loc[:, pd.MultiIndex.from_tuples(new_order)]\n",
    "                    \n",
    "        # Sort DataFrame by the entire index\n",
    "        filtered_df.sort_index(level=[f'base_{category}_fuel', 'lowModerateIncome_designation'], inplace=True)\n",
    "    else:\n",
    "        print(\"Warning: No matching columns found for reordering\")\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# Usage example (assuming df_basic_adoption_heating is properly formatted and loaded):\n",
    "# df_multiIndex_heating_adoption = create_multiIndex_adoption_df(df_basic_adoption_heating, 8, 'heating')\n",
    "# df_multiIndex_heating_adoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b81f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def filter_columns(df):\n",
    "#     keep_columns = [col for col in df.columns if 'Tier 1: Feasible' in col[1] or 'Tier 2: Feasible vs. Alternative' in col[1] or 'Tier 2: Feasible vs. Alternative' in col[1] or 'Tier 3: Subsidy-Dependent Feasibility' in col[1]]\n",
    "#     return df.loc[:, keep_columns]\n",
    "\n",
    "# def create_multiIndex_adoption_df(df, menu_mp, category):\n",
    "#     # Explicitly set 'lowModerateIncome_designation' as a categorical type with order\n",
    "#     income_categories = ['Low-Income', 'Moderate-Income', 'Middle-to-Upper-Income']\n",
    "\n",
    "#     df['lowModerateIncome_designation'] = pd.Categorical(df['lowModerateIncome_designation'], categories=income_categories, ordered=True)\n",
    "    \n",
    "#     # Define the columns for adoption data\n",
    "#     adoption_cols = [f'preIRA_mp{menu_mp}_{category}_adoption', \n",
    "#                      f'iraRef_mp{menu_mp}_{category}_adoption']\n",
    "\n",
    "#     # Group by f'base_{category}_fuel' and 'lowModerateIncome_designation', calculate normalized counts\n",
    "#     percentages_df = df.groupby([f'base_{category}_fuel', 'lowModerateIncome_designation'], observed=False)[adoption_cols].apply(\n",
    "#         lambda x: x.apply(lambda y: y.value_counts(normalize=True))).unstack().fillna(0) * 100\n",
    "#     percentages_df = percentages_df.round(2)\n",
    "\n",
    "#     # Ensure 'Tier 1: Feasible' columns exist, set to 0 if they don't\n",
    "#     for column in adoption_cols:\n",
    "#         if (column, 'Tier 1: Feasible') not in percentages_df.columns:\n",
    "#             percentages_df[(column, 'Tier 1: Feasible')] = 0\n",
    "#         if (column, 'Tier 2: Feasible vs. Alternative') not in percentages_df.columns:\n",
    "#             percentages_df[(column, 'Tier 2: Feasible vs. Alternative')] = 0\n",
    "#         if (column, 'Tier 3: Subsidy-Dependent Feasibility') not in percentages_df.columns:\n",
    "#             percentages_df[(column, 'Tier 3: Subsidy-Dependent Feasibility')] = 0\n",
    "\n",
    "\n",
    "#     # Create 'Total Adoption with Subsidy' by combining related columns\n",
    "#     for column in adoption_cols:\n",
    "#         percentages_df[(column, 'Total Adoption with Subsidy')] = percentages_df[(column, 'Tier 1: Feasible')] + percentages_df.get((column, 'Tier 2: Feasible vs. Alternative'), 0) + percentages_df.get((column, 'Tier 3: Subsidy-Dependent Feasibility'), 0)\n",
    "\n",
    "#     # Rebuild the column MultiIndex\n",
    "#     percentages_df.columns = pd.MultiIndex.from_tuples(percentages_df.columns)\n",
    "    \n",
    "#     # Filter DataFrame to keep relevant columns only\n",
    "#     filtered_df = filter_columns(percentages_df)\n",
    "\n",
    "#     # Dynamically build the new column order based on existing columns\n",
    "#     new_order = []\n",
    "#     for prefix in ['preIRA_mp', 'iraRef_mp']:\n",
    "#         for suffix in ['Tier 1: Feasible', 'Tier 2: Feasible vs. Alternative', 'Tier 3: Subsidy-Dependent Feasibility', 'Total Adoption with Subsidy']:\n",
    "#             col = (f'{prefix}{menu_mp}_{category}_adoption', suffix)\n",
    "#             if col in filtered_df.columns:\n",
    "#                 new_order.append(col)\n",
    "\n",
    "#     # Check if new_order is empty before reordering columns\n",
    "#     if new_order:\n",
    "#         # Reorder columns based on new_order\n",
    "#         filtered_df = filtered_df.loc[:, pd.MultiIndex.from_tuples(new_order)]\n",
    "                    \n",
    "#         # Sort DataFrame by the entire index\n",
    "#         filtered_df.sort_index(level=[f'base_{category}_fuel', 'lowModerateIncome_designation'], inplace=True)\n",
    "#     else:\n",
    "#         print(\"Warning: No matching columns found for reordering\")\n",
    "\n",
    "#     return filtered_df\n",
    "\n",
    "# # Usage example (assuming df_basic_adoption_heating is properly formatted and loaded):\n",
    "# # df_multiIndex_heating_adoption = create_multiIndex_adoption_df(df_basic_adoption_heating, 8, 'heating')\n",
    "# # df_multiIndex_heating_adoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae23823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LAST UPDATED SEPTEMBER 6, 2024\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# def subplot_grid_adoption_vBar(dataframes, scenarios_list, subplot_positions, filter_fuel=None, x_labels=None, plot_titles=None, y_labels=None, suptitle=None, figure_size=(12, 10), sharex=False, sharey=False):\n",
    "#     \"\"\"\n",
    "#     Creates a grid of subplots to visualize adoption rates across different scenarios, with an option to plot specific data related to adoption.\n",
    "#     \"\"\"\n",
    "#     num_subplots = len(subplot_positions)\n",
    "#     num_cols = max(pos[1] for pos in subplot_positions) + 1\n",
    "#     num_rows = max(pos[0] for pos in subplot_positions) + 1\n",
    "\n",
    "#     fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=figure_size, sharex=sharex, sharey=sharey)\n",
    "#     axes = np.array(axes).reshape(num_rows, num_cols)  # Ensure axes is always 2D\n",
    "\n",
    "#     for idx, (df, scenarios) in enumerate(zip(dataframes, scenarios_list)):\n",
    "#         # Apply the filter_fuel if provided\n",
    "#         if filter_fuel:\n",
    "#             df = df.loc[(df.index.get_level_values('base_fuel').isin(filter_fuel)), :]\n",
    "        \n",
    "#         pos = subplot_positions[idx]\n",
    "#         ax = axes[pos[0], pos[1]]\n",
    "#         x_label = x_labels[idx] if x_labels else \"\"\n",
    "#         y_label = y_labels[idx] if y_labels else \"\"\n",
    "#         title = plot_titles[idx] if plot_titles else \"\"\n",
    "\n",
    "#         plot_adoption_rate_bar(df, scenarios, title, x_label, y_label, ax)\n",
    "\n",
    "#     if suptitle:\n",
    "#         plt.suptitle(suptitle, fontweight='bold')\n",
    "\n",
    "#     # Define the relevant tiers to display in the legend\n",
    "#     relevant_tiers = [\n",
    "#         'Tier 1: Feasible',\n",
    "#         'Tier 2: Feasible vs. Alternative',\n",
    "#         'Tier 3: Subsidy-Dependent Feasibility'\n",
    "#     ]\n",
    "\n",
    "#     # Add a legend for only the relevant tiers\n",
    "#     legend_handles = [plt.Rectangle((0, 0), 1, 1, color=color_mapping[label]) for label in relevant_tiers]\n",
    "#     fig.legend(legend_handles, relevant_tiers, loc='lower center', ncol=len(relevant_tiers), prop={'size': 20}, labelspacing=0.5, bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "#     # Adjust the layout\n",
    "#     plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout to leave space for the suptitle\n",
    "#     plt.show()\n",
    "\n",
    "# def plot_adoption_rate_bar(df, scenarios, title, x_label, y_label, ax):\n",
    "#     # Assume the DataFrame 'df' has a suitable structure, similar to earlier examples\n",
    "#     adoption_data = df.loc[:, df.columns.get_level_values(1).isin(['Tier 1: Feasible', 'Tier 2: Feasible vs. Alternative', 'Tier 3: Subsidy-Dependent Feasibility'])]\n",
    "#     adoption_data.columns = adoption_data.columns.remove_unused_levels()\n",
    "\n",
    "#     # Define the color mapping as specified\n",
    "#     global color_mapping\n",
    "#     color_mapping = {\n",
    "#         'Tier 1: Feasible': 'steelblue',\n",
    "#         'Tier 2: Feasible vs. Alternative': 'lightblue',\n",
    "#         'Tier 3: Subsidy-Dependent Feasibility': 'lightsalmon'\n",
    "#     }\n",
    "\n",
    "#     # Plotting logic\n",
    "#     n = len(adoption_data.index)\n",
    "#     bar_width = 0.35  # Width of bars\n",
    "#     index = list(range(n))  # Base index for bars\n",
    "\n",
    "#     for scenario in scenarios:\n",
    "#         if (scenario, 'Tier 1: Feasible') in adoption_data.columns and (scenario, 'Tier 2: Feasible vs. Alternative') in adoption_data.columns and (scenario, 'Tier 3: Subsidy-Dependent Feasibility') in adoption_data.columns:\n",
    "#             tier3 = adoption_data[scenario, 'Tier 3: Subsidy-Dependent Feasibility'].values\n",
    "#             tier2 = adoption_data[scenario, 'Tier 2: Feasible vs. Alternative'].values\n",
    "#             tier1 = adoption_data[scenario, 'Tier 1: Feasible'].values\n",
    "#             ax.bar(index, tier3, bar_width, color=color_mapping['Tier 3: Subsidy-Dependent Feasibility'], edgecolor='white')\n",
    "#             ax.bar(index, tier2, bar_width, color=color_mapping['Tier 2: Feasible vs. Alternative'], edgecolor='white')\n",
    "#             ax.bar(index, tier1, bar_width, color=color_mapping['Tier 1: Feasible'], edgecolor='white')\n",
    "#             index = [i + bar_width for i in index]\n",
    "\n",
    "#     ax.set_xlabel(x_label, fontweight='bold', fontsize=20)\n",
    "#     ax.set_ylabel(y_label, fontweight='bold', fontsize=20)\n",
    "#     ax.set_title(title, fontweight='bold', fontsize=20)\n",
    "#     ax.set_xticks([i + bar_width / 2 for i in range(n)])\n",
    "#     ax.set_xticklabels([f'{name[1]}' for name in adoption_data.index.tolist()], rotation=90, ha='right')\n",
    "\n",
    "#     # Set font size for tick labels on the x-axis\n",
    "#     ax.tick_params(axis='x', labelsize=20)\n",
    "\n",
    "#     # Set font size for tick labels on the y-axis\n",
    "#     ax.tick_params(axis='y', labelsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8996d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UPDATED SEPTEMBER 14, 2024 @ 12:46 AM\n",
    "# def subplot_grid_adoption_vBar(dataframes, scenarios_list, subplot_positions, filter_fuel=None, x_labels=None, plot_titles=None, y_labels=None, suptitle=None, figure_size=(12, 10), sharex=False, sharey=False):\n",
    "#     \"\"\"\n",
    "#     Creates a grid of subplots to visualize adoption rates across different scenarios, with an option to plot specific data related to adoption.\n",
    "\n",
    "#     Parameters:\n",
    "#     - dataframes (list of pd.DataFrame): List of pandas DataFrames, each DataFrame is assumed to be formatted for use in plot_adoption_rate_bar.\n",
    "#     - scenarios_list (list of list): List of scenarios corresponding to each DataFrame.\n",
    "#     - subplot_positions (list of tuples): Positions of subplots in the grid, specified as (row, col) tuples.\n",
    "#     - filter_fuel (list of str, optional): List of fuel types to filter the DataFrames by 'base_fuel' column in a multi-index.\n",
    "#     - x_labels (list of str, optional): Labels for the x-axis of each subplot.\n",
    "#     - plot_titles (list of str, optional): Titles for each subplot.\n",
    "#     - y_labels (list of str, optional): Labels for the y-axis of each subplot.\n",
    "#     - suptitle (str, optional): A central title for the entire figure.\n",
    "#     - figure_size (tuple, optional): Size of the entire figure (width, height) in inches.\n",
    "#     - sharex (bool, optional): Whether subplots should share the same x-axis.\n",
    "#     - sharey (bool, optional): Whether subplots should share the same y-axis.\n",
    "\n",
    "#     Returns:\n",
    "#     None. Displays the figure based on the provided parameters.\n",
    "#     \"\"\"\n",
    "#     # Define the color mapping as specified\n",
    "#     color_mapping = {\n",
    "#         'Tier 1: Feasible': 'steelblue',\n",
    "#         'Tier 2: Feasible vs. Alternative': 'lightblue',\n",
    "#         'Tier 3: Subsidy-Dependent Feasibility': 'lightsalmon'\n",
    "#     }\n",
    "\n",
    "#     num_cols = max(pos[1] for pos in subplot_positions) + 1\n",
    "#     num_rows = max(pos[0] for pos in subplot_positions) + 1\n",
    "\n",
    "#     fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=figure_size, sharex=sharex, sharey=sharey)\n",
    "#     axes = np.array(axes).reshape(num_rows, num_cols)  # Ensure axes is always 2D\n",
    "\n",
    "#     for idx, (df, scenarios) in enumerate(zip(dataframes, scenarios_list)):\n",
    "#         # Apply the filter_fuel if provided\n",
    "#         if filter_fuel:\n",
    "#             df = df.loc[(df.index.get_level_values('base_fuel').isin(filter_fuel)), :]\n",
    "        \n",
    "#         pos = subplot_positions[idx]\n",
    "#         ax = axes[pos[0], pos[1]]\n",
    "#         x_label = x_labels[idx] if x_labels else \"\"\n",
    "#         y_label = y_labels[idx] if y_labels else \"\"\n",
    "#         title = plot_titles[idx] if plot_titles else \"\"\n",
    "\n",
    "#         plot_adoption_rate_bar(df, scenarios, title, x_label, y_label, ax)\n",
    "\n",
    "#     if suptitle:\n",
    "#         plt.suptitle(suptitle, fontweight='bold')\n",
    "\n",
    "#     # Add a legend for the color mapping at the bottom of the entire figure\n",
    "#     legend_labels = list(color_mapping.keys())\n",
    "#     legend_handles = [plt.Rectangle((0, 0), 1, 1, color=color_mapping[label]) for label in legend_labels]\n",
    "            \n",
    "#     fig.legend(legend_handles, legend_labels, loc='lower center', ncol=len(legend_labels), prop={'size': 20}, labelspacing=0.5, bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "#     # Adjust the layout\n",
    "#     plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout to leave space for the suptitle\n",
    "#     plt.show()\n",
    "\n",
    "# def plot_adoption_rate_bar(df, scenarios, title, x_label, y_label, ax):\n",
    "#     # Assume the DataFrame 'df' has a suitable structure, similar to earlier examples\n",
    "#     adoption_data = df.loc[:, df.columns.get_level_values(1).isin(['Tier 1: Feasible', 'Tier 2: Feasible vs. Alternative', 'Tier 3: Subsidy-Dependent Feasibility'])]\n",
    "#     adoption_data.columns = adoption_data.columns.remove_unused_levels()\n",
    "\n",
    "#     # Define the color mapping as specified\n",
    "#     global color_mapping\n",
    "#     color_mapping = {\n",
    "#         'Tier 1: Feasible': 'steelblue',\n",
    "#         'Tier 2: Feasible vs. Alternative': 'lightblue',\n",
    "#         'Tier 3: Subsidy-Dependent Feasibility': 'lightsalmon'\n",
    "#     }\n",
    "\n",
    "#     # Plotting logic\n",
    "#     n = len(adoption_data.index)\n",
    "#     bar_width = 0.35  # Width of bars\n",
    "#     index = list(range(n))  # Base index for bars\n",
    "\n",
    "#     for i, scenario in enumerate(scenarios):\n",
    "#         if (scenario, 'Tier 1: Feasible') in adoption_data.columns and (scenario, 'Tier 2: Feasible vs. Alternative') in adoption_data.columns and (scenario, 'Tier 3: Subsidy-Dependent Feasibility') in adoption_data.columns:\n",
    "#             tier1 = adoption_data[scenario, 'Tier 1: Feasible'].values\n",
    "#             tier2 = adoption_data[scenario, 'Tier 2: Feasible vs. Alternative'].values\n",
    "#             tier3 = adoption_data[scenario, 'Tier 3: Subsidy-Dependent Feasibility'].values\n",
    "\n",
    "#             # Adjust the index for this scenario\n",
    "#             scenario_index = np.array(index) + i * bar_width\n",
    "            \n",
    "#             # Plot the bars for the scenario\n",
    "#             ax.bar(scenario_index, tier1, bar_width, color=color_mapping['Tier 1: Feasible'], edgecolor='white')\n",
    "#             ax.bar(scenario_index, tier2, bar_width, bottom=tier1, color=color_mapping['Tier 2: Feasible vs. Alternative'], edgecolor='white')\n",
    "#             ax.bar(scenario_index, tier3, bar_width, bottom=(tier1+tier2), color=color_mapping['Tier 3: Subsidy-Dependent Feasibility'], edgecolor='white')\n",
    "\n",
    "\n",
    "#     ax.set_xlabel(x_label, fontweight='bold', fontsize=20)\n",
    "#     ax.set_ylabel(y_label, fontweight='bold', fontsize=20)\n",
    "#     ax.set_title(title, fontweight='bold', fontsize=20)\n",
    "    \n",
    "#     ax.set_xticks([i + bar_width / 2 for i in range(n)])\n",
    "#     ax.set_xticklabels([f'{name[1]}' for name in adoption_data.index.tolist()], rotation=90, ha='right')\n",
    "\n",
    "#     # Set font size for tick labels on the x-axis\n",
    "#     ax.tick_params(axis='x', labelsize=20)\n",
    "\n",
    "#     # Set font size for tick labels on the y-axis\n",
    "#     ax.tick_params(axis='y', labelsize=20)\n",
    "\n",
    "#     # Set y-ticks from 0 to 100 in steps of 10%\n",
    "#     ax.set_yticks(np.arange(0, 101, 10))\n",
    "#     ax.set_ylim(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480b9c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATED SEPTEMBER 14, 2024 @ 12:46 AM\n",
    "def subplot_grid_adoption_vBar(dataframes, scenarios_list, subplot_positions, filter_fuel=None, x_labels=None, plot_titles=None, y_labels=None, suptitle=None, figure_size=(12, 10), sharex=False, sharey=False):\n",
    "    \"\"\"\n",
    "    Creates a grid of subplots to visualize adoption rates across different scenarios, with an option to plot specific data related to adoption.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes (list of pd.DataFrame): List of pandas DataFrames, each DataFrame is assumed to be formatted for use in plot_adoption_rate_bar.\n",
    "    - scenarios_list (list of list): List of scenarios corresponding to each DataFrame.\n",
    "    - subplot_positions (list of tuples): Positions of subplots in the grid, specified as (row, col) tuples.\n",
    "    - filter_fuel (list of str, optional): List of fuel types to filter the DataFrames by 'base_fuel' column in a multi-index.\n",
    "    - x_labels (list of str, optional): Labels for the x-axis of each subplot.\n",
    "    - plot_titles (list of str, optional): Titles for each subplot.\n",
    "    - y_labels (list of str, optional): Labels for the y-axis of each subplot.\n",
    "    - suptitle (str, optional): A central title for the entire figure.\n",
    "    - figure_size (tuple, optional): Size of the entire figure (width, height) in inches.\n",
    "    - sharex (bool, optional): Whether subplots should share the same x-axis.\n",
    "    - sharey (bool, optional): Whether subplots should share the same y-axis.\n",
    "\n",
    "    Returns:\n",
    "    None. Displays the figure based on the provided parameters.\n",
    "    \"\"\"\n",
    "    # Define the color mapping as specified\n",
    "    color_mapping = {\n",
    "        'Tier 1: Feasible': 'steelblue',\n",
    "        'Tier 2: Feasible vs. Alternative': 'lightblue',\n",
    "        'Tier 3: Subsidy-Dependent Feasibility': 'lightsalmon'\n",
    "    }\n",
    "\n",
    "    num_cols = max(pos[1] for pos in subplot_positions) + 1\n",
    "    num_rows = max(pos[0] for pos in subplot_positions) + 1\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=figure_size, sharex=sharex, sharey=sharey)\n",
    "    axes = np.array(axes).reshape(num_rows, num_cols)  # Ensure axes is always 2D\n",
    "\n",
    "    for idx, (df, scenarios) in enumerate(zip(dataframes, scenarios_list)):\n",
    "        # Apply the filter_fuel if provided\n",
    "        if filter_fuel:\n",
    "            df = df.loc[(df.index.get_level_values('base_fuel').isin(filter_fuel)), :]\n",
    "        \n",
    "        pos = subplot_positions[idx]\n",
    "        ax = axes[pos[0], pos[1]]\n",
    "        x_label = x_labels[idx] if x_labels else \"\"\n",
    "        y_label = y_labels[idx] if y_labels else \"\"\n",
    "        title = plot_titles[idx] if plot_titles else \"\"\n",
    "\n",
    "        plot_adoption_rate_bar(df, scenarios, title, x_label, y_label, ax)\n",
    "\n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle, fontweight='bold')\n",
    "\n",
    "    # Add a legend for the color mapping at the bottom of the entire figure\n",
    "    legend_labels = list(color_mapping.keys())\n",
    "    legend_handles = [plt.Rectangle((0, 0), 1, 1, color=color_mapping[label]) for label in legend_labels]\n",
    "            \n",
    "    fig.legend(legend_handles, legend_labels, loc='lower center', ncol=len(legend_labels), prop={'size': 20}, labelspacing=0.5, bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the layout to leave space for the suptitle\n",
    "    plt.show()\n",
    "\n",
    "def plot_adoption_rate_bar(df, scenarios, title, x_label, y_label, ax):\n",
    "    # Assume the DataFrame 'df' has a suitable structure, similar to earlier examples\n",
    "    adoption_data = df.loc[:, df.columns.get_level_values(1).isin(['Tier 1: Feasible', 'Tier 2: Feasible vs. Alternative', 'Tier 3: Subsidy-Dependent Feasibility'])]\n",
    "    adoption_data.columns = adoption_data.columns.remove_unused_levels()\n",
    "\n",
    "    # Define the color mapping as specified\n",
    "    global color_mapping\n",
    "    color_mapping = {\n",
    "        'Tier 1: Feasible': 'steelblue',\n",
    "        'Tier 2: Feasible vs. Alternative': 'lightblue',\n",
    "        'Tier 3: Subsidy-Dependent Feasibility': 'lightsalmon'\n",
    "    }\n",
    "\n",
    "    # Plotting logic\n",
    "    n = len(adoption_data.index)\n",
    "    bar_width = 0.35  # Width of bars\n",
    "    index = list(range(n))  # Base index for bars\n",
    "\n",
    "    for i, scenario in enumerate(scenarios):\n",
    "        if (scenario, 'Tier 1: Feasible') in adoption_data.columns and (scenario, 'Tier 2: Feasible vs. Alternative') in adoption_data.columns and (scenario, 'Tier 3: Subsidy-Dependent Feasibility') in adoption_data.columns:\n",
    "            tier1 = adoption_data[scenario, 'Tier 1: Feasible'].values\n",
    "            tier2 = adoption_data[scenario, 'Tier 2: Feasible vs. Alternative'].values\n",
    "            tier3 = adoption_data[scenario, 'Tier 3: Subsidy-Dependent Feasibility'].values\n",
    "\n",
    "            # Adjust the index for this scenario\n",
    "            scenario_index = np.array(index) + i * bar_width\n",
    "            \n",
    "            # Plot the bars for the scenario\n",
    "            ax.bar(scenario_index, tier1, bar_width, color=color_mapping['Tier 1: Feasible'], edgecolor='white')\n",
    "            ax.bar(scenario_index, tier2, bar_width, bottom=tier1, color=color_mapping['Tier 2: Feasible vs. Alternative'], edgecolor='white')\n",
    "            ax.bar(scenario_index, tier3, bar_width, bottom=(tier1+tier2), color=color_mapping['Tier 3: Subsidy-Dependent Feasibility'], edgecolor='white')\n",
    "\n",
    "\n",
    "    ax.set_xlabel(x_label, fontweight='bold', fontsize=20)\n",
    "    ax.set_ylabel(y_label, fontweight='bold', fontsize=20)\n",
    "    ax.set_title(title, fontweight='bold', fontsize=20)\n",
    "    \n",
    "    ax.set_xticks([i + bar_width / 2 for i in range(n)])\n",
    "    ax.set_xticklabels([f'{name[1]}' for name in adoption_data.index.tolist()], rotation=90, ha='right')\n",
    "\n",
    "    # Set font size for tick labels on the x-axis\n",
    "    ax.tick_params(axis='x', labelsize=20)\n",
    "\n",
    "    # Set font size for tick labels on the y-axis\n",
    "    ax.tick_params(axis='y', labelsize=20)\n",
    "\n",
    "    # Set y-ticks from 0 to 100 in steps of 10%\n",
    "    ax.set_yticks(np.arange(0, 101, 10))\n",
    "    ax.set_ylim(0, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a6449",
   "metadata": {},
   "source": [
    "# Adoption Rate Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb1d50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATED ON AUGUST 23, 2024 @ 2:00 AM\n",
    "def format_group_percentages(counts, group):\n",
    "    # Initialize total adoption with subsidy to 0\n",
    "    total_adoption_with_subsidy = 0\n",
    "    \n",
    "    # Check and sum 'Tier 1: Feasible' and 'Tier 2: Feasible vs. Alternative' if they exist\n",
    "    if 'Tier 1: Feasible' in counts.columns:\n",
    "        total_adoption_with_subsidy += counts.loc[group, 'Tier 1: Feasible']\n",
    "    if 'Tier 2: Feasible vs. Alternative' in counts.columns:\n",
    "        total_adoption_with_subsidy += counts.loc[group, 'Tier 2: Feasible vs. Alternative']\n",
    "    if 'Tier 3: Subsidy-Dependent Feasibility' in counts.columns:\n",
    "        total_adoption_with_subsidy += counts.loc[group, 'Tier 3: Subsidy-Dependent Feasibility']\n",
    "\n",
    "    # Format percentages, including checks for existence before accessing\n",
    "    formatted_percentages = ', '.join(f\"{decision_prefix}{counts.loc[group, decision]:.1f}%\" \n",
    "                                      for decision, decision_prefix in [('Tier 1: Feasible', 'T1 '), ('Tier 2: Feasible vs. Alternative', 'T2 '),('Tier 3: Subsidy-Dependent Feasibility', 'T3 ')]\n",
    "                                      if decision in counts.columns)\n",
    "    formatted_percentages += f\", TAS {total_adoption_with_subsidy:.1f}%\"\n",
    "    return formatted_percentages\n",
    "\n",
    "def print_combined_adoption_decision_percentages(dataframes, data_columns, groups, groupby1, groupby2=None, filter_fuel=None):\n",
    "    # Initialize a dictionary to hold the results\n",
    "    results = {}\n",
    "    \n",
    "    # Add a key for overall percentages\n",
    "    overall_key = \"('Overall')\"\n",
    "    results[overall_key] = []\n",
    "\n",
    "    # Iterate over each DataFrame and corresponding main_data_column\n",
    "    for df, data_column in zip(dataframes, data_columns):\n",
    "#         df_filtered = df.copy()\n",
    "\n",
    "        # Filter out the 'Existing Equipment' category from the dataframe\n",
    "        df_filtered = df[df[data_column] != 'Existing Equipment']\n",
    "\n",
    "        # Apply the filter_fuel if provided\n",
    "        if filter_fuel:\n",
    "            df_filtered = df_filtered[df_filtered['base_fuel'].isin(filter_fuel)]\n",
    "        \n",
    "        # Calculate overall percentages for the entire data column\n",
    "        overall_counts = df_filtered[data_column].value_counts(normalize=True) * 100\n",
    "        # Calculate Total Adoption with Subsidy\n",
    "        total_adoption_with_subsidy = overall_counts.get('Tier 1: Feasible', 0) + overall_counts.get('Tier 2: Feasible vs. Alternative', 0) + overall_counts.get('Tier 3: Subsidy-Dependent Feasibility', 0)\n",
    "\n",
    "        overall_percentages = ', '.join(f\"{decision_prefix}{overall_counts[decision]:.1f}%\" \n",
    "                                        for decision, decision_prefix in [('Tier 1: Feasible', 'T1 '), ('Tier 2: Feasible vs. Alternative', 'T2 '),('Tier 3: Subsidy-Dependent Feasibility', 'T3 ')]\n",
    "                                        if decision in overall_counts.index)\n",
    "        overall_percentages += f\", TAS {total_adoption_with_subsidy:.1f}%\"\n",
    "        results[overall_key].append(overall_percentages)\n",
    "        \n",
    "        if groups == 1 or groups == '1':\n",
    "            # Calculate the percentages for each combination of categories\n",
    "            counts = df_filtered.groupby(f'{groupby1}')[f'{data_column}'].value_counts(normalize=True).unstack() * 100\n",
    "            for group in counts.index:\n",
    "                key = f\"('{groupby1}', '{group}')\"\n",
    "                if key not in results:\n",
    "                    results[key] = []\n",
    "                \n",
    "                # Calculate and format percentages including Total Adoption with Subsidy\n",
    "                formatted_percentages = format_group_percentages(counts, group)\n",
    "                results[key].append(formatted_percentages)\n",
    "                \n",
    "        elif groups == 2 or groups == '2' and groupby2 is not None:\n",
    "            # Calculate the percentages for each combination of categories\n",
    "            counts = df_filtered.groupby([groupby1, groupby2])[f'{data_column}'].value_counts(normalize=True).unstack() * 100\n",
    "            for group1_group2 in counts.index:\n",
    "                key = f\"('{group1_group2[0]}', '{group1_group2[1]}')\"\n",
    "                if key not in results:\n",
    "                    results[key] = []\n",
    "\n",
    "                # Calculate and format percentages including Total Adoption with Subsidy\n",
    "                formatted_percentages = format_group_percentages(counts, group1_group2)\n",
    "                results[key].append(formatted_percentages)\n",
    "    \n",
    "    # Print combined results for overall and then for each group\n",
    "    for key, values in results.items():\n",
    "        combined_values = ' | '.join(values)\n",
    "        print(f\"{key}: {combined_values}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3f135819df3a8eae20ec2dcf67b14080d6175bb7fbb1fdd3310fd4ead2471394"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
