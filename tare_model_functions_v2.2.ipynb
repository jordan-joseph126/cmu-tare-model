{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2b5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Packages\n",
    "# Finish updating this later\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2875f8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_prompt = \"\"\"\n",
    "Would you like to filter for a specific state's data? Please enter one of the following:\n",
    "N. I'd like to analyze all of the United States.\n",
    "Y. I'd like to filter data for a specific state.\n",
    "\"\"\"\n",
    "\n",
    "city_prompt = \"\"\"\n",
    "To accurately characterize load profile, it is recommended to select subsets of data with >= 1000 models (~240,000 representative dwelling units).\n",
    "\n",
    "The following cities (number of models also shown) are available for this state:\n",
    "\"\"\"\n",
    "\n",
    "city_menu_prompt = \"\"\"\n",
    "Would you like to filter a subset of city-level data? Please enter one of the following:\n",
    "N. I'd like to analyze all of my selected state.\n",
    "Y. I'd like to filter by city in the state.\n",
    "\"\"\"\n",
    "\n",
    "def get_menu_choice(prompt, choices):\n",
    "    while True:\n",
    "        choice = input(prompt).upper()\n",
    "        if choice in choices:\n",
    "            return choice\n",
    "        print(\"Invalid option. Please try again.\")\n",
    "\n",
    "def get_state_choice(df_copy):\n",
    "    while True:\n",
    "        input_state = input(\"Which state would you like to analyze data for? Please enter the two-letter abbreviation: \").upper()\n",
    "        if df_copy['in.state'].eq(input_state).any():\n",
    "            return input_state\n",
    "        print(\"Invalid state abbreviation. Please try again.\")\n",
    "\n",
    "def get_city_choice(df_copy, input_state):\n",
    "    while True:\n",
    "        input_cityFilter = input(\"Please enter the city name ONLY (e.g., Pittsburgh): \")\n",
    "        city_filter = df_copy['in.city'].eq(f\"{input_state}, {input_cityFilter}\")\n",
    "        if city_filter.any():\n",
    "            return input_cityFilter\n",
    "        print(\"Invalid city name. Please try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee82ce6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9fc382",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def standardize_fuel_name(fuel_desc):\n",
    "    # Ensure that the input is a string\n",
    "    if pd.isna(fuel_desc):\n",
    "        return 'Other'  # Return 'Other' for NaN values\n",
    "    elif isinstance(fuel_desc, str):\n",
    "        if 'Electric' in fuel_desc:\n",
    "            return 'Electricity'\n",
    "        elif 'Gas' in fuel_desc:\n",
    "            return 'Natural Gas'\n",
    "        elif 'Propane' in fuel_desc:\n",
    "            return 'Propane'\n",
    "        elif 'Oil' in fuel_desc:\n",
    "            return 'Fuel Oil'\n",
    "        else:\n",
    "            return 'Other'  # For any unexpected types, categorize as 'Other'\n",
    "    else:\n",
    "        return 'Other'  # Non-string, non-NaN values are categorized as 'Other'\n",
    "\n",
    "def preprocess_fuel_data(df, column_name):\n",
    "    \"\"\"Applies standardization to a specified column in the DataFrame.\"\"\"\n",
    "    print(f\"Processing column: {column_name}\")\n",
    "    print(f\"Initial data types: {df[column_name].dtype}\")\n",
    "    \n",
    "    # Updated this portion of the code to prevent the setting with copy warning\n",
    "    df.loc[:, column_name] = df[column_name].apply(standardize_fuel_name)\n",
    "    \n",
    "    print(f\"Data types after processing: {df[column_name].dtype}\")\n",
    "    return df\n",
    "\n",
    "def apply_fuel_filter(df, category, enable):\n",
    "    if enable == 'Yes':\n",
    "        fuel_list = ['Natural Gas', 'Electricity', 'Propane', 'Fuel Oil']\n",
    "        df_filtered = df[df[f'base_{category}_fuel'].isin(fuel_list)]\n",
    "        print(f\"Filtered for the following fuels: {fuel_list}\")\n",
    "        return df_filtered\n",
    "    return df\n",
    "\n",
    "def apply_technology_filter(df, category, enable):\n",
    "    \"\"\"\n",
    "    Applies technology filters to the dataframe based on the category and whether filtering is enabled.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame to filter.\n",
    "    - category: The category of consumption (e.g., 'heating', 'waterHeating').\n",
    "    - enable: String flag ('Yes' or 'No') indicating whether to apply the filter.\n",
    "    \"\"\"\n",
    "    if enable == 'Yes':\n",
    "        if category == 'heating':\n",
    "            tech_list = [\n",
    "                'Electricity ASHP', 'Electricity Baseboard', 'Electricity Electric Boiler', 'Electricity Electric Furnace',\n",
    "                'Fuel Oil Fuel Boiler', 'Fuel Oil Fuel Furnace', 'Natural Gas Fuel Boiler', 'Natural Gas Fuel Furnace',\n",
    "                'Propane Fuel Boiler', 'Propane Fuel Furnace'\n",
    "            ]\n",
    "            df_filtered = df[df['heating_type'].isin(tech_list)]\n",
    "            print(f\"Filtered for the following Heating technologies: {tech_list}\")    \n",
    "            return df_filtered\n",
    "        \n",
    "        elif category == 'waterHeating':\n",
    "            tech_list = [\n",
    "                'Electric Heat Pump, 80 gal', 'Electric Premium', 'Electric Standard',\n",
    "                'Fuel Oil Premium', 'Fuel Oil Standard', 'Natural Gas Premium', 'Natural Gas Standard',\n",
    "                'Propane Premium', 'Propane Standard'\n",
    "            ]\n",
    "            df_filtered = df[df['waterHeating_type'].isin(tech_list)]\n",
    "            print(f\"Filtered for the following Water Heating technologies: {tech_list}\")\n",
    "            return df_filtered\n",
    "    \n",
    "    return df\n",
    "\n",
    "def debug_filters(df, filter_name):\n",
    "    if df.empty:\n",
    "        print(f\"No rows left after applying {filter_name}\")\n",
    "    else:\n",
    "        print(f\"{len(df)} rows remain after applying {filter_name}\")\n",
    "\n",
    "# Function to extract city name\n",
    "def extract_city_name(row):\n",
    "    match = re.match(r'^[A-Z]{2}, (.+)$', row)\n",
    "    return match.group(1) if match else row\n",
    "        \n",
    "def df_enduse_refactored(df_baseline, fuel_filter='Yes', tech_filter='Yes'):\n",
    "    # Initial check\n",
    "    if df_baseline.empty:\n",
    "        print(\"Warning: Input DataFrame is empty\")\n",
    "        return df_baseline\n",
    "\n",
    "    # Standardize fuel names in the base columns before creating the df_enduse\n",
    "    df_baseline = preprocess_fuel_data(df_baseline, 'in.clothes_dryer')\n",
    "    df_baseline = preprocess_fuel_data(df_baseline, 'in.cooking_range')\n",
    "\n",
    "    # Map standardized names to new columns\n",
    "    df_baseline['base_clothesDrying_fuel'] = df_baseline['in.clothes_dryer']\n",
    "    df_baseline['base_cooking_fuel'] = df_baseline['in.cooking_range']\n",
    "    \n",
    "    # Initialize df_enduse from df_baseline with all required columns\n",
    "    # (assuming columns are correctly listed here)\n",
    "    # Create a new DataFrame named df_enduse\n",
    "    # using pd.DataFrame constructor and initialize it with columns from df_baseline\n",
    "    df_enduse = pd.DataFrame({\n",
    "        'bldg_id': df_baseline['bldg_id'],\n",
    "        'square_footage': df_baseline['in.sqft'],\n",
    "        'census_region': df_baseline['in.census_region'],\n",
    "        'building_america_climate_zone': df_baseline['in.building_america_climate_zone'],\n",
    "        'cambium_GEA_region': df_baseline['in.generation_and_emissions_assessment_region'],\n",
    "        'state': df_baseline['in.state'],\n",
    "        'city': df_baseline['in.city'].apply(extract_city_name),\n",
    "        'county': df_baseline['in.county'],\n",
    "        'puma': df_baseline['in.puma'],\n",
    "        'county_and_puma': df_baseline['in.county_and_puma'],\n",
    "        'weather_file_city': df_baseline['in.weather_file_city'],\n",
    "        'Longitude': df_baseline['in.weather_file_longitude'],\n",
    "        'Latitude': df_baseline['in.weather_file_latitude'],\n",
    "        'building_type': df_baseline['in.geometry_building_type_recs'],\n",
    "        'income': df_baseline['in.income'],\n",
    "        'federal_poverty_level': df_baseline['in.federal_poverty_level'],\n",
    "        'occupancy': df_baseline['in.occupants'],\n",
    "        'tenure': df_baseline['in.tenure'],\n",
    "        'vacancy_status': df_baseline['in.vacancy_status'],\n",
    "        'base_heating_fuel': df_baseline['in.heating_fuel'],\n",
    "        'heating_type': df_baseline['in.hvac_heating_type_and_fuel'],\n",
    "        'hvac_cooling_type': df_baseline['in.hvac_cooling_type'],\n",
    "        'vintage': df_baseline['in.vintage'],\n",
    "        'base_heating_efficiency': df_baseline['in.hvac_heating_efficiency'],\n",
    "        'base_electricity_heating_consumption': df_baseline['out.electricity.heating.energy_consumption.kwh'],\n",
    "        'base_fuelOil_heating_consumption': df_baseline['out.fuel_oil.heating.energy_consumption.kwh'],\n",
    "        'base_naturalGas_heating_consumption': df_baseline['out.natural_gas.heating.energy_consumption.kwh'],\n",
    "        'base_propane_heating_consumption': df_baseline['out.propane.heating.energy_consumption.kwh'],\n",
    "        'base_waterHeating_fuel': df_baseline['in.water_heater_fuel'],\n",
    "        'waterHeating_type': df_baseline['in.water_heater_efficiency'],\n",
    "        'base_electricity_waterHeating_consumption': df_baseline['out.electricity.hot_water.energy_consumption.kwh'],\n",
    "        'base_fuelOil_waterHeating_consumption': df_baseline['out.fuel_oil.hot_water.energy_consumption.kwh'],\n",
    "        'base_naturalGas_waterHeating_consumption': df_baseline['out.natural_gas.hot_water.energy_consumption.kwh'],\n",
    "        'base_propane_waterHeating_consumption': df_baseline['out.propane.hot_water.energy_consumption.kwh'],\n",
    "        'base_clothesDrying_fuel': df_baseline['in.clothes_dryer'],\n",
    "        'base_electricity_clothesDrying_consumption': df_baseline['out.electricity.clothes_dryer.energy_consumption.kwh'],\n",
    "        'base_naturalGas_clothesDrying_consumption': df_baseline['out.natural_gas.clothes_dryer.energy_consumption.kwh'],\n",
    "        'base_propane_clothesDrying_consumption': df_baseline['out.propane.clothes_dryer.energy_consumption.kwh'],\n",
    "        'base_cooking_fuel': df_baseline['in.cooking_range'],\n",
    "        'base_electricity_cooking_consumption': df_baseline['out.electricity.range_oven.energy_consumption.kwh'],\n",
    "        'base_naturalGas_cooking_consumption': df_baseline['out.natural_gas.range_oven.energy_consumption.kwh'],\n",
    "        'base_propane_cooking_consumption': df_baseline['out.propane.range_oven.energy_consumption.kwh']\n",
    "    })\n",
    "    \n",
    "    categories = ['heating', 'waterHeating', 'clothesDrying', 'cooking']\n",
    "    for category in categories:\n",
    "        if category == 'heating' or category == 'waterHeating':\n",
    "            fuel_types = ['electricity', 'fuelOil', 'naturalGas', 'propane']\n",
    "            # Calculate and update total consumption\n",
    "            total_consumption = sum(df_enduse.get(f'base_{fuel}_{category}_consumption', pd.Series([], dtype=float)).fillna(0) for fuel in fuel_types)\n",
    "            df_enduse[f'baseline_{category}_consumption'] = total_consumption.replace(0, np.nan)\n",
    "\n",
    "            debug_filters(df_enduse, f\"total {category} consumption calculation\")\n",
    "\n",
    "            # Apply filters\n",
    "            df_enduse = apply_fuel_filter(df_enduse, category, fuel_filter)\n",
    "            debug_filters(df_enduse, f\"{category} fuel filter\")\n",
    "\n",
    "            df_enduse = apply_technology_filter(df_enduse, category, tech_filter)\n",
    "            debug_filters(df_enduse, f\"{category} technology filter\")\n",
    "\n",
    "        else:\n",
    "            fuel_types = ['electricity', 'naturalGas', 'propane']\n",
    "            # Calculate and update total consumption\n",
    "            total_consumption = sum(df_enduse.get(f'base_{fuel}_{category}_consumption', pd.Series([], dtype=float)).fillna(0) for fuel in fuel_types)\n",
    "            df_enduse[f'baseline_{category}_consumption'] = total_consumption.replace(0, np.nan)\n",
    "\n",
    "            debug_filters(df_enduse, f\"total {category} consumption calculation\")\n",
    "\n",
    "            # Apply filters\n",
    "            df_enduse = apply_fuel_filter(df_enduse, category, fuel_filter)\n",
    "            debug_filters(df_enduse, f\"{category} fuel filter\")\n",
    "            \n",
    "    return df_enduse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "000ee906",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate emissions factors for fossil fuels\n",
    "# This is before adjusting for natural gas leakage\n",
    "# Note: We use electricity marginal damages directly instead of multiplying\n",
    "# CEDM emissions factors by the EASIUR marginal damages. \n",
    "def calculate_fossilFuel_emission_factor(fuel_type, so2_factor, nox_factor, pm25_factor, co2_factor, fuelConversion_factor1, fuelConversion_factor2):\n",
    "    \"\"\"\n",
    "    Calculate Emissions Factors: FOSSIL FUELS\n",
    "    Fossil Fuels (Natural Gas, Fuel Oil, Propane):\n",
    "    - NOx, SO2, CO2: \n",
    "        - RESNET Table 7.1.2 Emissions Factors for Household Combustion Fuels\n",
    "        - Source: https://www.resnet.us/wp-content/uploads/ANSIRESNETICC301-2022_resnetpblshd.pdf\n",
    "        - All factors are in units of lb/Mbtu so energy consumption in kWh need to be converted to kWh \n",
    "        - (1 lb / Mbtu) * (1 Mbtu / 1x10^6 Btu) * (3412 Btu / 1 kWh)\n",
    "    - PM2.5: \n",
    "        - A National Methodology and Emission Inventory for Residential Fuel Combustion\n",
    "        - Source: https://www3.epa.gov/ttnchie1/conference/ei12/area/haneke.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an empty dictionary called margEmis_factors to store the values\n",
    "    margEmis_factors = {}\n",
    "\n",
    "    # SO2, NOx, CO2: (1 lb / Mbtu) * (1 Mbtu / 1x10^6 Btu) * (3412 Btu / 1 kWh)\n",
    "    # PM2.5 - FUEL OIL: 0.83 lb/thousand gallons * (1 thousand gallons / 1000 gallons) * (1 gallon heating oil/138,500 BTU) * (3412 BTU/1 kWh)\n",
    "    # PM2.5 - NATURAL GAS: 1.9 lb/million cf * (million cf/1000000 cf) * (1 cf natural gas/1039 BTU) * (3412 BTU/1 kWh)\n",
    "    # PM2.5 - PROPANE: 0.17 lb/thousand gallons * (1 thousand gallons / 1000 gallons) * (1 gallon propane/91,452 BTU) * (3412 BTU/1 kWh)\n",
    "    margEmis_factors[f\"{fuel_type}_so2\"] = so2_factor * (1 / 1000000) * (3412 / 1)\n",
    "    margEmis_factors[f\"{fuel_type}_nox\"] = nox_factor * (1 / 1000000) * (3412 / 1)\n",
    "    margEmis_factors[f\"{fuel_type}_pm25\"] = pm25_factor * (1 / fuelConversion_factor1) * (1 / fuelConversion_factor2) * (3412 / 1)\n",
    "    margEmis_factors[f\"{fuel_type}_co2\"] = co2_factor * (1 / 1000000) * (3412 / 1)\n",
    "\n",
    "    return margEmis_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "472802d9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Working as of May 6, 2024\n",
    "# Added documentation\n",
    "# Improved efficiency and prevent dataframe fragmentation warnings\n",
    "gea_eGRID_mapping = {\n",
    "    'AZNMc': 'AZNM',\n",
    "    'CAMXc': 'CAMX',\n",
    "    'ERCTc': 'ERCT',\n",
    "    'FRCCc': 'FRCC',\n",
    "    'MROEc': 'MROE',\n",
    "    'MROWc': 'MROW',\n",
    "    'NEWEc': 'NEWE',   \n",
    "    'NWPPc': 'NWPP',\n",
    "    'NYSTc': 'NYUP',   # NYSTc contains 'NYUP', 'NYCW', 'NYLI'\n",
    "    'RFCMc': 'RFCM',\n",
    "    'RFCWc': 'RFCW',\n",
    "    'RFCEc': 'RFCE',\n",
    "    'RMPAc': 'RMPA',\n",
    "    'SRSOc': 'SRSO',\n",
    "    'SRTVc': 'SRTV',\n",
    "    'SRMVc': 'SRMV',\n",
    "    'SRMWc': 'SRMW',\n",
    "    'SRVCc': 'SRVC',\n",
    "    'SPNOc': 'SPNO',\n",
    "    'SPSOc': 'SPSO'\n",
    "}\n",
    "\n",
    "def calculate_marginal_damages(df, grid_decarb=False):\n",
    "    \"\"\"\n",
    "    Calculate the marginal damages of different pollutants based on various conditions and mappings.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The primary data frame containing pollutant emissions data and other relevant attributes.\n",
    "    - grid_decarb (bool): Flag to determine if grid decarbonization calculations are to be applied.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: The updated data frame with calculated marginal damages and potentially new columns.\n",
    "    \n",
    "    This function processes a given DataFrame 'df' to:\n",
    "    - Copy the DataFrame to avoid modification of the original data.\n",
    "    - Map regional identifiers to a subregion grid.\n",
    "    - Calculate the natural gas leakage factor based on state.\n",
    "    - Create and calculate damage factor columns if they do not exist.\n",
    "    - Depending on the flag 'grid_decarb', apply different damage calculation methods.\n",
    "    - Manage and merge newly created columns to avoid duplicates and ensure data integrity.\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to work on\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Define lists of pollutants and categories for calculations\n",
    "    pollutants = ['so2', 'nox', 'pm25', 'co2']\n",
    "    categories = ['heating', 'waterHeating', 'clothesDrying', 'cooking']\n",
    "\n",
    "    # Map 'cambium_GEA_region' to 'subregion_eGRID' for regional breakdown\n",
    "    df_copy['subregion_eGRID'] = df_copy['cambium_GEA_region'].map(gea_eGRID_mapping)\n",
    "\n",
    "    # Map 'state' to 'naturalGas_leakage_factor'\n",
    "    state_to_factor = dict(zip(df_margEmis_factors['state'], df_margEmis_factors['naturalGas_leakage_factor']))\n",
    "    df_copy['naturalGas_leakage_factor'] = df_copy['state'].map(state_to_factor)\n",
    "\n",
    "    # Process each pollutant for social costs and damage factors\n",
    "    for pollutant in pollutants:\n",
    "        # Check if damage factor columns already exist; if not, create them\n",
    "        if f'margSocialCosts_{pollutant}' not in df_copy.columns:\n",
    "            df_copy[f'margSocialCosts_{pollutant}'] = df_copy.apply(lambda row: damages_fossilFuel_lookup[f'margSocialCosts_{pollutant}'][(row['Longitude'], row['Latitude'])], axis=1)\n",
    "        \n",
    "        if f'margDamage_factor_{pollutant}' not in df_copy.columns:\n",
    "            df_copy[f'margDamage_factor_{pollutant}'] = df_copy['subregion_eGRID'].apply(lambda x: damages_CEDM_lookup.get((pollutant, x), None))\n",
    "\n",
    "    # Set constant for transmission and distribution losses\n",
    "    td_losses = 0.06\n",
    "   \n",
    "    # Placeholder DataFrame for new or modified columns\n",
    "    new_columns_df = pd.DataFrame(index=df_copy.index)  # DataFrame to hold new or modified columns\n",
    "\n",
    "    # Depending on the grid decarbonization status, calculate damages accordingly\n",
    "    if grid_decarb:\n",
    "        new_columns_df = calculate_damages_decarb_grid(df_copy, menu_mp, categories, years, td_losses, dict_margDamages_gridDecarb)\n",
    "    else:\n",
    "        new_columns_df = calculate_damages_current_grid(df_copy, menu_mp, categories, pollutants, td_losses, national_lookup, electricity_lookup, damages_CEDM_lookup)\n",
    "\n",
    "    # Exclude columns that already exist in df_copy to avoid duplicates\n",
    "    columns_to_add = new_columns_df.columns.difference(df_copy.columns)\n",
    "\n",
    "    # Concatenate new columns avoiding duplicates and reducing fragmentation\n",
    "    df_copy = pd.concat([df_copy, new_columns_df[columns_to_add]], axis=1)\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "def calculate_damages_current_grid(df_copy, menu_mp, categories, pollutants, td_losses, national_lookup, electricity_lookup, damages_CEDM_lookup):\n",
    "    \"\"\"\n",
    "    Calculate damages for the current electricity grid scenario.\n",
    "\n",
    "    Parameters:\n",
    "        df_copy (DataFrame): The DataFrame containing consumption data.\n",
    "        menu_mp (int): The menu number for the measure package.\n",
    "        categories (list): List of end-use categories.\n",
    "        pollutants (list): List of pollutants.\n",
    "        td_losses (float): Transmission and distribution losses.\n",
    "        national_lookup (dict): Lookup table for national emissions factors.\n",
    "        electricity_lookup (dict): Lookup table for electricity emissions.\n",
    "        damages_CEDM_lookup (dict): Lookup table for damages from CEDM.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with calculated damages.\n",
    "    \"\"\"\n",
    "    if menu_mp == 0:\n",
    "        # Handling based on 'menu_mp' (Measure Packages)\n",
    "        for category in categories:\n",
    "            print(f\"End-use category: {category}\")\n",
    "            for pollutant in pollutants:\n",
    "                # Calculate emissions for each fuel type\n",
    "                emis_naturalGas = df_copy[f'base_naturalGas_{category}_consumption'] * national_lookup.get(('naturalGas', pollutant), np.nan)\n",
    "                emis_propane = df_copy[f'base_propane_{category}_consumption'] * national_lookup.get(('propane', pollutant), np.nan)\n",
    "                \n",
    "                # Calculate electricity emissions using the lookup\n",
    "                df_copy['electricity_lookup_values'] = df_copy.apply(lambda row: electricity_lookup.get((pollutant, row['state']), np.nan), axis=1)\n",
    "                emis_electricity = df_copy[f'base_electricity_{category}_consumption'] * (1 / (1 - td_losses)) * df_copy['electricity_lookup_values']\n",
    "\n",
    "                if 'cooking' in category or 'clothesDrying' in category:\n",
    "                    # Total emissions for categories without fuel oil usage\n",
    "                    total_emissions = emis_electricity.fillna(0) + emis_naturalGas.fillna(0) + emis_propane.fillna(0)\n",
    "                else:\n",
    "                    emis_fuelOil = df_copy[f'base_fuelOil_{category}_consumption'] * national_lookup.get(('fuelOil', pollutant), np.nan)\n",
    "                    # Total emissions for categories with fuel oil usage\n",
    "                    total_emissions = emis_electricity.fillna(0) + emis_naturalGas.fillna(0) + emis_propane.fillna(0) + emis_fuelOil.fillna(0)\n",
    "\n",
    "                # Calculate and store damages\n",
    "                df_copy[f'baseline_{category}_damages_{pollutant}'] = round(total_emissions * df_copy[f'margSocialCosts_{pollutant}'], 2)\n",
    "                \n",
    "                # Debug: Check baseline damages\n",
    "                print(f\"Baseline Damages for {category} - {pollutant}:\")\n",
    "                print(df_copy[f'baseline_{category}_damages_{pollutant}'].head())\n",
    "\n",
    "            # Calculate total health and climate damages for the category\n",
    "            df_copy[f'baseline_{category}_damages_health'] = round(df_copy[f'baseline_{category}_damages_so2'] + df_copy[f'baseline_{category}_damages_nox'] + df_copy[f'baseline_{category}_damages_pm25'], 2)\n",
    "            df_copy[f'baseline_{category}_damages_climate'] = round(df_copy[f'baseline_{category}_damages_co2'], 2)\n",
    "            \n",
    "            # Debug: Check health and climate damages\n",
    "            print(f\"Baseline Health Damages for {category}:\")\n",
    "            print(df_copy[f'baseline_{category}_damages_health'].head())\n",
    "            print(f\"Baseline Climate Damages for {category}:\")\n",
    "            print(df_copy[f'baseline_{category}_damages_climate'].head())\n",
    "\n",
    "    else:\n",
    "        # MEASURE PACKAGES scenario (not the baseline, but specific interventions)\n",
    "        for category in categories:\n",
    "            print(f\"End-use category: {category}\")\n",
    "            for pollutant in pollutants:                \n",
    "                df_copy[f'mp{menu_mp}_{category}_damages_{pollutant}'] = df_copy.apply(lambda row: row[f'mp{menu_mp}_{category}_consumption'] * (1/(1-td_losses)) * damages_CEDM_lookup.get((pollutant, row['subregion_eGRID']), np.nan), axis=1).fillna(0).round(2)\n",
    "\n",
    "                df_copy[f'mp{menu_mp}_{category}_reduction_damages_{pollutant}'] = (df_copy[f'baseline_{category}_damages_{pollutant}'] - df_copy[f'mp{menu_mp}_{category}_damages_{pollutant}']).round(2)\n",
    "\n",
    "            df_copy[f'mp{menu_mp}_{category}_damages_health'] = (df_copy[f'mp{menu_mp}_{category}_damages_so2'] + df_copy[f'mp{menu_mp}_{category}_damages_nox'] + df_copy[f'mp{menu_mp}_{category}_damages_pm25']).round(2)\n",
    "            df_copy[f'mp{menu_mp}_{category}_damages_climate'] = df_copy[f'mp{menu_mp}_{category}_damages_co2'].round(2)\n",
    "\n",
    "            df_copy[f'mp{menu_mp}_{category}_reduction_damages_health'] = (df_copy[f'baseline_{category}_damages_health'] - df_copy[f'mp{menu_mp}_{category}_damages_health']).round(2)\n",
    "            df_copy[f'mp{menu_mp}_{category}_reduction_damages_climate'] = (df_copy[f'baseline_{category}_damages_climate'] - df_copy[f'mp{menu_mp}_{category}_damages_climate']).round(2)\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def calculate_damages_decarb_grid(df_copy, menu_mp, categories, years, td_losses, dict_margDamages_gridDecarb):\n",
    "    \"\"\"\n",
    "    Calculates the damages due to decarbonization of the grid across multiple categories\n",
    "    and pollutants, taking transmission and distribution losses into account.\n",
    "\n",
    "    Parameters:\n",
    "    - df_copy (DataFrame): A DataFrame containing the data on which calculations will be performed.\n",
    "    - menu_mp (str): A modifier representing a specific menu policy or scenario.\n",
    "    - categories (list): A list of equipment categories to calculate damages for.\n",
    "    - years (list): A list of years for which damages will be calculated.\n",
    "    - td_losses (float): Transmission and distribution loss factor to adjust consumption data.\n",
    "    - dict_margDamages_gridDecarb (dict): A nested dictionary where keys are years and values are \n",
    "      sub-dictionaries mapping (subregion, pollutant) pairs to marginal damage values.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: The original DataFrame with new columns added for calculated damages.\n",
    "\n",
    "    This function iterates over specified equipment categories and their respective lifetimes,\n",
    "    calculating damages for each pollutant in each year based on grid consumption data adjusted\n",
    "    for transmission and distribution losses. The damages are calculated separately for health \n",
    "    impacts (from SO2, NOx, PM2.5) and climate impacts (from CO2). The results are added as new\n",
    "    columns to the input DataFrame.\n",
    "    \"\"\"\n",
    "    # Specifications for equipment lifetimes in years\n",
    "    equipment_specs = {\n",
    "        'heating': 15,\n",
    "        'waterHeating': 12,\n",
    "        'clothesDrying': 13,\n",
    "        'cooking': 15\n",
    "    }\n",
    "    \n",
    "    # List of considered pollutants\n",
    "    pollutants = ['so2', 'nox', 'pm25', 'co2']\n",
    "\n",
    "    # Dictionary to store new data columns\n",
    "    new_columns_data = {}\n",
    "\n",
    "    for category, lifetime in equipment_specs.items():\n",
    "        print(f\"End-use category: {category}\")  \n",
    "        for year in range(1, lifetime + 1):\n",
    "            year_label = year + 2018\n",
    "            for pollutant in pollutants:\n",
    "                # Determine column names based on health or climate impact\n",
    "                damage_col = f'{year_label}_mp{menu_mp}_{category}_damages' + ('_health' if pollutant in ['so2', 'nox', 'pm25'] else '_climate')\n",
    "                \n",
    "                # Adjust consumption data by transmission and distribution losses\n",
    "                td_losses_multiplier = (1 / (1 - td_losses))\n",
    "\n",
    "                # Calculate damages\n",
    "                damage_data = df_copy[f'mp{menu_mp}_{category}_consumption'] * td_losses_multiplier\n",
    "                damage_data *= df_copy['subregion_eGRID'].map(lambda x: dict_margDamages_gridDecarb[year_label].get((x, pollutant), np.nan))\n",
    "                damage_data *= df_copy['naturalGas_leakage_factor']\n",
    "                new_columns_data[damage_col] = damage_data.round(2)\n",
    "\n",
    "    # Creating a new DataFrame from the dictionary of new columns\n",
    "    new_columns_df = pd.DataFrame(new_columns_data, index=df_copy.index)\n",
    "    \n",
    "    # Concatenate this DataFrame with the original DataFrame\n",
    "    df_copy = pd.concat([df_copy, new_columns_df], axis=1)\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ac34345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_annual_fuelCost(df, state_region, df_fuelPrices_perkWh, cpi_ratio):\n",
    "    \"\"\"\n",
    "    -------------------------------------------------------------------------------------------------------\n",
    "    Step 2: Calculate Annual Operating (Fuel) Costs\n",
    "    -------------------------------------------------------------------------------------------------------\n",
    "    - Create a mapping dictionary for fuel types\n",
    "    - Create new merge cifolumns to ensure a proper match.\n",
    "    - Merge df_copy with df_fuel_prices to get fuel prices for electricity, natural gas, propane, and fuel oil\n",
    "    - Calculate the per kWh fuel costs for each fuel type and region\n",
    "    - Calculate the baseline fuel cost \n",
    "    -------------------------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # For Baseline Consumption (Measure Package 0)\n",
    "    if menu_mp == 0:    \n",
    "        # Fuel and region mappings remain unchanged\n",
    "        region_mapping = {\n",
    "            'South': 'South',\n",
    "            'Midwest': 'Midwest',\n",
    "            'West': 'Midwest',\n",
    "            'Northeast': 'Northeast'\n",
    "        }\n",
    "        df_copy['region_merge'] = df_copy['census_region'].map(region_mapping)\n",
    "\n",
    "        # Standardize the fuel types\n",
    "        fuel_mapping = {\n",
    "            'Electricity': 'electricity',\n",
    "            'Natural Gas': 'naturalGas',\n",
    "            'Fuel Oil': 'fuelOil',\n",
    "            'Propane': 'propane'\n",
    "        }\n",
    "        # Apply mapping for each category of fuel usage\n",
    "        categories = ['heating', 'waterHeating', 'clothesDrying', 'cooking']\n",
    "        for category in categories:\n",
    "            df_copy[f'fuel_type_{category}'] = df_copy[f'base_{category}_fuel'].map(fuel_mapping)\n",
    "\n",
    "        # Calculate the per kWh fuel costs for each fuel type and region\n",
    "        df_fuel_prices_lookup = df_fuelPrices_perkWh.set_index(['fuel_type', 'state_region'])['cost_per_kWh'].to_dict()\n",
    "\n",
    "        # Step 4: Calculate the per kWh fuel costs for each fuel type and region\n",
    "        df_copy['fuelPrice_electricity_perkWh'] = df_copy.apply(lambda row: df_fuel_prices_lookup.get(('electricity', row['state']), np.nan), axis=1) * cpi_ratio\n",
    "        df_copy['fuelPrice_naturalGas_perkWh'] = df_copy.apply(lambda row: df_fuel_prices_lookup.get(('naturalGas', row['state']), np.nan), axis=1) * cpi_ratio\n",
    "        df_copy['fuelPrice_propane_perkWh'] = df_copy.apply(lambda row: df_fuel_prices_lookup.get(('propane', row['region_merge']), np.nan), axis=1) * cpi_ratio\n",
    "        df_copy['fuelPrice_fuelOil_perkWh'] = df_copy.apply(lambda row: df_fuel_prices_lookup.get(('fuelOil', 'National'), np.nan), axis=1) * cpi_ratio\n",
    "\n",
    "        # Calculate the baseline fuel cost for each category based on the respective fuel type\n",
    "        for category in categories:\n",
    "            fuel_type_col = f'fuel_type_{category}'\n",
    "            df_copy[f'baseline_{category}_fuelCost'] = df_copy.apply(\n",
    "                lambda row: row[f'baseline_{category}_consumption'] * row[f'fuelPrice_{row[fuel_type_col]}_perkWh'], axis=1\n",
    "            ).round(2)\n",
    "\n",
    "        return df_copy\n",
    "        return df_fuel_prices_lookup\n",
    "    \n",
    "    # For Measure Packages 7, 8, 9, and 10\n",
    "    else:\n",
    "        # Apply mapping for each category of fuel usage\n",
    "        categories = ['heating', 'waterHeating', 'clothesDrying', 'cooking']\n",
    "        for category in categories:\n",
    "            # Use the calculated per kWh electricity price\n",
    "            df_copy[f'mp{menu_mp}_{category}_fuelCost'] = (df_copy[f'mp{menu_mp}_{category}_consumption'] * df_copy['fuelPrice_electricity_perkWh']).round(2)\n",
    "            df_copy[f'mp{menu_mp}_{category}_savings_fuelCost'] = (df_copy[f'baseline_{category}_fuelCost'].sub(df_copy[f'mp{menu_mp}_{category}_fuelCost'], axis=0, fill_value=0)).round(2)\n",
    "            df_copy[f'mp{menu_mp}_{category}_delta_fuelCost'] = (df_copy[f'mp{menu_mp}_{category}_fuelCost'].sub(df_copy[f'baseline_{category}_fuelCost'], axis=0, fill_value=0)).round(2)\n",
    "            df_copy[f'mp{menu_mp}_{category}_percentChange_fuelCost'] = (((df_copy[f'mp{menu_mp}_{category}_fuelCost'].sub(df_copy[f'baseline_{category}_fuelCost'], axis=0, fill_value=0)) / df_copy[f'baseline_{category}_fuelCost']) * 100).round(2)\n",
    "    \n",
    "        return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501783c1",
   "metadata": {},
   "source": [
    "# Retrofit Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613aa076",
   "metadata": {},
   "source": [
    "## Basic Retrofit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d019f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_consumption_reduction(df, category):\n",
    "    categories = ['heating', 'waterHeating', 'clothesDrying', 'cooking']\n",
    "    for category in categories:\n",
    "        if category == 'heating':\n",
    "            df[f'mp{menu_mp}_heating_reduction_consumption'] = (df[f'baseline_heating_consumption'].sub(df[f'mp{menu_mp}_heating_consumption'], axis=0, fill_value=0)).round(2) \n",
    "            df[f'mp{menu_mp}_heating_change_consumption'] = (df[f'mp{menu_mp}_heating_consumption'].sub(df[f'baseline_heating_consumption'], axis=0, fill_value=0)).round(2)\n",
    "            df[f'mp{menu_mp}_heating_percentChange_consumption'] = (((df[f'mp{menu_mp}_heating_consumption'].sub(df[f'baseline_heating_consumption'], axis=0, fill_value=0)) / df[f'baseline_heating_consumption']) * 100).round(2)\n",
    "        else:\n",
    "            df[f'mp{menu_mp}_{category}_reduction_consumption'] = (df[f'baseline_{category}_consumption'].sub(df[f'mp{menu_mp}_{category}_consumption'], axis=0, fill_value=0)).round(2) \n",
    "            df[f'mp{menu_mp}_{category}_change_consumption'] = (df[f'mp{menu_mp}_{category}_consumption'].sub(df[f'baseline_{category}_consumption'], axis=0, fill_value=0)).round(2)\n",
    "            df[f'mp{menu_mp}_{category}_percentChange_consumption'] = (((df[f'mp{menu_mp}_{category}_consumption'].sub(df[f'baseline_{category}_consumption'], axis=0, fill_value=0)) / df[f'baseline_{category}_consumption']) * 100).round(2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46bd45e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_enduse_compare(df_mp, menu_mp, df_baseline):\n",
    "    # Create a new DataFrame named df_compare\n",
    "    # using pd.DataFrame constructor and initialize it with columns from df_mp\n",
    "    df_compare = pd.DataFrame({\n",
    "        'bldg_id':df_mp['bldg_id'],\n",
    "        'hvac_has_ducts': df_mp['in.hvac_has_ducts'],\n",
    "        'baseline_heating_type': df_mp['in.hvac_heating_type_and_fuel'],\n",
    "        'hvac_heating_efficiency': df_mp['in.hvac_heating_efficiency'],\n",
    "        'hvac_heating_type_and_fuel': df_mp['in.hvac_heating_type_and_fuel'],\n",
    "        'size_heat_pump_backup_primary_k_btu_h': df_mp['out.params.size_heat_pump_backup_primary_k_btu_h'],\n",
    "        'size_heating_system_primary_k_btu_h': df_mp['out.params.size_heating_system_primary_k_btu_h'],\n",
    "        'size_heating_system_secondary_k_btu_h': df_mp['out.params.size_heating_system_secondary_k_btu_h'],\n",
    "        'upgrade_hvac_heating_efficiency': df_mp['upgrade.hvac_heating_efficiency'],\n",
    "        'water_heater_efficiency': df_mp['in.water_heater_efficiency'],\n",
    "        'water_heater_fuel': df_mp['in.water_heater_fuel'],\n",
    "        'water_heater_in_unit': df_mp['in.water_heater_in_unit'],\n",
    "        'size_water_heater_gal': df_mp['out.params.size_water_heater_gal'],\n",
    "        'upgrade_water_heater_efficiency': df_mp['upgrade.water_heater_efficiency'],\n",
    "        'clothes_dryer_in_unit': df_mp['in.clothes_dryer'],\n",
    "        'upgrade_clothes_dryer': df_mp['upgrade.clothes_dryer'],\n",
    "        'cooking_range_in_unit': df_euss_am_mp7['in.cooking_range'],\n",
    "        'upgrade_cooking_range': df_euss_am_mp7['upgrade.cooking_range']\n",
    "    })\n",
    "    \n",
    "    categories = ['heating', 'waterHeating', 'clothesDrying', 'cooking']\n",
    "    for category in categories:\n",
    "        if category == 'heating':\n",
    "            # Heating Dataframe\n",
    "            # MP9 = MP8 (Electrification, High Efficiency) + MP1 (Basic Enclosure)\n",
    "            if input_mp == 'upgrade09':\n",
    "                menu_mp = 9\n",
    "                df_compare[f'mp{menu_mp}_heating_consumption'] = df_mp['out.electricity.heating.energy_consumption.kwh'].round(2)\n",
    "\n",
    "                # Measure Package 1: Basic Enclosure Package\n",
    "                # Attic floor insulation (upgrade.insulation_ceiling)\n",
    "                df_compare['base_insulation_atticFloor'] = df_mp['in.insulation_ceiling']\n",
    "                df_compare['upgrade_insulation_atticFloor'] = df_mp['upgrade.insulation_ceiling']\n",
    "                df_compare['out_params_floor_area_attic_ft_2'] = df_mp['out.params.floor_area_attic_ft_2']\n",
    "\n",
    "                # Air leakage reduction (upgrade.infiltration_reduction == '30%')\n",
    "                df_compare['upgrade_infiltration_reduction'] = df_mp['upgrade.infiltration_reduction']\n",
    "\n",
    "                # Duct sealing (upgrade.ducts == '10% Leakage, R-8')            \n",
    "                df_compare['base_ducts'] = df_mp['in.ducts']\n",
    "                df_compare['upgrade_duct_sealing'] = df_mp['upgrade.ducts']\n",
    "                df_compare['out_params_duct_unconditioned_surface_area_ft_2'] = df_mp['out.params.duct_unconditioned_surface_area_ft_2']\n",
    "\n",
    "                # Drill-and-fill wall insulation (upgrade.insulation_wall == 'Wood Stud, R-13')\n",
    "                df_compare['base_insulation_wall'] = df_mp['in.insulation_wall']\n",
    "                df_compare['upgrade_insulation_wall'] = df_mp['upgrade.insulation_wall']\n",
    "                df_compare['out_params_wall_area_above_grade_exterior_ft_2'] = df_mp['out.params.wall_area_above_grade_exterior_ft_2']\n",
    "\n",
    "            # MP8 = MP8 (Electrification, High Efficiency) + MP2 (Enhanced Enclosure)\n",
    "            elif input_mp == 'upgrade10':\n",
    "                menu_mp = 10\n",
    "                df_compare[f'mp{menu_mp}_heating_consumption'] = df_mp['out.electricity.heating.energy_consumption.kwh'].round(2)\n",
    "\n",
    "                # Measure Package 1: Basic Enclosure Package\n",
    "                # Attic floor insulation (upgrade.insulation_ceiling)\n",
    "                df_compare['base_insulation_atticFloor'] = df_mp['in.insulation_ceiling']\n",
    "                df_compare['upgrade_insulation_atticFloor'] = df_mp['upgrade.insulation_ceiling']\n",
    "                df_compare['out_params_floor_area_attic_ft_2'] = df_mp['out.params.floor_area_attic_ft_2']\n",
    "\n",
    "                # Air leakage reduction (upgrade.infiltration_reduction == '30%')\n",
    "                df_compare['upgrade_infiltration_reduction'] = df_mp['upgrade.infiltration_reduction']\n",
    "\n",
    "                # Duct sealing (upgrade.ducts == '10% Leakage, R-8')                        \n",
    "                df_compare['base_ducts'] = df_mp['in.ducts']\n",
    "                df_compare['upgrade_duct_sealing'] = df_mp['upgrade.ducts']\n",
    "                df_compare['out_params_duct_unconditioned_surface_area_ft_2'] = df_mp['out.params.duct_unconditioned_surface_area_ft_2']\n",
    "\n",
    "                # Drill-and-fill wall insulation (upgrade.insulation_wall == 'Wood Stud, R-13')\n",
    "                df_compare['base_insulation_wall'] = df_mp['in.insulation_wall']\n",
    "                df_compare['upgrade_insulation_wall'] = df_mp['upgrade.insulation_wall']\n",
    "                df_compare['out_params_wall_area_above_grade_exterior_ft_2'] = df_mp['out.params.wall_area_above_grade_exterior_ft_2']\n",
    "\n",
    "                # Measure Package 2: Enhanced Enclosure Package\n",
    "                # Foundation wall insulation and rim joist insulation\n",
    "                df_compare['base_foundation_type'] = df_mp['in.geometry_foundation_type']\n",
    "                df_compare['base_insulation_foundation_wall'] = df_mp['in.insulation_foundation_wall']\n",
    "                df_compare['base_insulation_rim_joist'] = df_mp['in.insulation_rim_joist']\n",
    "\n",
    "                # Only upgrade column for foundation wall insulation, but we will assume technical documentation and modeling consistent\n",
    "                df_compare['upgrade_insulation_foundation_wall'] = df_mp['upgrade.insulation_foundation_wall']\n",
    "                df_compare['out_params_floor_area_foundation_ft_2'] = df_mp['out.params.floor_area_foundation_ft_2']\n",
    "                df_compare['out_params_rim_joist_area_above_grade_exterior_ft_2'] = df_mp['out.params.rim_joist_area_above_grade_exterior_ft_2']                        \n",
    "\n",
    "                # Seal Vented Crawl Space\n",
    "                df_compare['upgrade_seal_crawlspace'] = df_mp['upgrade.geometry_foundation_type']\n",
    "\n",
    "                # Insulate finished attics and cathedral ceilings\n",
    "                df_compare['base_insulation_roof'] = df_mp['in.insulation_roof']\n",
    "                df_compare['upgrade_insulation_roof'] = df_mp['upgrade.insulation_roof']\n",
    "                df_compare['out_params_roof_area_ft_2'] = df_mp['out.params.roof_area_ft_2']\n",
    "            \n",
    "            else:\n",
    "                df_compare[f'mp{menu_mp}_heating_consumption'] = df_mp['out.electricity.heating.energy_consumption.kwh'].round(2)\n",
    "        # Water Heating Dataframe    \n",
    "        elif category == 'waterHeating':\n",
    "            df_compare[f'mp{menu_mp}_waterHeating_consumption'] = df_mp['out.electricity.hot_water.energy_consumption.kwh'].round(2)\n",
    "\n",
    "        # Clothes Drying Dataframe\n",
    "        elif category == 'clothesDrying':\n",
    "            df_compare[f'mp{menu_mp}_clothesDrying_consumption'] = df_mp['out.electricity.clothes_dryer.energy_consumption.kwh'].round(2)\n",
    "\n",
    "        # Cooking Dataframe\n",
    "        elif category == 'cooking':\n",
    "            df_compare[f'mp{menu_mp}_cooking_consumption'] = df_euss_am_mp7['out.electricity.range_oven.energy_consumption.kwh'].round(2)\n",
    "            \n",
    "    # Merge dataframes on bldg id column so everything is lined up\n",
    "    df_compare = pd.merge(df_baseline, df_compare, how='inner', on = 'bldg_id')\n",
    "    calculate_consumption_reduction(df_compare, category)    \n",
    "        \n",
    "    return df_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58c1f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stats_table(df, data_columns, column_name_mapping, number_formatting, include_zero=True):\n",
    "    \"\"\"\n",
    "    Generate a formatted summary statistics table for specified columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame from which to compute statistics.\n",
    "    - data_columns (list of str): The columns to include in the summary statistics.\n",
    "    - column_name_mapping (dict): A dictionary to rename the columns in the summary statistics output.\n",
    "    - number_formatting (str): The format string to use for numeric values in the output.\n",
    "    - include_zero (bool, optional): Whether to include zero values in the statistics. Defaults to True.\n",
    "      If False, zeros are replaced with NaN, which are then ignored in the computations.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing the summary statistics, with formatted numeric values\n",
    "      and renamed columns according to the input specifications.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the DataFrame to avoid modifying the original data\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Replace 0 values with NaN in the selected columns if include_zero is set to False\n",
    "    if not include_zero:\n",
    "        df_copy[data_columns] = df_copy[data_columns].replace(0, np.nan)\n",
    "\n",
    "    # Compute summary statistics for the selected columns\n",
    "    # The 'describe' function returns summary statistics including count, mean, std, min, 25%, 50%, 75%, max\n",
    "    # Apply formatting to each number in these statistics according to the given format\n",
    "    summary_stats = df_copy[data_columns].describe().apply(lambda col: col.map(lambda x: f\"{x:{number_formatting}}\"))\n",
    "\n",
    "    # Rename the columns in the summary statistics DataFrame according to the provided mapping\n",
    "    summary_stats.rename(columns=column_name_mapping, inplace=True)\n",
    "\n",
    "    return summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e31f1fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for storing the lifetimes of different equipment categories\n",
    "equipment_specs = {\n",
    "    'heating': 15,\n",
    "    'waterHeating': 12,\n",
    "    'clothesDrying': 13,\n",
    "    'cooking': 15\n",
    "}\n",
    "\n",
    "def calculate_public_npv(df, interest_rate, grid_decarb=False):\n",
    "    \"\"\"\n",
    "    Calculate the public Net Present Value (NPV) for a specific category of damages,\n",
    "    taking into account changes in emissions-related damages over the lifetime of equipment,\n",
    "    under scenarios with and without grid decarbonization.\n",
    "\n",
    "    The function computes the NPV from a public perspective by assessing the health and climate damages\n",
    "    over the lifetime of a project (e.g., a heat pump installation). It supports two scenarios:\n",
    "    - Non-Decarbonized Grid: Assumes consistent damage reductions over the equipment's lifetime.\n",
    "    - Decarbonized Grid: Accounts for annual changes in damage reductions due to potential grid decarbonization.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): A pandas DataFrame containing the relevant data.\n",
    "    - interest_rate (float): The discount rate used in the NPV calculation.\n",
    "    - grid_decarb (bool, optional): Flag indicating whether grid decarbonization is considered. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: The input DataFrame with an additional column containing the calculated public NPV for the specified category.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Check and manage existing DataFrame with new columns\n",
    "    new_columns_df = pd.DataFrame(index=df_copy.index)  # DataFrame to hold new or modified columns\n",
    "\n",
    "    # Depending on the grid decarbonization status, calculate damages accordingly\n",
    "    if not grid_decarb:\n",
    "        new_columns_df = calculate_lifetime_damages(df_copy, menu_mp, equipment_specs, interest_rate)\n",
    "    else:\n",
    "        new_columns_df = calculate_lifetime_damages_gridDecarb(df_copy, menu_mp, equipment_specs, interest_rate)\n",
    "\n",
    "    # Merge new columns ensuring no duplicates\n",
    "    for col in new_columns_df.columns:\n",
    "        if col not in df_copy.columns:\n",
    "            df_copy[col] = new_columns_df[col]\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "def calculate_lifetime_damages(df_copy, menu_mp, equipment_specs, interest_rate):\n",
    "    for category, lifetime in equipment_specs.items():      \n",
    "        base_climate = df_copy[f'baseline_{category}_damages_climate']\n",
    "        base_health = df_copy[f'baseline_{category}_damages_health']\n",
    "                \n",
    "        retrofit_climate = df_copy[f'mp{menu_mp}_{category}_damages_climate']\n",
    "        retrofit_health = df_copy[f'mp{menu_mp}_{category}_damages_health']                \n",
    "        \n",
    "        base_damages = base_climate + base_health\n",
    "        retrofit_damages = retrofit_climate + retrofit_health\n",
    "        \n",
    "        # Compute and round the NPV, then store it in a new DataFrame column\n",
    "        df_copy[f'mp{menu_mp}_{category}_climate_npv'] = round(((base_climate - retrofit_climate)) * ((1 - ((1 + interest_rate) ** (-1 * lifetime))) / interest_rate), 2)\n",
    "        df_copy[f'mp{menu_mp}_{category}_health_npv'] = round(((base_health - retrofit_health)) * ((1 - ((1 + interest_rate) ** (-1 * lifetime))) / interest_rate), 2)\n",
    "        df_copy[f'mp{menu_mp}_{category}_public_npv'] = round(((base_damages - retrofit_damages)) * ((1 - ((1 + interest_rate) ** (-1 * lifetime))) / interest_rate), 2)\n",
    "        \n",
    "    return df_copy\n",
    "\n",
    "def calculate_lifetime_damages_gridDecarb(df_copy, menu_mp, equipment_specs, interest_rate):\n",
    "    # Decarbonized Grid Scenario:\n",
    "    # Process each category of damages\n",
    "    for category, lifetime in equipment_specs.items():\n",
    "        df_copy[f'gridDecarb_mp{menu_mp}_{category}_climate_npv'] = 0\n",
    "        df_copy[f'gridDecarb_mp{menu_mp}_{category}_health_npv'] = 0\n",
    "        df_copy[f'gridDecarb_mp{menu_mp}_{category}_public_npv'] = 0\n",
    "            \n",
    "        for year in range(1, lifetime + 1):\n",
    "            base_climate = df_copy[f'baseline_{category}_damages_climate']\n",
    "            base_health = df_copy[f'baseline_{category}_damages_health']\n",
    "                \n",
    "            gridDecarb_retrofit_climate = df_copy[f'{year + 2018}_mp{menu_mp}_{category}_damages_climate']\n",
    "            gridDecarb_retrofit_health = df_copy[f'{year + 2018}_mp{menu_mp}_{category}_damages_health']\n",
    "                \n",
    "            base_damages = base_climate + base_health\n",
    "            gridDecarb_retrofit_damages = gridDecarb_retrofit_climate + gridDecarb_retrofit_health\n",
    "                \n",
    "            discount_factor = ( 1 / ((1 + interest_rate) ** year))\n",
    "                \n",
    "            gridDecarb_climate_npv = (base_climate - gridDecarb_retrofit_climate) * discount_factor\n",
    "            df_copy[f'gridDecarb_mp{menu_mp}_{category}_climate_npv'] += gridDecarb_climate_npv.round(2)\n",
    "                \n",
    "            gridDecarb_health_npv = (base_health - gridDecarb_retrofit_health) * discount_factor\n",
    "            df_copy[f'gridDecarb_mp{menu_mp}_{category}_health_npv'] += gridDecarb_health_npv.round(2)\n",
    "                \n",
    "            gridDecarb_public_npv = (base_damages - gridDecarb_retrofit_damages) * discount_factor\n",
    "            df_copy[f'gridDecarb_mp{menu_mp}_{category}_public_npv'] += gridDecarb_public_npv.round(2)\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2549f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CCI to adjust for cost differences when compared to the national average\n",
    "# Function to map city to its average cost\n",
    "def map_average_cost(city):\n",
    "    if city in average_cost_map:\n",
    "        return average_cost_map[city]\n",
    "    elif city == 'Not in a census Place' or city == 'In another census Place':\n",
    "        return average_cost_map.get('+30 City Average')\n",
    "    else:\n",
    "        return average_cost_map.get('+30 City Average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f3b2993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_heating_system_specs(df):\n",
    "    # Check if necessary columns are in the DataFrame\n",
    "    necessary_columns = ['size_heating_system_primary_k_btu_h', 'size_heat_pump_backup_primary_k_btu_h',\n",
    "                         'size_heating_system_secondary_k_btu_h', 'baseline_heating_type']\n",
    "    if not all(column in df.columns for column in necessary_columns):\n",
    "        raise ValueError(\"DataFrame does not contain all necessary columns.\")\n",
    "\n",
    "    # Total heating load in kBtuh\n",
    "    df['total_heating_load_kBtuh'] = df['size_heating_system_primary_k_btu_h'] + df['size_heat_pump_backup_primary_k_btu_h'] + df['size_heating_system_secondary_k_btu_h']\n",
    "    \n",
    "#     # Total heating load in kW\n",
    "#     df['total_heating_load_kW'] = df['total_heating_load_kBtuh'] * 1000 / 3412.142\n",
    "   \n",
    "    # Use regex to remove the fuel and leave only the heating type:\n",
    "    df['baseline_heating_type'] = df['baseline_heating_type'].str.extract(r'^(?:\\d+\\s+)?(?:Natural Gas|Electricity|Propane|Fuel Oil|Fuel)\\s+(?:Fuel\\s+)?(?:Electric\\s+)?(.+)$')\n",
    "    \n",
    "    # AFUE extraction for existing, baseline equipment (Replacement Costs)\n",
    "    df['baseline_AFUE'] = df['hvac_heating_efficiency'].str.extract(r'([\\d.]+)%').astype(float)\n",
    "    \n",
    "    # SEER extraction for existing, baseline equipment (Replacement Costs)\n",
    "    df['baseline_SEER'] = df['hvac_heating_efficiency'].str.extract(r'SEER ([\\d.]+)').astype(float)\n",
    "    \n",
    "    # HSPF extraction for existing, baseline equipment (Replacement Costs)\n",
    "    df['baseline_HSPF'] = df['hvac_heating_efficiency'].str.extract(r'([\\d.]+) HSPF').astype(float)\n",
    "\n",
    "    # HSPF extraction for upgraded equipment (New Install Costs)\n",
    "    df['ugrade_newInstall_HSPF'] = df['upgrade_hvac_heating_efficiency'].str.extract(r'(\\d+\\.\\d+)')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39eff356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_heating_installation_premium(df, rsMeans_national_avg, cpi_ratio_2021_2013):\n",
    "    necessary_columns = ['hvac_cooling_type', 'heating_type', 'rsMeans_CCI_avg']\n",
    "    if not all(column in df.columns for column in necessary_columns):\n",
    "        raise ValueError(\"DataFrame does not contain all necessary columns.\")\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # Initialization to zero\n",
    "        premium_cost = 0\n",
    "        \n",
    "        # Installation cost for homes with existing AC\n",
    "        # Deetjen: Replace SEER 15, 8.5 HSPF ASHP with SEER 15, 8.5 HSPF ASHP: NREL REMDB 50th Percentile Cost is $3300 USD-2013        \n",
    "        if row['hvac_cooling_type'] != 'None':\n",
    "            premium_cost = 0\n",
    "        \n",
    "        # Installation cost for homes without central AC, but an existing furnace or baseboard\n",
    "        # Deetjen: Install SEER 15, 8.5 HSPF ASHP: NREL REMDB 50th Percentile Cost is $3700 USD-2013        \n",
    "        elif 'Furnace' in row['heating_type'] or 'Baseboard' in row['heating_type']:\n",
    "            premium_cost = 400 * cpi_ratio_2021_2013\n",
    "        \n",
    "        # Installation cost for homes without central AC and an existing boiler as heating system\n",
    "        # Deetjen: Install SEER 15, 8.5 HSPF ASHP: NREL REMDB High Cost is $4800 USD-2013        \n",
    "        elif 'Boiler' in row['heating_type']:\n",
    "            premium_cost = 1500 * cpi_ratio_2021_2013\n",
    "        \n",
    "        # Apply CPI adjustment above and regional cost index adjustment below\n",
    "        adjusted_cost = round(premium_cost * (row['rsMeans_CCI_avg'] / rsMeans_national_avg), 2)\n",
    "        df.at[index, f'mp{menu_mp}_heating_installation_premium'] = adjusted_cost\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacement Cost Function and Helper Functions (Parametes, Formula)\n",
    "\n",
    "# Helper function to get parameters based on end use\n",
    "def get_end_use_replacement_parameters(df, end_use):\n",
    "    parameters = {\n",
    "        'heating': {\n",
    "            'conditions': [\n",
    "                (df['base_heating_fuel'] == 'Propane'),\n",
    "                (df['base_heating_fuel'] == 'Fuel Oil'),\n",
    "                (df['base_heating_fuel'] == 'Natural Gas'),\n",
    "                (df['base_heating_fuel'] == 'Electricity') & (df['heating_type'] == 'Electricity ASHP'),\n",
    "                (df['base_heating_fuel'] == 'Electricity')\n",
    "            ],\n",
    "            'tech_eff_pairs': [\n",
    "                ('Propane Furnace', '94 AFUE'),\n",
    "                ('Fuel Oil Furnace', '95 AFUE'),\n",
    "                ('Natural Gas Furnace', '95 AFUE'),\n",
    "                ('Electric ASHP', 'SEER 18, 9.3 HSPF'),\n",
    "                ('Electric Furnace', '100 AFUE')\n",
    "            ],\n",
    "            'cost_components': ['unitCost', 'otherCost', 'cost_per_kBtuh']\n",
    "        },\n",
    "        'waterHeating': {\n",
    "            'conditions': [\n",
    "                (df['base_waterHeating_fuel'] == 'Fuel Oil'),\n",
    "                (df['base_waterHeating_fuel'] == 'Natural Gas'),\n",
    "                (df['base_waterHeating_fuel'] == 'Propane'),\n",
    "                (df['water_heater_efficiency'].isin(['Electric Standard', 'Electric Premium'])),\n",
    "                (df['water_heater_efficiency'] == 'Electric Heat Pump, 80 gal')\n",
    "            ],\n",
    "            'tech_eff_pairs': [\n",
    "                ('Fuel Oil Water Heater', 0.68),\n",
    "                ('Natural Gas Water Heater', 0.67),\n",
    "                ('Propane Water Heater', 0.67),\n",
    "                ('Electric Water Heater', 0.95),\n",
    "                ('Electric Heat Pump Water Heater, 80 gal', 2.35)\n",
    "            ],\n",
    "            'cost_components': ['unitCost', 'cost_per_gallon']\n",
    "        },\n",
    "        'clothesDrying': {\n",
    "            'conditions': [\n",
    "                (df['base_clothesDrying_fuel'] == 'Electricity'),\n",
    "                (df['base_clothesDrying_fuel'] == 'Natural Gas'),\n",
    "                (df['base_clothesDrying_fuel'] == 'Propane')\n",
    "            ],\n",
    "            'tech_eff_pairs': [\n",
    "                ('Electric Clothes Dryer', 3.1),\n",
    "                ('Natural Gas Clothes Dryer', 2.75),\n",
    "                ('Propane Clothes Dryer', 2.75)\n",
    "            ],\n",
    "            'cost_components': ['unitCost']\n",
    "        },\n",
    "        'cooking': {\n",
    "            'conditions': [\n",
    "                (df['base_cooking_fuel'] == 'Electricity'),\n",
    "                (df['base_cooking_fuel'] == 'Natural Gas'),\n",
    "                (df['base_cooking_fuel'] == 'Propane')\n",
    "            ],\n",
    "            'tech_eff_pairs': [\n",
    "                ('Electric Range', 0.74),\n",
    "                ('Natural Gas Range', 0.4),\n",
    "                ('Propane Range', 0.4)\n",
    "            ],\n",
    "            'cost_components': ['unitCost']\n",
    "        }\n",
    "    }\n",
    "    if end_use not in parameters:\n",
    "        raise ValueError(f\"Invalid end_use specified: {end_use}\")\n",
    "    return parameters[end_use]\n",
    "\n",
    "def calculate_replacement_cost_per_row(df_valid, sampled_costs_dict, rsMeans_national_avg, menu_mp, end_use):\n",
    "    \"\"\"\n",
    "    Helper function to calculate the replacement cost for each row based on the end use.\n",
    "\n",
    "    Parameters:\n",
    "    df_valid (pd.DataFrame): Filtered DataFrame containing valid rows.\n",
    "    sampled_costs_dict (dict): Dictionary with sampled costs for each component.\n",
    "    rsMeans_national_avg (float): National average value for cost adjustment.\n",
    "    menu_mp (int): Menu option identifier.\n",
    "    end_use (str): Type of end-use to calculate replacement cost for ('heating', 'waterHeating', 'clothesDrying', 'cooking').\n",
    "\n",
    "    Returns:\n",
    "    tuple: Tuple containing the calculated replacement costs and the cost column name.\n",
    "    \"\"\"\n",
    "    if end_use == 'heating':\n",
    "        replacement_cost = (\n",
    "            sampled_costs_dict['unitCost'] +\n",
    "            sampled_costs_dict['otherCost'] +\n",
    "            (df_valid['total_heating_load_kBtuh'] * sampled_costs_dict['cost_per_kBtuh'])\n",
    "        ) * (df_valid['rsMeans_CCI_avg'] / rsMeans_national_avg)\n",
    "        cost_column_name = f'mp{menu_mp}_heating_replacementCost'\n",
    "    elif end_use == 'waterHeating':\n",
    "        replacement_cost = (\n",
    "            sampled_costs_dict['unitCost'] +\n",
    "            (sampled_costs_dict['cost_per_gallon'] * df_valid['size_water_heater_gal'])\n",
    "        ) * (df_valid['rsMeans_CCI_avg'] / rsMeans_national_avg)\n",
    "        cost_column_name = f'mp{menu_mp}_waterHeating_replacementCost'\n",
    "    else:\n",
    "        replacement_cost = sampled_costs_dict['unitCost'] * (df_valid['rsMeans_CCI_avg'] / rsMeans_national_avg)\n",
    "        cost_column_name = f'mp{menu_mp}_{end_use}_replacementCost'\n",
    "    \n",
    "    return replacement_cost, cost_column_name\n",
    "\n",
    "def calculate_replacement_cost(df, cost_dict, rsMeans_national_avg, menu_mp, end_use):\n",
    "    \"\"\"\n",
    "    General function to calculate replacement costs for various end-uses based on fuel types, costs, and efficiency.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing data for different scenarios.\n",
    "    cost_dict (dict): Dictionary with cost information for different technology and efficiency combinations.\n",
    "    rsMeans_national_avg (float): National average value for cost adjustment.\n",
    "    menu_mp (int): Menu option identifier.\n",
    "    end_use (str): Type of end-use to calculate replacement cost for ('heating', 'waterHeating', 'clothesDrying', 'cooking').\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Updated DataFrame with calculated replacement costs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate menu_mp\n",
    "    valid_menu_mps = [7, 8, 9, 10]\n",
    "    if menu_mp not in valid_menu_mps:\n",
    "        raise ValueError(\"Please enter a valid measure package number for menu_mp. Should be 7, 8, 9, or 10.\")\n",
    "    \n",
    "    # Get conditions, technology-efficiency pairs, and cost components for the specified end_use\n",
    "    params = get_end_use_replacement_parameters(df, end_use)\n",
    "    conditions = params['conditions']\n",
    "    tech_eff_pairs = params['tech_eff_pairs']\n",
    "    cost_components = params['cost_components']\n",
    "   \n",
    "    # Map each condition to its tech and efficiency\n",
    "    tech = np.select(conditions, [pair[0] for pair in tech_eff_pairs], default='unknown')\n",
    "    eff = np.select(conditions, [pair[1] for pair in tech_eff_pairs], default=np.nan)\n",
    "\n",
    "    # Convert efficiency values to appropriate types\n",
    "    if end_use == 'heating':\n",
    "        eff = np.array([str(e) if e != 'unknown' else np.nan for e in eff])\n",
    "    else:\n",
    "        eff = np.array([float(e) if e != 'unknown' else np.nan for e in eff])\n",
    "\n",
    "    # Filter out rows with unknown technology and NaN efficiency\n",
    "    valid_indices = tech != 'unknown'\n",
    "    tech = tech[valid_indices]\n",
    "    eff = eff[valid_indices]\n",
    "    df_valid = df.loc[valid_indices].copy()\n",
    "\n",
    "    # Initialize dictionaries to store sampled costs\n",
    "    sampled_costs_dict = {}\n",
    "\n",
    "    # Calculate costs for each component\n",
    "    for cost_component in cost_components:\n",
    "        progressive_costs = np.array([cost_dict.get((t, e), {}).get(f'{cost_component}_progressive', np.nan) for t, e in zip(tech, eff)])\n",
    "        reference_costs = np.array([cost_dict.get((t, e), {}).get(f'{cost_component}_reference', np.nan) for t, e in zip(tech, eff)])\n",
    "        conservative_costs = np.array([cost_dict.get((t, e), {}).get(f'{cost_component}_conservative', np.nan) for t, e in zip(tech, eff)])\n",
    "\n",
    "        # Handle missing cost data\n",
    "        if np.isnan(progressive_costs).any() or np.isnan(reference_costs).any() or np.isnan(conservative_costs).any():\n",
    "            missing_indices = np.where(np.isnan(progressive_costs) | np.isnan(reference_costs) | np.isnan(conservative_costs))\n",
    "            print(f\"Missing data at indices: {missing_indices}\")\n",
    "            print(f\"Tech with missing data: {tech[missing_indices]}\")\n",
    "            print(f\"Efficiencies with missing data: {eff[missing_indices]}\")\n",
    "            \n",
    "            raise ValueError(f\"Missing cost data for some technology and efficiency combinations in cost_component {cost_component}\")\n",
    "\n",
    "        # Calculate mean and standard deviation assuming the costs represent the 10th, 50th, and 90th percentiles of a normal distribution\n",
    "        mean_costs = reference_costs\n",
    "        std_costs = (conservative_costs - progressive_costs) / (norm.ppf(0.90) - norm.ppf(0.10))\n",
    "\n",
    "        # Sample from the normal distribution for each row\n",
    "        sampled_costs = np.random.normal(loc=mean_costs, scale=std_costs)\n",
    "        sampled_costs_dict[cost_component] = sampled_costs\n",
    "\n",
    "    # Calculate the replacement cost for each row\n",
    "    replacement_cost, cost_column_name = calculate_replacement_cost_per_row(df_valid, sampled_costs_dict, rsMeans_national_avg, menu_mp, end_use)\n",
    "\n",
    "    # Add the calculated costs to the DataFrame, rounded to 2 decimal places\n",
    "    df_valid.loc[:, cost_column_name] = np.round(replacement_cost, 2)\n",
    "\n",
    "    # Reintegrate the valid rows back into the original DataFrame\n",
    "    df.loc[valid_indices, cost_column_name] = df_valid[cost_column_name]\n",
    "\n",
    "    return df\n",
    "\n",
    "# # Example usage\n",
    "# calculate_replacement_cost(df=df_euss_am_mp8_home,\n",
    "#                            cost_dict=dict_waterHeating_equipment_cost,\n",
    "#                            rsMeans_national_avg=rsMeans_national_avg,\n",
    "#                            menu_mp=menu_mp, \n",
    "#                            end_use='waterHeating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e83772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation Cost Function and Helper Functions (Parametes, Formula)\n",
    "\n",
    "# Helper function to get parameters based on end use\n",
    "def get_end_use_installation_parameters(df, end_use):\n",
    "    parameters = {\n",
    "        'heating': {\n",
    "            'conditions': [\n",
    "                (df['hvac_has_ducts'] == 'Yes') & (menu_mp == 7),\n",
    "                (df['hvac_has_ducts'] == 'No') & (menu_mp == 7),\n",
    "                (df['hvac_has_ducts'] == 'Yes') & (menu_mp != 7),\n",
    "                (df['hvac_has_ducts'] == 'No') & (menu_mp != 7)\n",
    "            ],\n",
    "            'tech_eff_pairs': [\n",
    "                ('Electric ASHP', 'SEER 18, 9.3 HSPF'),\n",
    "                ('Electric MSHP', 'SEER 18, 9.6 HSPF'),\n",
    "                ('Electric MSHP - Ducted', 'SEER 15.5, 10 HSPF'),\n",
    "                ('Electric MSHP', 'SEER 29.3, 14 HSPF')\n",
    "            ],\n",
    "            'cost_components': ['unitCost', 'otherCost', 'cost_per_kBtuh']\n",
    "        },\n",
    "        'waterHeating': {\n",
    "            'conditions': [\n",
    "                (df['upgrade_water_heater_efficiency'] == 'Electric Heat Pump, 50 gal, 3.45 UEF'),\n",
    "                (df['upgrade_water_heater_efficiency'] == 'Electric Heat Pump, 66 gal, 3.35 UEF'),\n",
    "                (df['upgrade_water_heater_efficiency'] == 'Electric Heat Pump, 80 gal, 3.45 UEF')\n",
    "            ],\n",
    "            'tech_eff_pairs': [\n",
    "                ('Electric Heat Pump Water Heater, 50 gal', 3.45),\n",
    "                ('Electric Heat Pump Water Heater, 66 gal', 3.35),\n",
    "                ('Electric Heat Pump Water Heater, 80 gal', 3.45),\n",
    "            ],\n",
    "            'cost_components': ['unitCost', 'cost_per_gallon']\n",
    "        },\n",
    "        'clothesDrying': {\n",
    "            'conditions': [\n",
    "                df['upgrade_clothes_dryer'].str.contains('Electric, Premium, Heat Pump, Ventless', na=False),\n",
    "                ~df['upgrade_clothes_dryer'].str.contains('Electric, Premium, Heat Pump, Ventless', na=False),\n",
    "            ],\n",
    "            'tech_eff_pairs': [\n",
    "                ('Electric HP Clothes Dryer', 5.2),\n",
    "                ('Electric Clothes Dryer', 3.1),\n",
    "            ],\n",
    "            'cost_components': ['unitCost']\n",
    "        },\n",
    "        'cooking': {\n",
    "            'conditions': [\n",
    "                df['upgrade_cooking_range'].str.contains('Electric, Induction', na=False),\n",
    "                ~df['upgrade_cooking_range'].str.contains('Electric, Induction', na=False),\n",
    "            ],\n",
    "            'tech_eff_pairs': [\n",
    "                ('Electric Induction Range', 0.84),\n",
    "                ('Electric Range, Modern', 0.74),\n",
    "            ],\n",
    "            'cost_components': ['unitCost']\n",
    "        }\n",
    "    }\n",
    "    if end_use not in parameters:\n",
    "        raise ValueError(f\"Invalid end_use specified: {end_use}\")\n",
    "    return parameters[end_use]\n",
    "\n",
    "def calculate_installation_cost_per_row(df_valid, sampled_costs_dict, rsMeans_national_avg, menu_mp, end_use):\n",
    "    \"\"\"\n",
    "    Helper function to calculate the installation cost for each row based on the end use.\n",
    "\n",
    "    Parameters:\n",
    "    df_valid (pd.DataFrame): Filtered DataFrame containing valid rows.\n",
    "    sampled_costs_dict (dict): Dictionary with sampled costs for each component.\n",
    "    rsMeans_national_avg (float): National average value for cost adjustment.\n",
    "    menu_mp (int): Menu option identifier.\n",
    "    end_use (str): Type of end-use to calculate installation cost for ('heating', 'waterHeating', 'clothesDrying', 'cooking').\n",
    "\n",
    "    Returns:\n",
    "    tuple: Tuple containing the calculated installation costs and the cost column name.\n",
    "    \"\"\"\n",
    "    if end_use == 'heating':\n",
    "        installation_cost = (\n",
    "            sampled_costs_dict['unitCost'] +\n",
    "            sampled_costs_dict['otherCost'] +\n",
    "            (df_valid['total_heating_load_kBtuh'] * sampled_costs_dict['cost_per_kBtuh'])\n",
    "        ) * (df_valid['rsMeans_CCI_avg'] / rsMeans_national_avg)\n",
    "        cost_column_name = f'mp{menu_mp}_heating_installationCost'\n",
    "    elif end_use == 'waterHeating':\n",
    "        installation_cost = (\n",
    "            sampled_costs_dict['unitCost'] +\n",
    "            (sampled_costs_dict['cost_per_gallon'] * df_valid['size_water_heater_gal'])\n",
    "        ) * (df_valid['rsMeans_CCI_avg'] / rsMeans_national_avg)\n",
    "        cost_column_name = f'mp{menu_mp}_waterHeating_installationCost'\n",
    "    else:\n",
    "        installation_cost = sampled_costs_dict['unitCost'] * (df_valid['rsMeans_CCI_avg'] / rsMeans_national_avg)\n",
    "        cost_column_name = f'mp{menu_mp}_{end_use}_installationCost'\n",
    "    \n",
    "    return installation_cost, cost_column_name\n",
    "\n",
    "def calculate_installation_cost(df, cost_dict, rsMeans_national_avg, menu_mp, end_use):\n",
    "    \"\"\"\n",
    "    General function to calculate installation costs for various end-uses based on fuel types, costs, and efficiency.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing data for different scenarios.\n",
    "    cost_dict (dict): Dictionary with cost information for different technology and efficiency combinations.\n",
    "    rsMeans_national_avg (float): National average value for cost adjustment.\n",
    "    menu_mp (int): Menu option identifier.\n",
    "    end_use (str): Type of end-use to calculate installation cost for ('heating', 'waterHeating', 'clothesDrying', 'cooking').\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Updated DataFrame with calculated installation costs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate menu_mp \n",
    "    valid_menu_mps = [7, 8, 9, 10]\n",
    "    if menu_mp not in valid_menu_mps:\n",
    "        raise ValueError(\"Please enter a valid measure package number for menu_mp. Should be 7, 8, 9, or 10.\")\n",
    "    \n",
    "    # Get conditions, technology-efficiency pairs, and cost components for the specified end_use\n",
    "    params = get_end_use_installation_parameters(df, end_use)\n",
    "    conditions = params['conditions']\n",
    "    tech_eff_pairs = params['tech_eff_pairs']\n",
    "    cost_components = params['cost_components']\n",
    "   \n",
    "    # Map each condition to its tech and efficiency\n",
    "    tech = np.select(conditions, [pair[0] for pair in tech_eff_pairs], default='unknown')\n",
    "    eff = np.select(conditions, [pair[1] for pair in tech_eff_pairs], default=np.nan)\n",
    "\n",
    "    # Convert efficiency values to appropriate types\n",
    "    if end_use == 'heating':\n",
    "        eff = np.array([str(e) if e != 'unknown' else np.nan for e in eff])\n",
    "    else:\n",
    "        eff = np.array([float(e) if e != 'unknown' else np.nan for e in eff])\n",
    "\n",
    "    # Filter out rows with unknown technology and NaN efficiency\n",
    "    valid_indices = tech != 'unknown'\n",
    "    tech = tech[valid_indices]\n",
    "    eff = eff[valid_indices]\n",
    "    df_valid = df.loc[valid_indices].copy()\n",
    "\n",
    "    # Initialize dictionaries to store sampled costs\n",
    "    sampled_costs_dict = {}\n",
    "\n",
    "    # Calculate costs for each component\n",
    "    for cost_component in cost_components:\n",
    "        progressive_costs = np.array([cost_dict.get((t, e), {}).get(f'{cost_component}_progressive', np.nan) for t, e in zip(tech, eff)])\n",
    "        reference_costs = np.array([cost_dict.get((t, e), {}).get(f'{cost_component}_reference', np.nan) for t, e in zip(tech, eff)])\n",
    "        conservative_costs = np.array([cost_dict.get((t, e), {}).get(f'{cost_component}_conservative', np.nan) for t, e in zip(tech, eff)])\n",
    "\n",
    "        # Handle missing cost data\n",
    "        if np.isnan(progressive_costs).any() or np.isnan(reference_costs).any() or np.isnan(conservative_costs).any():\n",
    "            missing_indices = np.where(np.isnan(progressive_costs) | np.isnan(reference_costs) | np.isnan(conservative_costs))\n",
    "            print(f\"Missing data at indices: {missing_indices}\")\n",
    "            print(f\"Tech with missing data: {tech[missing_indices]}\")\n",
    "            print(f\"Efficiencies with missing data: {eff[missing_indices]}\")\n",
    "            \n",
    "            raise ValueError(f\"Missing cost data for some technology and efficiency combinations in cost_component {cost_component}\")\n",
    "\n",
    "        # Calculate mean and standard deviation assuming the costs represent the 10th, 50th, and 90th percentiles of a normal distribution\n",
    "        mean_costs = reference_costs\n",
    "        std_costs = (conservative_costs - progressive_costs) / (norm.ppf(0.90) - norm.ppf(0.10))\n",
    "\n",
    "        # Sample from the normal distribution for each row\n",
    "        sampled_costs = np.random.normal(loc=mean_costs, scale=std_costs)\n",
    "        sampled_costs_dict[cost_component] = sampled_costs\n",
    "\n",
    "    # Calculate the installation cost for each row\n",
    "    installation_cost, cost_column_name = calculate_installation_cost_per_row(df_valid, sampled_costs_dict, rsMeans_national_avg, menu_mp, end_use)\n",
    "\n",
    "    # Add the calculated costs to the DataFrame, rounded to 2 decimal places\n",
    "    df_valid.loc[:, cost_column_name] = np.round(installation_cost, 2)\n",
    "\n",
    "    # Reintegrate the valid rows back into the original DataFrame\n",
    "    df.loc[valid_indices, cost_column_name] = df_valid[cost_column_name]\n",
    "\n",
    "    return df\n",
    "\n",
    "# # Example usage\n",
    "# calculate_installation_cost(df=df_euss_am_mp8_home,\n",
    "#                            cost_dict=dict_waterHeating_equipment_cost,\n",
    "#                            rsMeans_national_avg=rsMeans_national_avg,\n",
    "#                            menu_mp=menu_mp, \n",
    "#                            end_use='waterHeating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb774d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_private_NPV(df, interest_rate, ira_rebates=False):\n",
    "    \"\"\"\n",
    "    Calculate the private net present value (NPV) for various equipment categories,\n",
    "    considering different cost assumptions and potential IRA rebates. The function adjusts\n",
    "    equipment costs for inflation and regional cost differences, and calculates NPV based\n",
    "    on cost savings between baseline and retrofit scenarios.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): Input DataFrame with installation costs, fuel savings, and potential rebates.\n",
    "        interest_rate (float): Annual discount rate used for NPV calculation.\n",
    "        ira_rebates (bool): Flag to consider IRA rebates in calculations.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The input DataFrame updated with calculated private NPV and adjusted equipment costs.\n",
    "    \"\"\"\n",
    "    # Define the lifetimes of different equipment categories    \n",
    "    equipment_specs = {\n",
    "        'heating': 15,\n",
    "        'waterHeating': 12,\n",
    "        'clothesDrying': 13,\n",
    "        'cooking': 15\n",
    "    }\n",
    "    \n",
    "    new_columns_df = pd.DataFrame(index=df.index)  # To hold new columns\n",
    "    \n",
    "    # Iterate over each equipment category to calculate financial metrics\n",
    "    for category, lifetime in equipment_specs.items():\n",
    "        total_capital_cost, net_capital_cost = calculate_costs(df, category, ira_rebates)\n",
    "        calculate_and_update_npv(new_columns_df, df, category, interest_rate, lifetime, total_capital_cost, net_capital_cost, ira_rebates)\n",
    "    \n",
    "    # Add all columns from new_columns_df at once with .concatenate to avoid dataframe fragmentation\n",
    "    df = pd.concat([df, new_columns_df], axis=1)\n",
    "    return df\n",
    "\n",
    "def calculate_costs(df, category, ira_rebates):\n",
    "    \"\"\"\n",
    "    Calculate total and net capital costs based on the equipment category and cost assumptions.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame containing cost data.\n",
    "        category (str): Equipment category.\n",
    "        ira_rebates (bool): Flag indicating whether IRA rebates are applied.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Total and net capital costs.\n",
    "    \"\"\"\n",
    "    if not ira_rebates:\n",
    "        if category == 'heating':\n",
    "            # Include specific weatherization upgrade costs based on scenarios and calculate private NPV\n",
    "            if input_mp == 'upgrade09':            \n",
    "                weatherization_cost = df[f'mp9_enclosure_upgradeCost']\n",
    "            elif input_mp == 'upgrade10':\n",
    "                weatherization_cost = df[f'mp10_enclosure_upgradeCost']\n",
    "            else:\n",
    "                weatherization_cost = 0.0\n",
    "            \n",
    "            # Add together\n",
    "            total_capital_cost = df[f'mp{menu_mp}_{category}_installationCost'] + weatherization_cost + df[f'mp{menu_mp}_heating_installation_premium']\n",
    "            net_capital_cost = total_capital_cost - df[f'mp{menu_mp}_{category}_replacementCost']\n",
    "            \n",
    "        else:\n",
    "            total_capital_cost = df[f'mp{menu_mp}_{category}_installationCost']\n",
    "            net_capital_cost = total_capital_cost - df[f'mp{menu_mp}_{category}_replacementCost']\n",
    "    else:\n",
    "        if category == 'heating':\n",
    "            # Include specific weatherization upgrade costs based on scenarios and calculate private NPV\n",
    "            if input_mp == 'upgrade09':            \n",
    "                weatherization_cost = df[f'mp9_enclosure_upgradeCost'] - df[f'weatherization_rebate_amount']\n",
    "            elif input_mp == 'upgrade10':\n",
    "                weatherization_cost = df[f'mp10_enclosure_upgradeCost'] - df[f'weatherization_rebate_amount']\n",
    "            else:\n",
    "                weatherization_cost = 0.0       \n",
    "            \n",
    "            installation_cost = df[f'mp{menu_mp}_{category}_installationCost'] + weatherization_cost + df[f'mp{menu_mp}_{category}_installation_premium']\n",
    "            rebate_amount = df[f'mp{menu_mp}_{category}_rebate_amount']\n",
    "            total_capital_cost = installation_cost - rebate_amount\n",
    "            net_capital_cost = total_capital_cost - df[f'mp{menu_mp}_{category}_replacementCost']\n",
    "        \n",
    "        else:\n",
    "            installation_cost = df[f'mp{menu_mp}_{category}_installationCost']\n",
    "            rebate_amount = df[f'mp{menu_mp}_{category}_rebate_amount']\n",
    "            total_capital_cost = installation_cost - rebate_amount\n",
    "            net_capital_cost = total_capital_cost - df[f'mp{menu_mp}_{category}_replacementCost']\n",
    "\n",
    "    return total_capital_cost, net_capital_cost\n",
    "\n",
    "def calculate_and_update_npv(new_columns_df, df, category, interest_rate, lifetime, total_capital_cost, net_capital_cost, ira_rebates):\n",
    "    \"\"\"\n",
    "    Calculate and update the NPV values in the DataFrame based on provided capital costs.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): DataFrame to update.\n",
    "        category (str): Equipment category.\n",
    "        interest_rate (float): Discount rate for NPV calculation.\n",
    "        lifetime (int): Expected lifetime of the equipment.\n",
    "        total_capital_cost (float): Total capital cost of the equipment.\n",
    "        net_capital_cost (float): Net capital cost after considering replacements.\n",
    "    \"\"\"\n",
    "    savings = df[f'mp{menu_mp}_{category}_savings_fuelCost']\n",
    "    discount_factor = (1 - ((1 + interest_rate) ** (-lifetime))) / interest_rate\n",
    "    \n",
    "    # Store total and net capital costs and calculate retrofit lifecycle costs (private NPV)\n",
    "    if not ira_rebates:\n",
    "        # Assigning NPV calculations to new_columns_df\n",
    "        new_columns_df[f'mp{menu_mp}_{category}_total_capitalCost'] = total_capital_cost\n",
    "        new_columns_df[f'mp{menu_mp}_{category}_net_capitalCost'] = net_capital_cost\n",
    "        \n",
    "        new_columns_df[f'mp{menu_mp}_{category}_private_npv_total'] = round(savings * discount_factor - total_capital_cost, 2)\n",
    "        new_columns_df[f'mp{menu_mp}_{category}_private_npv'] = round(savings * discount_factor - net_capital_cost, 2)\n",
    "    else:\n",
    "        # Assigning NPV calculations to new_columns_df with IRA prefixes\n",
    "        new_columns_df[f'ira_mp{menu_mp}_{category}_total_capitalCost'] = total_capital_cost\n",
    "        new_columns_df[f'ira_mp{menu_mp}_{category}_net_capitalCost'] = net_capital_cost\n",
    "        \n",
    "        new_columns_df[f'ira_mp{menu_mp}_{category}_private_npv_total'] = round(savings * discount_factor - total_capital_cost, 2)\n",
    "        new_columns_df[f'ira_mp{menu_mp}_{category}_private_npv'] = round(savings * discount_factor - net_capital_cost, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e56595c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def adoption_decision(df, ira_rebates=False, grid_decarb=False):\n",
    "    \"\"\"\n",
    "    Updates the provided DataFrame with new columns that reflect decisions about equipment adoption\n",
    "    and public impacts based on net present values (NPV). The function handles different scenarios\n",
    "    based on input flags for incentives and grid decarbonization.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The DataFrame containing home equipment data.\n",
    "        ira_rebates (bool, optional): Flag to include IRA rebates in calculations. Defaults to False.\n",
    "        grid_decarb (bool, optional): Flag to include grid decarbonization impacts. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The modified DataFrame with additional columns for decisions and impacts.\n",
    "\n",
    "    Notes:\n",
    "        - The function handles multiple cost assumptions ('progressive', 'reference', 'conservative').\n",
    "        - It adds columns for both individual and public economic evaluations.\n",
    "        - Adoption decisions and public impacts are dynamically calculated based on the input parameters.\n",
    "    \"\"\"\n",
    "    # Define the lifetimes of different equipment categories\n",
    "    upgrade_columns = {\n",
    "        'heating': 'upgrade_hvac_heating_efficiency',\n",
    "        'waterHeating': 'upgrade_water_heater_efficiency',\n",
    "        'clothesDrying': 'upgrade_clothes_dryer',\n",
    "        'cooking': 'upgrade_cooking_range'\n",
    "    }\n",
    "    \n",
    "    # Define different cost scenarios\n",
    "    cost_assumptions = ['progressive', 'reference', 'conservative']\n",
    "    new_columns_df = pd.DataFrame(index=df.index)  # DataFrame to hold new or modified columns\n",
    "\n",
    "    # Iterate over each equipment category and its respective upgrade column\n",
    "    for category, upgrade_column in upgrade_columns.items():\n",
    "        # Determine prefix and suffix based on IRA rebates and grid decarbonization\n",
    "        prefix = f'ira_' if ira_rebates else ''\n",
    "        grid_suffix = 'gridDecarb_' if grid_decarb and ira_rebates else ''\n",
    "\n",
    "        # Column names for net NPV, private NPV, and public NPV\n",
    "        net_npv_col_name = f'{prefix}{grid_suffix}mp{menu_mp}_{category}_net_npv'\n",
    "        private_npv_col = f'{prefix}mp{menu_mp}_{category}_private_npv'\n",
    "        public_npv_col = f'{grid_suffix}mp{menu_mp}_{category}_public_npv'\n",
    "\n",
    "        # Calculate net NPV by summing private and public NPVs\n",
    "        new_columns_df[net_npv_col_name] = df[private_npv_col] + df[public_npv_col]\n",
    "\n",
    "        # Initialize columns for adoption decisions and public impact\n",
    "        adoption_col_name = f'{prefix}{grid_suffix}mp{menu_mp}_{category}_adoption'\n",
    "        retrofit_col_name = f'{grid_suffix}mp{menu_mp}_{category}_retrofit_publicImpact'\n",
    "        new_columns_df[adoption_col_name] = 'Averse to Adoption'  # Default value for all rows\n",
    "        new_columns_df[retrofit_col_name] = 'No Retrofit'  # Default public impact\n",
    "\n",
    "        # Conditions for determining adoption decisions\n",
    "        conditions = [\n",
    "            (df[f'mp{menu_mp}_{category}_reduction_consumption'] == 0) | df[upgrade_column].isna(),\n",
    "            df[private_npv_col] > 0,\n",
    "            (df[private_npv_col] <= 0) & (new_columns_df[net_npv_col_name] > 0)\n",
    "        ]\n",
    "        choices = ['Existing Equipment', 'Adoption', 'Potential Adoption with Subsidy']\n",
    "        new_columns_df[adoption_col_name] = np.select(conditions, choices, default='Averse to Adoption')\n",
    "\n",
    "        # Conditions and choices for public impacts\n",
    "        public_conditions = [\n",
    "            df[public_npv_col] > 0,\n",
    "            df[public_npv_col] < 0\n",
    "        ]\n",
    "        public_choices = ['Public Benefit', 'Public Detriment']\n",
    "        new_columns_df[retrofit_col_name] = np.select(public_conditions, public_choices, default='No Retrofit')\n",
    "\n",
    "    # Concatenate the new columns DataFrame to the original DataFrame once, outside the loop\n",
    "    df = pd.concat([df, new_columns_df], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b219b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_adoption_consistency(df, category, upgrade_column):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    cols_to_display = ['bldg_id',\n",
    "                       f'base_{category}_fuel',\n",
    "                       f'{upgrade_column}',\n",
    "                       f'baseline_{category}_consumption',\n",
    "                       f'mp{menu_mp}_{category}_consumption',\n",
    "                       f'mp{menu_mp}_{category}_reduction_consumption',\n",
    "                       f'baseline_{category}_fuelCost',\n",
    "                       f'mp{menu_mp}_{category}_fuelCost',        \n",
    "                       f'mp{menu_mp}_{category}_savings_fuelCost',\n",
    "                       f'mp{menu_mp}_{category}_net_capitalCost',\n",
    "                       f'mp{menu_mp}_{category}_private_npv',\n",
    "                       f'baseline_{category}_damages_health',\n",
    "                       f'baseline_{category}_damages_climate',\n",
    "                       f'mp{menu_mp}_{category}_damages_health',\n",
    "                       f'mp{menu_mp}_{category}_damages_climate',\n",
    "                       f'mp{menu_mp}_{category}_reduction_damages_health',\n",
    "                       f'mp{menu_mp}_{category}_reduction_damages_climate',\n",
    "                       f'mp{menu_mp}_{category}_public_npv',\n",
    "                       f'mp{menu_mp}_{category}_retrofit_publicImpact',\n",
    "                       f'mp{menu_mp}_{category}_net_npv',\n",
    "                       f'mp{menu_mp}_{category}_adoption',  \n",
    "                       ]    \n",
    "        \n",
    "    # Filter the dataframe to show only the columns relevant for the current cost_type\n",
    "    df_filtered = df_copy[cols_to_display]\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d357334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(df_compare, category, upgrade_column):\n",
    "    \"\"\"\n",
    "    Summarizes results from a DataFrame based on the given category.\n",
    "\n",
    "    Parameters:\n",
    "        df_compare (DataFrame): The DataFrame containing the results to be summarized.\n",
    "        category (str): The category for which the results should be summarized.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The summarized results DataFrame.\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the category is related to heating or water heating\n",
    "    # These are the only end-uses with Fuel Oil\n",
    "    if 'heating' in category or 'waterHeating' in category:\n",
    "        fuels = ['electricity', 'fuelOil', 'naturalGas', 'propane']\n",
    "    # Cooking and Clothes Drying do not use Fuel Oil versions\n",
    "    else:\n",
    "        fuels = ['electricity', 'naturalGas', 'propane']\n",
    "    \n",
    "    years = [\n",
    "        '2018', '2019', '2020', '2021', '2022', '2023', '2024', '2025',\n",
    "        '2026', '2027', '2028', '2029', '2030', '2031', '2032', '2033',\n",
    "        '2034', '2035', '2036', '2037', '2038', '2039', '2040', '2041',\n",
    "        '2042', '2043', '2044', '2045', '2046', '2047', '2048', '2049', '2050'\n",
    "    ]\n",
    "    \n",
    "    # Define the columns to be displayed in the summarized results\n",
    "    cols_to_display = ['bldg_id',\n",
    "                       'state',\n",
    "                       'county',\n",
    "                       'puma',\n",
    "                       'square_footage',\n",
    "                       'income',\n",
    "                       'federal_poverty_level',\n",
    "                       'occupancy',\n",
    "                       'tenure',\n",
    "                       f'base_{category}_fuel'] + [f'base_{fuel}_{category}_consumption' for fuel in fuels] + [\n",
    "                       f'baseline_{category}_consumption',\n",
    "                       f'{upgrade_column}',\n",
    "                       f'mp{menu_mp}_{category}_consumption', \n",
    "                       f'mp{menu_mp}_{category}_reduction_consumption',\n",
    "                       f'baseline_{category}_fuelCost',\n",
    "                       f'mp{menu_mp}_{category}_fuelCost',        \n",
    "                       f'mp{menu_mp}_{category}_savings_fuelCost',\n",
    "                       f'mp{menu_mp}_{category}_delta_fuelCost',\n",
    "                       f'baseline_{category}_damages_health',\n",
    "                       f'baseline_{category}_damages_climate',\n",
    "                       f'mp{menu_mp}_{category}_damages_health',\n",
    "                       f'mp{menu_mp}_{category}_damages_climate',\n",
    "                       f'mp{menu_mp}_{category}_reduction_damages_health',\n",
    "                       f'mp{menu_mp}_{category}_reduction_damages_climate',    \n",
    "                       f'mp{menu_mp}_{category}_public_npv',\n",
    "                       f'mp{menu_mp}_{category}_retrofit_publicImpact']\n",
    "\n",
    "\n",
    "    # The individual year calculation (2018) indicates that it is the gridDecarb scenario\n",
    "    if f'2018_mp{menu_mp}_{category}_damages_health' in df_compare.columns or f'2018_mp{menu_mp}_{category}_damages_climate' in df_compare.columns:\n",
    "        # Add columns for gradually decarbonizing grid\n",
    "        for year in years:\n",
    "            damages_gridDecarb_columns = [f'{year}_mp{menu_mp}_{category}_damages_health', f'{year}_mp{menu_mp}_{category}_damages_climate']\n",
    "            cols_to_display += damages_gridDecarb_columns\n",
    "\n",
    "    # Add enclosure upgrades for MP9 and MP10 heating\n",
    "    if category == 'heating':\n",
    "        if menu_mp == 9 or menu_mp == 10:\n",
    "            enclosure_columns = [f'mp{menu_mp}_enclosure_upgradeCost']\n",
    "            cols_to_display.extend(enclosure_columns)\n",
    "\n",
    "    # Retrofit Cost and Adoption Cols\n",
    "    retrofit_cost_cols = [\n",
    "        f'mp{menu_mp}_{category}_installationCost',\n",
    "        f'mp{menu_mp}_{category}_replacementCost', \n",
    "        f'mp{menu_mp}_{category}_net_capitalCost',\n",
    "        f'mp{menu_mp}_{category}_private_npv',\n",
    "        f'mp{menu_mp}_{category}_net_npv',\n",
    "        f'mp{menu_mp}_{category}_adoption',  \n",
    "    ]\n",
    "        \n",
    "    cols_to_display.extend(retrofit_cost_cols)  # Use extend to flatten the list\n",
    "        \n",
    "    df_results = df_compare[cols_to_display]\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0339d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_percent_AMI(df_results_IRA):\n",
    "#     \"\"\"\n",
    "#     Calculates the percentage of Area Median Income (AMI) and assigns a designation based on the income level.\n",
    "\n",
    "#     Parameters:\n",
    "#         df_results_IRA (DataFrame): Input DataFrame containing income information.\n",
    "#             - This is the dataframe returned from the summarize_results function\n",
    "#     Returns:\n",
    "#         DataFrame: Modified DataFrame with additional columns for income calculations and designation.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Simplify the conditions for income categorization\n",
    "#     conditions = [\n",
    "#         df_results_IRA['income'] == '<10000',\n",
    "#         df_results_IRA['income'] == '200000+',\n",
    "#         ~df_results_IRA['income'].str.contains(r'\\d+-\\d+')\n",
    "#     ]\n",
    "\n",
    "#     # Define the choices for income categorization\n",
    "#     choices_med = [9999.0, 200000.0, np.nan]\n",
    "\n",
    "#     # Extract the lower and higher bounds of income ranges\n",
    "#     df_results_IRA['income_low'] = df_results_IRA['income'].str.extract(r'(\\d+)-\\d+').astype(float)\n",
    "#     df_results_IRA['income_high'] = df_results_IRA['income'].str.extract(r'\\d+-([\\d+]+)').astype(float)\n",
    "\n",
    "#     # Fill missing values in income_low and income_high with defaults\n",
    "#     df_results_IRA['income_low'].fillna(9999.0, inplace=True)\n",
    "#     df_results_IRA['income_high'].fillna(200000.0, inplace=True)\n",
    "\n",
    "#     # Calculate the median income and assign to income_med column\n",
    "#     df_results_IRA['income_med'] = np.select(\n",
    "#         conditions,\n",
    "#         choices_med,\n",
    "#         default=(df_results_IRA['income_low'] + df_results_IRA['income_high']) / 2\n",
    "#     )\n",
    "\n",
    "#     # Drop the intermediate columns income_low and income_high\n",
    "#     df_results_IRA.drop(['income_low', 'income_high'], axis=1, inplace=True)\n",
    "\n",
    "#     # Perform additional tasks on the DataFrame\n",
    "#     df_results_IRA['census_county_medianIncome'] = df_results_IRA['puma'].map(df_county_medianIncome.set_index('gis_joinID_puma')['median_income_USD2018'])\n",
    "#     df_results_IRA['income_med'] = df_results_IRA['income_med'].astype(float)\n",
    "#     df_results_IRA['census_county_medianIncome'] = df_results_IRA['census_county_medianIncome'].astype(float)\n",
    "#     df_results_IRA['percent_AMI'] = ((df_results_IRA['income_med'] / df_results_IRA['census_county_medianIncome']) * 100).round(2)\n",
    "\n",
    "#     # Categorize the income level based on percent_AMI\n",
    "#     conditions_lmi = [\n",
    "#         df_results_IRA['percent_AMI'] <= 80.0,\n",
    "#         (df_results_IRA['percent_AMI'] > 80.0) & (df_results_IRA['percent_AMI'] <= 150.0)\n",
    "#     ]\n",
    "    \n",
    "#     choices_lmi = ['Low-Income', 'Moderate-Income']\n",
    "\n",
    "#     df_results_IRA['lowModerateIncome_designation'] = np.select(conditions_lmi, choices_lmi, default='Middle-to-Upper-Income')\n",
    "\n",
    "#     # Output the modified DataFrame\n",
    "#     return df_results_IRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a82d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def generate_ami_2018(row):\n",
    "    mean = row['income']\n",
    "    low = row['income_low']\n",
    "    high = row['income_high']\n",
    "    \n",
    "    # Calculate std assuming 10th and 90th percentiles\n",
    "    std = (high - low) / (norm.ppf(0.90) - norm.ppf(0.10))\n",
    "    \n",
    "    # Sample from the normal distribution\n",
    "    ami_2018 = np.random.normal(loc=mean, scale=std)\n",
    "    \n",
    "    # Ensure the generated income is within the bounds\n",
    "    ami_2018 = max(low, min(high, ami_2018))\n",
    "    return ami_2018\n",
    "\n",
    "def calculate_percent_AMI(df_results_IRA, df_county_medianIncome):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of Area Median Income (AMI) and assigns a designation based on the income level.\n",
    "\n",
    "    Parameters:\n",
    "        df_results_IRA (DataFrame): Input DataFrame containing income information.\n",
    "            - This is the dataframe returned from the summarize_results function\n",
    "        df_county_medianIncome (DataFrame): DataFrame containing the median income for each county.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Modified DataFrame with additional columns for income calculations and designation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize income_low and income_high based on income ranges\n",
    "    df_results_IRA['income_low'] = np.nan\n",
    "    df_results_IRA['income_high'] = np.nan\n",
    "\n",
    "    for idx, row in df_results_IRA.iterrows():\n",
    "        if row['income'] == '<10000':\n",
    "            df_results_IRA.at[idx, 'income_low'] = 9999.0\n",
    "            df_results_IRA.at[idx, 'income_high'] = 9999.0\n",
    "            df_results_IRA.at[idx, 'income'] = 9999.0\n",
    "        elif row['income'] == '200000+':\n",
    "            df_results_IRA.at[idx, 'income_low'] = 200000.0\n",
    "            df_results_IRA.at[idx, 'income_high'] = 200000.0\n",
    "            df_results_IRA.at[idx, 'income'] = 200000.0\n",
    "        else:\n",
    "            bounds = row['income'].split('-')\n",
    "            df_results_IRA.at[idx, 'income_low'] = float(bounds[0])\n",
    "            df_results_IRA.at[idx, 'income_high'] = float(bounds[1])\n",
    "            df_results_IRA.at[idx, 'income'] = (df_results_IRA.at[idx, 'income_low'] + df_results_IRA.at[idx, 'income_high']) / 2\n",
    "    \n",
    "    df_results_IRA['income'] = df_results_IRA.apply(generate_ami_2018, axis=1)\n",
    "\n",
    "    # Drop the intermediate columns income_low and income_high\n",
    "    df_results_IRA.drop(['income_low', 'income_high'], axis=1, inplace=True)\n",
    "\n",
    "    # Map the median income from df_county_medianIncome to df_results_IRA\n",
    "    df_results_IRA['census_county_medianIncome'] = df_results_IRA['puma'].map(\n",
    "        df_county_medianIncome.set_index('gis_joinID_puma')['median_income_USD2018']\n",
    "    )\n",
    "\n",
    "    # Ensure income and census_county_medianIncome columns are float\n",
    "    df_results_IRA['income'] = df_results_IRA['income'].astype(float).round(2)\n",
    "    df_results_IRA['census_county_medianIncome'] = df_results_IRA['census_county_medianIncome'].astype(float).round(2)\n",
    "\n",
    "    # Calculate percent_AMI\n",
    "    df_results_IRA['percent_AMI'] = ((df_results_IRA['income'] / df_results_IRA['census_county_medianIncome']) * 100).round(2)\n",
    "\n",
    "    # Categorize the income level based on percent_AMI\n",
    "    conditions_lmi = [\n",
    "        df_results_IRA['percent_AMI'] <= 80.0,\n",
    "        (df_results_IRA['percent_AMI'] > 80.0) & (df_results_IRA['percent_AMI'] <= 150.0)\n",
    "    ]\n",
    "    choices_lmi = ['Low-Income', 'Moderate-Income']\n",
    "\n",
    "    df_results_IRA['lowModerateIncome_designation'] = np.select(\n",
    "        conditions_lmi, choices_lmi, default='Middle-to-Upper-Income'\n",
    "    )\n",
    "\n",
    "    # Output the modified DataFrame\n",
    "    return df_results_IRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "511387cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine household IRA rebate\n",
    "def get_max_rebate_amount(row, category):\n",
    "    \"\"\"\n",
    "    Helper function to determine the maximum rebate amounts based on the category and row data.\n",
    "    \n",
    "    Parameters:\n",
    "    - row (pd.Series): A row from the DataFrame.\n",
    "    - category (str): The end-use category for which the rebate is being calculated.\n",
    "    \n",
    "    Returns:\n",
    "    - max_rebate_amount (float): The maximum rebate amount for the category.\n",
    "    - max_weatherization_rebate_amount (float): The maximum weatherization rebate amount.\n",
    "    \"\"\"\n",
    "    if category == 'heating':                \n",
    "        if 'ASHP' in str(row['upgrade_hvac_heating_efficiency']) or 'MSHP' in str(row['upgrade_hvac_heating_efficiency']):\n",
    "            max_rebate_amount = 8000.00\n",
    "        else:\n",
    "            max_rebate_amount = 0.00\n",
    "\n",
    "    elif category == 'waterHeating':\n",
    "        if 'Electric Heat Pump' in str(row['upgrade_water_heater_efficiency']):\n",
    "            max_rebate_amount = 1750.00\n",
    "        else:\n",
    "            max_rebate_amount = 0.00\n",
    "\n",
    "    elif category == 'clothesDrying':\n",
    "        if 'Electric, Premium, Heat Pump, Ventless' in str(row['upgrade_clothes_dryer']):\n",
    "            max_rebate_amount = 840.00\n",
    "        else:\n",
    "            max_rebate_amount = 0.00\n",
    "\n",
    "    elif category == 'cooking':\n",
    "        if 'Electric, ' in str(row['upgrade_cooking_range']):\n",
    "            max_rebate_amount = 840.00\n",
    "        else:\n",
    "            max_rebate_amount = 0.00\n",
    "\n",
    "    max_weatherization_rebate_amount = 1600.00\n",
    "\n",
    "    return max_rebate_amount, max_weatherization_rebate_amount\n",
    "\n",
    "def calculate_rebate(df_results_IRA, row, category, menu_mp, income_designation, coverage_rate):\n",
    "    \"\"\"\n",
    "    Helper function to calculate and assign the rebate amounts.\n",
    "    \n",
    "    Parameters:\n",
    "    - df_results_IRA (pd.DataFrame): The DataFrame containing the data.\n",
    "    - row (pd.Series): A row from the DataFrame.\n",
    "    - category (str): The end-use category for which the rebate is being calculated.\n",
    "    - menu_mp (int): The menu_mp variable to determine weatherization rebates.\n",
    "    - income_designation (str): The income designation (e.g., 'Low-Income', 'Moderate-Income').\n",
    "    - coverage_rate (float): The coverage rate (1.0 for 100%, 0.5 for 50%).\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    max_rebate_amount, max_weatherization_rebate_amount = get_max_rebate_amount(row, category)\n",
    "    \n",
    "    project_coverage = round((row[f'mp{menu_mp}_{category}_installationCost'] * coverage_rate), 2)\n",
    "    if project_coverage <= max_rebate_amount:\n",
    "        df_results_IRA.at[row.name, f'mp{menu_mp}_{category}_rebate_amount'] = project_coverage\n",
    "    else:\n",
    "        df_results_IRA.at[row.name, f'mp{menu_mp}_{category}_rebate_amount'] = max_rebate_amount\n",
    "\n",
    "    if f'mp{menu_mp}_enclosure_upgradeCost' in df_results_IRA.columns:\n",
    "        weatherization_project_coverage = round((row[f'mp{menu_mp}_enclosure_upgradeCost'] * coverage_rate), 2)\n",
    "        if weatherization_project_coverage <= max_weatherization_rebate_amount:\n",
    "            df_results_IRA.at[row.name, f'weatherization_rebate_amount'] = weatherization_project_coverage\n",
    "        else:\n",
    "            df_results_IRA.at[row.name, f'weatherization_rebate_amount'] = max_weatherization_rebate_amount\n",
    "\n",
    "def calculate_rebateIRA(df_results_IRA, category, menu_mp):\n",
    "    \"\"\"\n",
    "    Calculates rebate amounts for different end-uses based on income designation.\n",
    "    \n",
    "    Parameters:\n",
    "    - df_results_IRA (pd.DataFrame): The DataFrame containing the data.\n",
    "    - category (str): The end-use category for which the rebate is being calculated.\n",
    "    - menu_mp (int): The menu_mp variable to determine weatherization rebates.\n",
    "\n",
    "    Returns:\n",
    "    - df_results_IRA (pd.DataFrame): The DataFrame with the added rebate amount columns.\n",
    "    \"\"\"\n",
    "    for index, row in df_results_IRA.iterrows():\n",
    "        if row['lowModerateIncome_designation'] == 'Low-Income':\n",
    "            calculate_rebate(df_results_IRA, row, category, menu_mp, 'Low-Income', 1.00)\n",
    "        elif row['lowModerateIncome_designation'] == 'Moderate-Income':\n",
    "            calculate_rebate(df_results_IRA, row, category, menu_mp, 'Moderate-Income', 0.50)\n",
    "        else:\n",
    "            df_results_IRA.at[index, f'mp{menu_mp}_{category}_rebate_amount'] = 0.00\n",
    "            if menu_mp == 9 or menu_mp == 10:\n",
    "                df_results_IRA.at[index, f'weatherization_rebate_amount'] = 0.00\n",
    "\n",
    "    return df_results_IRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f74d5bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ira_adoption_consistency(df, category, upgrade_column):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    cols_to_display = ['bldg_id',\n",
    "                       f'base_{category}_fuel',\n",
    "                       f'{upgrade_column}',\n",
    "                       f'baseline_{category}_consumption',\n",
    "                       f'mp{menu_mp}_{category}_consumption',\n",
    "                       f'mp{menu_mp}_{category}_reduction_consumption',\n",
    "                       f'baseline_{category}_fuelCost',\n",
    "                       f'mp{menu_mp}_{category}_fuelCost',        \n",
    "                       f'mp{menu_mp}_{category}_savings_fuelCost',\n",
    "                       f'mp{menu_mp}_{category}_net_capitalCost',\n",
    "                       f'mp{menu_mp}_{category}_private_npv',\n",
    "                       f'baseline_{category}_damages_health',\n",
    "                       f'baseline_{category}_damages_climate',\n",
    "                       f'mp{menu_mp}_{category}_damages_health',\n",
    "                       f'mp{menu_mp}_{category}_damages_climate',\n",
    "                       f'mp{menu_mp}_{category}_reduction_damages_health',\n",
    "                       f'mp{menu_mp}_{category}_reduction_damages_climate',\n",
    "                       f'mp{menu_mp}_{category}_public_npv',\n",
    "                       f'mp{menu_mp}_{category}_retrofit_publicImpact',\n",
    "                       f'mp{menu_mp}_{category}_net_npv',\n",
    "                       f'mp{menu_mp}_{category}_adoption',\n",
    "                       f'ira_mp{menu_mp}_{category}_net_capitalCost',\n",
    "                       f'ira_mp{menu_mp}_{category}_private_npv',\n",
    "                       f'ira_mp{menu_mp}_{category}_net_npv',\n",
    "                       f'ira_mp{menu_mp}_{category}_adoption',\n",
    "                       ]    \n",
    "\n",
    "    # Filter the dataframe to show only the relevant columns\n",
    "    df_filtered = df_copy[cols_to_display]\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ae094b",
   "metadata": {},
   "source": [
    "## Moderate Retrofit (MP9): MP8 + Basic Enclosure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1929a10d",
   "metadata": {},
   "source": [
    "## Advanced Retrofit (MP10): MP8 + Enhanced Enclosure\n",
    "**Notes**\n",
    "- There are some inconsistencies for variable names and syntax for calculations\n",
    "- The calculations should still end up the same regardless because of order of operations\n",
    "- Plan to update for consistency to avoid user confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f82f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Helper function to get conditions and tech-efficiency pairs for enclosure retrofit\n",
    "def get_enclosure_parameters(df, retrofit_col):\n",
    "    if retrofit_col == 'insulation_atticFloor_upgradeCost':\n",
    "        conditions = [\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-30') & (df['base_insulation_atticFloor'] == 'R-13'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-30') & (df['base_insulation_atticFloor'] == 'R-7'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-30') & (df['base_insulation_atticFloor'] == 'Uninsulated'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-49') & (df['base_insulation_atticFloor'] == 'R-30'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-49') & (df['base_insulation_atticFloor'] == 'R-19'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-49') & (df['base_insulation_atticFloor'] == 'R-13'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-49') & (df['base_insulation_atticFloor'] == 'R-7'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-49') & (df['base_insulation_atticFloor'] == 'Uninsulated'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-60') & (df['base_insulation_atticFloor'] == 'R-38'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-60') & (df['base_insulation_atticFloor'] == 'R-30'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-60') & (df['base_insulation_atticFloor'] == 'R-19'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-60') & (df['base_insulation_atticFloor'] == 'R-13'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-60') & (df['base_insulation_atticFloor'] == 'R-7'),\n",
    "            (df['upgrade_insulation_atticFloor'] == 'R-60') & (df['base_insulation_atticFloor'] == 'Uninsulated')\n",
    "        ]\n",
    "        tech_eff_pairs = [\n",
    "            ('Attic Floor Insulation: R-30', 'R-13'),\n",
    "            ('Attic Floor Insulation: R-30', 'R-7'),\n",
    "            ('Attic Floor Insulation: R-30', 'Uninsulated'),\n",
    "            ('Attic Floor Insulation: R-49', 'R-30'),\n",
    "            ('Attic Floor Insulation: R-49', 'R-19'),\n",
    "            ('Attic Floor Insulation: R-49', 'R-13'),\n",
    "            ('Attic Floor Insulation: R-49', 'R-7'),\n",
    "            ('Attic Floor Insulation: R-49', 'Uninsulated'),\n",
    "            ('Attic Floor Insulation: R-60', 'R-38'),\n",
    "            ('Attic Floor Insulation: R-60', 'R-30'),\n",
    "            ('Attic Floor Insulation: R-60', 'R-19'),\n",
    "            ('Attic Floor Insulation: R-60', 'R-13'),\n",
    "            ('Attic Floor Insulation: R-60', 'R-7'),\n",
    "            ('Attic Floor Insulation: R-60', 'Uninsulated')\n",
    "        ]\n",
    "    elif retrofit_col == 'infiltration_reduction_upgradeCost':\n",
    "        conditions = [\n",
    "            (df['upgrade_infiltration_reduction'] == '30%')\n",
    "        ]\n",
    "        tech_eff_pairs = [\n",
    "            ('Air Leakage Reduction: 30% Reduction', 'Varies')\n",
    "        ]\n",
    "    elif retrofit_col == 'duct_sealing_upgradeCost':\n",
    "        conditions = [\n",
    "            (df['upgrade_duct_sealing'] == '10% Leakage, R-8') & (df['base_ducts'].str.contains('10% Leakage')),\n",
    "            (df['upgrade_duct_sealing'] == '10% Leakage, R-8') & (df['base_ducts'].str.contains('20% Leakage')),\n",
    "            (df['upgrade_duct_sealing'] == '10% Leakage, R-8') & (df['base_ducts'].str.contains('30% Leakage')),\n",
    "        ]\n",
    "        tech_eff_pairs = [\n",
    "            ('Duct Sealing: 10% Leakage, R-8', '10% Leakage'),\n",
    "            ('Duct Sealing: 10% Leakage, R-8', '20% Leakage'),\n",
    "            ('Duct Sealing: 10% Leakage, R-8', '30% Leakage'),\n",
    "        ]\n",
    "    elif retrofit_col == 'insulation_wall_upgradeCost':\n",
    "        conditions = [\n",
    "            (df['upgrade_insulation_wall'] == 'Wood Stud, R-13')\n",
    "        ]\n",
    "        tech_eff_pairs = [\n",
    "            ('Drill-and-fill Wall Insulation: Wood Stud, R-13', 'Wood Stud, Uninsulated')\n",
    "        ]\n",
    "    elif retrofit_col == 'insulation_foundation_wall_upgradeCost':\n",
    "        conditions = [\n",
    "            (df['upgrade_insulation_foundation_wall'] == 'Wall R-10, Interior')\n",
    "        ]\n",
    "        tech_eff_pairs = [\n",
    "            ('Foundation Wall Insulation: Wall R-10, Interior', 'Uninsulated')\n",
    "        ]\n",
    "    elif retrofit_col == 'insulation_rim_joist_upgradeCost':\n",
    "        conditions = [\n",
    "            (df['base_insulation_foundation_wall'] == 'Uninsulated') & (df['base_foundation_type'].isin(['Unvented Crawlspace', 'Vented Crawlspace', 'Heated Basement']))\n",
    "        ]\n",
    "        tech_eff_pairs = [\n",
    "            ('Rim Joist Insulation: Wall R-10, Exterior', 'Uninsulated')\n",
    "        ]\n",
    "    elif retrofit_col == 'seal_crawlspace_upgradeCost':\n",
    "        conditions = [\n",
    "            (df['upgrade_seal_crawlspace'] == 'Unvented Crawlspace')\n",
    "        ]\n",
    "        tech_eff_pairs = [\n",
    "            ('Seal Vented Crawlspace: Unvented Crawlspace', 'Vented Crawlspace')\n",
    "        ]\n",
    "    elif retrofit_col == 'insulation_roof_upgradeCost':\n",
    "        conditions = [\n",
    "            (df['upgrade_insulation_roof'] == 'Finished, R-30')\n",
    "        ]\n",
    "        tech_eff_pairs = [\n",
    "            ('Insulate Finished Attics and Cathedral Ceilings: Finished, R-30', 'R-30')\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid retrofit_col specified: {retrofit_col}\")\n",
    "    \n",
    "    return {'conditions': conditions, 'tech_eff_pairs': tech_eff_pairs}\n",
    "\n",
    "def calculate_enclosure_retrofit_upgradeCosts(df, cost_dict, retrofit_col, params_col, rsMeans_national_avg):\n",
    "    \"\"\"\n",
    "    Calculate the enclosure retrofit upgrade costs based on given parameters and conditions.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing data for different scenarios.\n",
    "    cost_dict (dict): Dictionary with cost information for different technology and efficiency combinations.\n",
    "    retrofit_col (str): Column name for the retrofit cost.\n",
    "    params_col (str): Column name for the parameter to use in the cost calculation.\n",
    "    rsMeans_national_avg (float): National average value for cost adjustment.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Updated DataFrame with calculated retrofit costs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get conditions and tech-efficiency pairs for the specified retrofit\n",
    "    params = get_enclosure_parameters(df, retrofit_col)\n",
    "    conditions = params['conditions']\n",
    "    tech_eff_pairs = params['tech_eff_pairs']\n",
    "\n",
    "    # Debug: Print the extracted parameters\n",
    "    print(\"Extracted Parameters:\", params)\n",
    "\n",
    "    # Map each condition to its tech and efficiency\n",
    "    tech = np.select(conditions, [pair[0] for pair in tech_eff_pairs], default='unknown')\n",
    "    eff = np.select(conditions, [pair[1] for pair in tech_eff_pairs], default='unknown')\n",
    "\n",
    "    # Debug: Print the mapped tech and efficiency pairs\n",
    "    print(\"Mapped Tech:\", tech)\n",
    "    print(\"Mapped Efficiency:\", eff)\n",
    "\n",
    "    # Filter out rows with unknown technology and efficiency\n",
    "    valid_indices = tech != 'unknown'\n",
    "    tech = tech[valid_indices]\n",
    "    eff = eff[valid_indices]\n",
    "    df_valid = df.loc[valid_indices].copy()\n",
    "\n",
    "    # Debug: Print the valid indices and corresponding tech-efficiency pairs\n",
    "    print(\"Valid Indices:\", valid_indices)\n",
    "    print(\"Valid Tech:\", tech)\n",
    "    print(\"Valid Efficiency:\", eff)\n",
    "\n",
    "    # Initialize dictionary to store sampled costs\n",
    "    sampled_costs_dict = {}\n",
    "\n",
    "    # Calculate costs for each component (normalized_cost)\n",
    "    for cost_component in ['normalized_cost']:\n",
    "        progressive_costs = np.array([cost_dict.get((t, e), {}).get(f'{cost_component}_progressive', np.nan) for t, e in zip(tech, eff)])\n",
    "        reference_costs = np.array([cost_dict.get((t, e), {}).get(f'{cost_component}_reference', np.nan) for t, e in zip(tech, eff)])\n",
    "        conservative_costs = np.array([cost_dict.get((t, e), {}).get(f'{cost_component}_conservative', np.nan) for t, e in zip(tech, eff)])\n",
    "\n",
    "        # Handle missing cost data\n",
    "        if np.isnan(progressive_costs).any() or np.isnan(reference_costs).any() or np.isnan(conservative_costs).any():\n",
    "            missing_indices = np.where(np.isnan(progressive_costs) | np.isnan(reference_costs) | np.isnan(conservative_costs))\n",
    "            print(f\"Missing data at indices: {missing_indices}\")\n",
    "            print(f\"Tech with missing data: {tech[missing_indices]}\")\n",
    "            print(f\"Efficiencies with missing data: {eff[missing_indices]}\")\n",
    "            \n",
    "            raise ValueError(f\"Missing cost data for some technology and efficiency combinations in cost_component {cost_component}\")\n",
    "\n",
    "        # Calculate mean and standard deviation assuming the costs represent the 10th, 50th, and 90th percentiles of a normal distribution\n",
    "        mean_costs = reference_costs\n",
    "        std_costs = (conservative_costs - progressive_costs) / (norm.ppf(0.90) - norm.ppf(0.10))\n",
    "\n",
    "        # Sample from the normal distribution for each row\n",
    "        sampled_costs = np.random.normal(loc=mean_costs, scale=std_costs)\n",
    "        sampled_costs_dict[cost_component] = sampled_costs\n",
    "\n",
    "    # Calculate the retrofit cost for each row\n",
    "    retrofit_cost = (\n",
    "        sampled_costs_dict['normalized_cost'] * df_valid[params_col]\n",
    "    ) * (df_valid['rsMeans_CCI_avg'] / rsMeans_national_avg)\n",
    "\n",
    "    # Add the calculated costs to the DataFrame, rounded to 2 decimal places\n",
    "    df_valid.loc[:, retrofit_col] = np.round(retrofit_cost, 2)\n",
    "\n",
    "    # Reintegrate the valid rows back into the original DataFrame\n",
    "    df.loc[valid_indices, retrofit_col] = df_valid[retrofit_col]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# cost_dict = load_cost_dict()  # Define your cost dictionary\n",
    "# rsMeans_national_avg = 100  # Example value\n",
    "# retrofit_col = 'insulation_atticFloor_upgradeCost'  # Example retrofit column\n",
    "# params_col = 'attic_area'  # Example parameter column\n",
    "# df = calculate_enclosure_retrofit_upgradeCosts(df, cost_dict, retrofit_col, params_col, rsMeans_national_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75b9aba",
   "metadata": {},
   "source": [
    "# Storing Output Results and Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9df5eb",
   "metadata": {},
   "source": [
    "## Save Results: Merge DFs and Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "454a5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df_merge(df_compare, df_results_IRA, df_results_IRA_gridDecarb):\n",
    "    # Identify common columns (excluding 'bldg_id' which is the merging key)\n",
    "    common_columns_IRA = set(df_compare.columns) & set(df_results_IRA.columns)\n",
    "    common_columns_IRA.discard('bldg_id')\n",
    "        \n",
    "    # Drop duplicate columns in df_results_IRA and merge\n",
    "    df_results_IRA = df_results_IRA.drop(columns=common_columns_IRA)\n",
    "    print(f\"\"\"Dropped the following duplicate columns before merge: \n",
    "    {common_columns_IRA}\n",
    "    \"\"\")\n",
    "    merged_df = pd.merge(df_compare, df_results_IRA, on='bldg_id', how='inner')\n",
    "\n",
    "    # Repeat the steps above for the merged_df and df_results_IRA_gridDecarb\n",
    "    common_columnsIRA_gridDecarb = set(merged_df.columns) & set(df_results_IRA_gridDecarb.columns)\n",
    "    common_columnsIRA_gridDecarb.discard('bldg_id')\n",
    "    df_results_IRA_gridDecarb = df_results_IRA_gridDecarb.drop(columns=common_columnsIRA_gridDecarb)\n",
    "    print(f\"\"\"Dropped the following duplicate columns before merge: \n",
    "    {common_columnsIRA_gridDecarb}\n",
    "    \"\"\")\n",
    "        \n",
    "    # Create cleaned, merged results df with no duplicate columns\n",
    "    df_results_export = pd.merge(merged_df, df_results_IRA_gridDecarb, on='bldg_id', how='inner')\n",
    "    print(\"Dataframes have been cleaned of duplicate columns and merged successfully. Ready to export!\")\n",
    "    return df_results_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12fc34f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model_run_output(df_results_export):\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    # Baseline model run results\n",
    "    if menu_mp == '0' or menu_mp==0:\n",
    "        results_filename = f\"baseline_wholeHome_results_{location_id}_{results_export_formatted_date}.csv\"\n",
    "        print(f\"BASELINE RESULTS:\")\n",
    "        print(f\"Dataframe results will be saved in this csv file: {results_filename}\")\n",
    "\n",
    "        # Change the directory to the upload folder and export the file\n",
    "        results_change_directory = \"baseline\"\n",
    "\n",
    "    # Measure Package model run results\n",
    "    else:\n",
    "        if menu_mp == '8' or menu_mp==8:\n",
    "            print(f\"MEASURE PACKAGE {menu_mp} (MP{menu_mp}) RESULTS:\")\n",
    "            results_filename = f\"mp{menu_mp}_scenarios_results_wholeHome_{location_id}_{results_export_formatted_date}.csv\"\n",
    "            print(f\"Dataframe results will be saved in this csv file: {results_filename}\")\n",
    "\n",
    "            # Change the directory to the upload folder and export the file\n",
    "            results_change_directory = \"retrofit_basic\"\n",
    "\n",
    "        elif menu_mp == '9' or menu_mp==9:\n",
    "            results_filename = f\"mp{menu_mp}_scenarios_results_wholeHome_{location_id}_{results_export_formatted_date}.csv\"\n",
    "            print(f\"MEASURE PACKAGE {menu_mp} (MP{menu_mp}) RESULTS:\")\n",
    "            print(f\"Dataframe results will be saved in this csv file: {results_filename}\")\n",
    "\n",
    "            # Change the directory to the upload folder and export the file\n",
    "            results_change_directory = \"retrofit_moderate\"\n",
    "\n",
    "        elif menu_mp == '10' or menu_mp==10:\n",
    "            results_filename = f\"mp{menu_mp}_scenarios_results_wholeHome_{location_id}_{results_export_formatted_date}.csv\"\n",
    "            print(f\"MEASURE PACKAGE {menu_mp} (MP{menu_mp}) RESULTS:\")\n",
    "            print(f\"Dataframe results will be saved in this csv file: {results_filename}\")\n",
    "\n",
    "            # Change the directory to the upload folder and export the file\n",
    "            results_change_directory = \"retrofit_advanced\"\n",
    "\n",
    "        else:\n",
    "            print(\"No matching scenarios for this Measure Package (MP)\")\n",
    "\n",
    "    # Export dataframe results as a csv to the specified filepath\n",
    "    results_export_filepath = os.path.join(output_folder_path, results_change_directory, results_filename)\n",
    "    df_results_export.to_csv(results_export_filepath)\n",
    "    print(f\"Dataframe for MP{menu_mp} WHOLE-HOME results were exported here: {results_export_filepath}\")\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b40e7",
   "metadata": {},
   "source": [
    "## Convert Results Output CSVs to Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6ca945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scenario_data(end_use, output_folder_path, scenario_string, model_run_date_time, columns_to_string):\n",
    "    # Construct the output folder path with the scenario of interest\n",
    "    scenario_folder_path = os.path.join(output_folder_path, scenario_string)\n",
    "    print(f\"Output Results Folder Path: {scenario_folder_path}\")\n",
    "\n",
    "    # List all files in the specified folder with the specified date in the filename\n",
    "    files = [f for f in os.listdir(scenario_folder_path) if os.path.isfile(os.path.join(scenario_folder_path, f)) and model_run_date_time in f]\n",
    "\n",
    "    # Initialize dataframe as None\n",
    "    df_outputs = None\n",
    "\n",
    "    # Assume there is one main file per scenario that includes all necessary data\n",
    "    if files:\n",
    "        file_path = os.path.join(scenario_folder_path, files[0])  # Assumes the first file is the correct one\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            df_outputs = pd.read_csv(file_path, index_col=0, dtype=columns_to_string)\n",
    "            print(f\"Loaded {end_use} data for Scenario '{scenario_string}'\", \"\\n\")\n",
    "        else:\n",
    "            print(\"File not found for the specified scenario\", \"\\n\")\n",
    "\n",
    "    if df_outputs is None:\n",
    "        print(f\"No {end_use} data found for Scenario '{scenario_string}'\")\n",
    "\n",
    "    return df_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6b00a",
   "metadata": {},
   "source": [
    "## Visuals for Public and Private Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "629494a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added base fuel color-coded legend\n",
    "# Possibly update colors to make more color blind accessible\n",
    "color_map_fuel = {\n",
    "    'Electricity': 'seagreen',\n",
    "    'Natural Gas': 'steelblue',\n",
    "    'Propane': 'orange',\n",
    "    'Fuel Oil': 'firebrick',\n",
    "}\n",
    "\n",
    "# Define a function to plot the histogram and percentile subplot\n",
    "def create_subplot_histogram(ax, df, x_col, bin_number, x_label=None, y_label=None, lower_percentile=2.5, upper_percentile=97.5, color_code='base_fuel', statistic='count', include_zero=False, show_legend=False):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    if not include_zero:\n",
    "        df_copy[x_col] = df_copy[x_col].replace(0, np.nan)\n",
    "\n",
    "    lower_limit = df_copy[x_col].quantile(lower_percentile / 100)\n",
    "    upper_limit = df_copy[x_col].quantile(upper_percentile / 100)\n",
    "\n",
    "    valid_data = df_copy[x_col][(df_copy[x_col] >= lower_limit) & (df_copy[x_col] <= upper_limit)]\n",
    "\n",
    "    # Get the corresponding color for each fuel category\n",
    "    colors = [color_map_fuel.get(fuel, 'gray') for fuel in df_copy[color_code].unique()]\n",
    "\n",
    "    # Set the hue_order to match the unique fuel categories and their corresponding colors\n",
    "    hue_order = [fuel for fuel in df_copy[color_code].unique() if fuel in color_map_fuel]\n",
    "\n",
    "    ax = sns.histplot(data=df_copy, x=valid_data, kde=False, bins=bin_number, hue=color_code, hue_order=hue_order, stat=statistic, multiple=\"stack\", palette=colors, ax=ax, legend=show_legend)\n",
    "\n",
    "    if x_label is not None:\n",
    "        ax.set_xlabel(x_label, fontsize=22)  # Set font size for x-axis label\n",
    "\n",
    "    if y_label is not None:\n",
    "        ax.set_ylabel(y_label, fontsize=22)  # Set font size for y-axis label\n",
    "\n",
    "    ax.set_xlim(left=lower_limit, right=upper_limit)\n",
    "\n",
    "    # Set font size for tick labels\n",
    "    ax.tick_params(axis='both', labelsize=22)\n",
    "\n",
    "    sns.despine()\n",
    "\n",
    "def create_subplot_grid_histogram(df, subplot_positions, x_cols, x_labels, y_label=None, bin_number=20, lower_percentile=2.5, upper_percentile=97.5, statistic='count', color_code='base_fuel', include_zero=False, suptitle=None, sharex=False, sharey=False, column_titles=None, show_legend=True, figure_size=(12, 10), export_filename=None, export_format='png', dpi=300):\n",
    "    num_subplots = len(subplot_positions)\n",
    "    num_cols = max(pos[1] for pos in subplot_positions) + 1\n",
    "    num_rows = max(pos[0] for pos in subplot_positions) + 1\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=figure_size, sharex=sharex, sharey=sharey)\n",
    "\n",
    "    # Create a dictionary to map subplot positions to their respective axes\n",
    "    subplot_axes = {(pos[0], pos[1]): axes[pos[0], pos[1]] for pos in subplot_positions}\n",
    "\n",
    "    # Define the parameters for each histogram subplot\n",
    "    plot_params = [{'ax': subplot_axes[pos], 'x_col': col, 'x_label': label, 'y_label': y_label, 'bin_number': bin_number, 'lower_percentile': lower_percentile, 'upper_percentile': upper_percentile, 'statistic': statistic, 'color_code': color_code, 'include_zero': include_zero, 'show_legend': show_legend}\n",
    "                   for pos, col, label in zip(subplot_positions, x_cols, x_labels)]\n",
    "\n",
    "    # Plot each histogram subplot using the defined parameters\n",
    "    for params in plot_params:\n",
    "        create_subplot_histogram(df=df, **params)\n",
    "\n",
    "    # Add a super title to the entire figure if suptitle is provided\n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle, fontweight='bold')\n",
    "\n",
    "    # Add titles over the columns\n",
    "    if column_titles:\n",
    "        for col_index, title in enumerate(column_titles):\n",
    "            axes[0, col_index].set_title(title, fontsize=22, fontweight='bold')\n",
    "    \n",
    "    # If sharey is True, remove y-axis labels on all subplots except the leftmost ones in each row\n",
    "    if sharey:\n",
    "        for row_index in range(num_rows):\n",
    "            for col_index in range(num_cols):\n",
    "                if col_index > 0:\n",
    "                    axes[row_index, col_index].set_yticklabels([])\n",
    "\n",
    "    # Add a legend for the color mapping at the bottom of the entire figure\n",
    "    legend_labels = list(color_map_fuel.keys())\n",
    "    legend_handles = [plt.Rectangle((0, 0), 1, 1, color=color_map_fuel[label]) for label in legend_labels]\n",
    "    fig.legend(legend_handles, legend_labels, loc='lower center', ncol=len(legend_labels), prop={'size': 22}, labelspacing=0.5, bbox_to_anchor=(0.5, -0.05))             \n",
    "    \n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Export the figure if export_filename is provided\n",
    "    if export_filename:\n",
    "        save_figure_path = os.path.join(save_figure_directory, export_filename)\n",
    "        plt.savefig(save_figure_path, format=export_format, dpi=dpi)\n",
    "    # Otherwise show the plot in Jupyter Notebook\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a177b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added column titles parameter\n",
    "color_map_fuel = {\n",
    "    'Electricity': 'seagreen',\n",
    "    'Natural Gas': 'steelblue',\n",
    "    'Propane': 'orange',\n",
    "    'Fuel Oil': 'firebrick',\n",
    "}\n",
    "\n",
    "# Define a function to plot the boxplots\n",
    "def create_subplot_boxplot(ax, df, y_col, x_col, x_label=None, y_label=None, lower_percentile=1, upper_percentile=99, show_outliers=True, include_zero=True):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    if not include_zero:\n",
    "        df_copy[x_col] = df_copy[x_col].replace(0, np.nan)\n",
    "\n",
    "    # Get the corresponding color for each fuel category\n",
    "    colors = [color_map_fuel.get(fuel, 'gray') for fuel in df_copy[y_col].unique()]\n",
    "\n",
    "    # Set the order to match the unique fuel categories and their corresponding colors\n",
    "    order = [fuel for fuel in df_copy[y_col].unique() if fuel in color_map_fuel]\n",
    "\n",
    "    ax = sns.boxplot(data=df_copy, x=x_col, y=y_col, order=order, palette=colors, showfliers=show_outliers, ax=ax)\n",
    "\n",
    "    if x_label is not None:\n",
    "        ax.set_xlabel(x_label, fontsize=22)  # Set font size for x-axis label\n",
    "\n",
    "    if y_label is not None:\n",
    "        ax.set_ylabel(y_label, fontsize=22)  # Set font size for y-axis label\n",
    "\n",
    "    # Calculate number of obs per group & median to position labels\n",
    "    medians = df_copy.groupby([y_col])[x_col].median().values\n",
    "    num_obs = df_copy[y_col].value_counts().values\n",
    "    num_obs = [str(x) for x in num_obs.tolist()]\n",
    "    num_obs = [\"n = \" + i for i in num_obs]\n",
    "\n",
    "    # Set custom y-axis labels using num_obs without changing y-axis positions\n",
    "    ax.set_yticklabels([f\"\"\"{label.get_text()} \n",
    "    ({num})\"\"\" for label, num in zip(ax.get_yticklabels(), num_obs)])\n",
    "\n",
    "    # Set font size for tick labels\n",
    "    ax.tick_params(axis='both', labelsize=22)\n",
    "    \n",
    "    sns.despine()\n",
    "\n",
    "def create_subplot_grid_boxplot(df, subplot_positions, x_cols, x_labels, suptitle=None, y_label=None, show_outliers=False, include_zero=True, column_titles=None, figure_size=(12, 10), sharex=False, sharey=False, export_filename=None, export_format='png', dpi=300):\n",
    "    num_subplots = len(subplot_positions)\n",
    "    num_cols = max(pos[1] for pos in subplot_positions) + 1\n",
    "    num_rows = max(pos[0] for pos in subplot_positions) + 1\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=figure_size, sharex=sharex, sharey=sharey)\n",
    "\n",
    "    # Create a dictionary to map subplot positions to their respective axes\n",
    "    subplot_axes = {(pos[0], pos[1]): axes[pos[0], pos[1]] for pos in subplot_positions}\n",
    "\n",
    "    # Define the parameters for each boxplot subplot\n",
    "    plot_params = [{'ax': subplot_axes[pos], 'y_col': 'base_fuel', 'x_col': col, 'x_label': label, 'y_label': y_label, 'show_outliers': show_outliers, 'include_zero': include_zero}\n",
    "                   for pos, col, label in zip(subplot_positions, x_cols, x_labels)]\n",
    "\n",
    "    # Plot each boxplot subplot using the defined parameters\n",
    "    for params in plot_params:\n",
    "        create_subplot_boxplot(df=df, **params)\n",
    "\n",
    "    # Add titles over the columns\n",
    "    if column_titles:\n",
    "        for col_index, title in enumerate(column_titles):\n",
    "            axes[0, col_index].set_title(title, fontsize=22, fontweight='bold')\n",
    "\n",
    "    # Add a super title to the entire figure if suptitle is provided\n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle, fontweight='bold')\n",
    "        \n",
    "    if sharey:\n",
    "        for row_index in range(num_rows):\n",
    "            for col_index in range(num_cols):\n",
    "                if col_index > 0:\n",
    "                    axes[row_index, col_index].set_yticklabels([])\n",
    "                    axes[row_index, col_index].set_ylabel('')\n",
    "                else:\n",
    "                    # Add the y-axis label in the leftmost column\n",
    "                    axes[row_index, col_index].set_ylabel(y_label)\n",
    "\n",
    "                    # Add the fuel type labels based on unique values in 'base_fuel' column\n",
    "                    if row_index == 0:\n",
    "                        fuel_types = df['base_fuel'].unique()\n",
    "                        axes[row_index, col_index].set_yticks(range(len(fuel_types)))\n",
    "                        axes[row_index, col_index].set_yticklabels(fuel_types)\n",
    "\n",
    "    # Add a legend for the color mapping at the bottom of the entire figure\n",
    "    legend_labels = list(color_map_fuel.keys())\n",
    "    legend_handles = [plt.Rectangle((0, 0), 1, 1, color=color_map_fuel[label]) for label in legend_labels]\n",
    "    fig.legend(legend_handles, legend_labels, loc='lower center', ncol=len(legend_labels), prop={'size': 22}, labelspacing=0.5, bbox_to_anchor=(0.5, -0.05))             \n",
    "        \n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Export the figure if export_filename is provided\n",
    "    if export_filename:\n",
    "        save_figure_path = os.path.join(save_figure_directory, export_filename)\n",
    "        plt.savefig(save_figure_path, format=export_format, dpi=dpi)\n",
    "    # Otherwise show the plot in Jupyter Notebook\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7991dd9f",
   "metadata": {},
   "source": [
    "# Adoption Rate Scenario Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8511f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mapping = {\n",
    "    'Existing Equipment': 'gray',\n",
    "    'Adoption': 'steelblue',\n",
    "    'Potential Adoption with Subsidy': 'lightblue', \n",
    "    'Averse to Adoption': 'lightsalmon',\n",
    "}\n",
    "\n",
    "def create_subplot_adoption(df, main_data_column, groups, groupby1, groupby2=None, x_label=None, y_label=None, plot_title=None, ax=None, desired_order=None, display_obs=None):\n",
    "    \"\"\"\n",
    "    Creates a subplot showing the adoption rates across different groups using stacked bar charts.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The pandas DataFrame containing the data.\n",
    "    - main_data_column (str): The name of the column in df representing the main data to plot.\n",
    "    - groups (int or str): Determines whether to group the data by one or two dimensions.\n",
    "    - groupby1 (str): The name of the first grouping column.\n",
    "    - groupby2 (str, optional): The name of the second grouping column, if applicable.\n",
    "    - x_label (str, optional): Label for the x-axis.\n",
    "    - y_label (str, optional): Label for the y-axis.\n",
    "    - plot_title (str, optional): Title for the plot.\n",
    "    - ax (matplotlib.axes.Axes, optional): The matplotlib Axes object to plot on.\n",
    "    - desired_order (list, optional): The desired order of categories to display.\n",
    "    - display_obs (str, optional): Determines whether to display counts or percentages of observations.\n",
    "    \n",
    "    Returns:\n",
    "    - ax (matplotlib.axes.Axes): The Axes object with the plot.\n",
    "    \"\"\"  \n",
    "#     # Filter out the 'Existing Equipment' category from the dataframe\n",
    "#     df = df[df[main_data_column] != 'Existing Equipment']\n",
    "    \n",
    "    if groups == 1 or groups == '1':\n",
    "        # Calculate the percentages for each combination of categories\n",
    "        counts = df.groupby(f'{groupby1}')[f'{main_data_column}'].value_counts(normalize=True).unstack()\n",
    "\n",
    "        # Calculate the total count of each groupby1 category\n",
    "        total_counts = df[f'{groupby1}'].value_counts()\n",
    "\n",
    "        # Manually define the desired order of categories\n",
    "        desired_order = []\n",
    "\n",
    "        # Check if different adoption decisions are present in counts, and add them if present\n",
    "        if 'Existing Equipment' in counts.columns:\n",
    "            desired_order.append('Existing Equipment') \n",
    "        if 'Adoption' in counts.columns:\n",
    "            desired_order.append('Adoption')\n",
    "        if 'Potential Adoption with Subsidy' in counts.columns:\n",
    "            desired_order.append('Potential Adoption with Subsidy')\n",
    "        if 'Averse to Adoption' in counts.columns:\n",
    "            desired_order.append('Averse to Adoption')\n",
    "            \n",
    "        # Create a stacked bar chart\n",
    "        if ax is not None:\n",
    "            counts = counts[desired_order]  # Reorder the columns\n",
    "            ax = counts.plot(kind='barh', stacked=True, color=[color_mapping.get(label, 'gray') for label in counts.columns], ax=ax, width=0.8)\n",
    "\n",
    "        else:\n",
    "            counts = counts[desired_order]  # Reorder the columns\n",
    "            ax = counts.plot(kind='barh', stacked=True, color=[color_mapping.get(label, 'gray') for label in counts.columns], width=0.8)\n",
    "\n",
    "#         # After plotting, remove the 'Existing Equipment' from the desired_order if present\n",
    "#         desired_order = [category for category in desired_order if category != 'Existing Equipment']\n",
    "\n",
    "        # Display the number of observations next to each bar\n",
    "        if display_obs is not None:\n",
    "            # Add total count of observations next to bar\n",
    "            if display_obs == 'count':\n",
    "                # Add the total count as text on top of each groupby1 bar\n",
    "                for i, group1 in enumerate(counts.index):\n",
    "                    total_count = total_counts[group1]\n",
    "                    ax.text(1.1, i, total_count, va='center', ha='left')\n",
    "            \n",
    "            # Add percentage of housing stock represented\n",
    "            elif display_obs == 'percentage':\n",
    "                # Add the percentage of observation as text on top of each bar\n",
    "                for i, group1 in enumerate(counts.index):\n",
    "                    total_count = total_counts[group1]\n",
    "                    total_observations = total_counts.sum()\n",
    "                    percentage = total_count / total_observations * 100\n",
    "                    ha = 'left' if total_count < 0.5 else 'right'  # Adjust the horizontal alignment based on the count\n",
    "                    ax.text(1.12, i, f'{percentage:.2f}%', va='center', ha=ha, fontweight='bold')\n",
    "                    \n",
    "    if groups == 2 or groups == '2':\n",
    "        if groupby2 is None:\n",
    "            groupby2 = 'federal_poverty_level'\n",
    "        if x_label is None:\n",
    "            x_label = f'{main_data_column}'\n",
    "        if y_label is None:\n",
    "            y_label = groupby1\n",
    "        if plot_title is None:\n",
    "            plot_title = f'Stacked Bar Chart: {groupby1}'\n",
    "\n",
    "        # Calculate the percentages for each combination of categories\n",
    "        counts = df.groupby([groupby1, groupby2])[f'{main_data_column}'].value_counts(normalize=True).unstack()\n",
    "\n",
    "        # Calculate the total count of each combination of groupby1 and groupby2\n",
    "        total_counts = df.groupby([groupby1, groupby2]).size()\n",
    "\n",
    "        # Manually define the desired order of categories\n",
    "        desired_order = []\n",
    "\n",
    "        # Check if different adoption decisions are present in counts, and add them if present\n",
    "        if 'Existing Equipment' in counts.columns:\n",
    "            desired_order.append('Existing Equipment')  \n",
    "        if 'Adoption' in counts.columns:\n",
    "            desired_order.append('Adoption')\n",
    "        if 'Potential Adoption with Subsidy' in counts.columns:\n",
    "            desired_order.append('Potential Adoption with Subsidy')\n",
    "        if 'Averse to Adoption' in counts.columns:\n",
    "            desired_order.append('Averse to Adoption')\n",
    "\n",
    "        # Create a stacked bar chart\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        counts = counts[desired_order]  # Reorder the columns\n",
    "        ax = counts.plot(kind='barh', stacked=True, color=[color_mapping.get(label, 'gray') for label in counts.columns], ax=ax, width=0.8)\n",
    "\n",
    "#         # After plotting, remove the 'Existing Equipment' from the desired_order if present\n",
    "#         desired_order = [category for category in desired_order if category != 'Existing Equipment']\n",
    "        \n",
    "        # Display the number of observations next to each bar\n",
    "        if display_obs is not None:\n",
    "            # Add total count of observations next to bar\n",
    "            if display_obs == 'count':\n",
    "                # Add the total count as text on top of each bar\n",
    "                for i, (group1, group2) in enumerate(counts.index):\n",
    "                    total_count = total_counts[(group1, group2)]\n",
    "                    ha = 'left' if total_count < 0.5 else 'right'  # Adjust the horizontal alignment based on the count\n",
    "                    ax.text(1.1, i, total_count, va='center', ha=ha, fontweight='bold')\n",
    "\n",
    "            # Add percentage of housing stock represented\n",
    "            elif display_obs == 'percentage':\n",
    "                # Add the percentage of observation as text on top of each bar\n",
    "                for i, (group1, group2) in enumerate(counts.index):\n",
    "                    total_count = total_counts[(group1, group2)]\n",
    "                    total_observations = total_counts.sum()\n",
    "                    percentage = total_count / total_observations * 100\n",
    "                    ha = 'left' if total_count < 0.5 else 'right'  # Adjust the horizontal alignment based on the count\n",
    "                    ax.text(1.12, i, f'{percentage:.2f}%', va='center', ha=ha, fontweight='bold')\n",
    "\n",
    "    # Set the labels and title\n",
    "    ax.set_xlabel(f'{x_label}', fontsize=32)  # Set font size for x-axis label\n",
    "    ax.set_ylabel(f'{y_label}', fontsize=32)  # Set font size for y-axis label\n",
    "    ax.set_title(f'{plot_title}', fontweight='bold', fontsize=32)  # Set font size and bold for title\n",
    "#     ax.set_title(f'{plot_title}', fontweight='bold', fontsize=40)  # Set font size and bold for title\n",
    "    \n",
    "    # Format the x-axis labels as percentages\n",
    "    ax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0, symbol='%'))\n",
    "       \n",
    "    # Set background to white\n",
    "    ax.set_facecolor('white')      \n",
    "\n",
    "    # Add gridlines to the plot \n",
    "    ax.set_axisbelow(False)\n",
    "    ax.grid(which='both', axis='x', linestyle='--', color='black')\n",
    "    ax.grid(which='both', axis='y', linestyle='', color='none')\n",
    "    \n",
    "    # Remove the legend\n",
    "    ax.get_legend().remove()   \n",
    "    \n",
    "    # Set font size for tick labels on the x-axis\n",
    "    ax.tick_params(axis='x', labelsize=32)\n",
    "\n",
    "#     # Set font size for tick labels on the y-axis\n",
    "#     ax.tick_params(axis='y', labelsize=28)\n",
    "\n",
    "    # Remove y-ticks\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # Set x-axis limits to include bounds at the figure edge\n",
    "    ax.set_xlim(0, 1)  # Assuming percentages, adjust if necessary\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1fad73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subplot_grid_adoption(dataframes, subplot_positions, x_cols, groups=2, groupby1='base_fuel', groupby2='default_groupby2', x_labels=None, plot_titles=None, y_labels=None, suptitle=None, figure_size=(12, 10), sharex=False, sharey=False, rotate_landscape=False, filter_fuel=None, display_obs=None, export_filename=None, export_format='png', dpi=300):\n",
    "    \"\"\"\n",
    "    Creates a grid of subplots to visualize adoption rates across different groups, with an option to include 'Existing Equipment' in the analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframes (list of pd.DataFrame): A list of pandas DataFrames to plot.\n",
    "    - subplot_positions (list of tuples): Positions of subplots in the grid, specified as (row, col) tuples.\n",
    "    - x_cols (list of str): Names of the columns in each DataFrame to be used for the main data in plots.\n",
    "    - groups (int, optional): Determines whether to group the data by one or two dimensions (default is 2).\n",
    "    - groupby1 (str, optional): The name of the first grouping column (default is f'base_{category}_fuel').\n",
    "    - groupby2 (str, optional): The name of the second grouping column, with a placeholder default 'default_groupby2'.\n",
    "    - x_labels (list of str, optional): Labels for the x-axis of each subplot (defaults to names in x_cols if not provided).\n",
    "    - plot_titles (list of str, optional): Titles for each subplot (defaults to empty strings if not provided).\n",
    "    - y_labels (list of str, optional): Labels for the y-axis of each subplot (defaults to empty strings if not provided).\n",
    "    - suptitle (str, optional): A central title for the entire figure.\n",
    "    - figure_size (tuple, optional): Size of the entire figure (width, height) in inches (default is (12, 10)).\n",
    "    - sharex (bool, optional): Whether subplots should share the same x-axis (default is False).\n",
    "    - sharey (bool, optional): Whether subplots should share the same y-axis (default is False).\n",
    "    - rotate_landscape (bool, optional): Rotates the grid for a landscape orientation (default is False).\n",
    "    - filter_fuel (list, optional): Filters the data to include only specified f'base_{category}_fuel' values before plotting.\n",
    "    - display_obs (str, optional): Determines whether to display counts or percentages of observations next to each bar.\n",
    "    - export_filename (str, optional): If provided, the figure will be saved to this filename instead of displayed.\n",
    "    - export_format (str, optional): The file format for saving the figure (default is 'png').\n",
    "    - dpi (int, optional): The resolution of the figure for saving (default is 300).\n",
    "\n",
    "    Returns:\n",
    "    None. Displays or saves the figure based on the provided parameters.\n",
    "\n",
    "    Note:\n",
    "    - Ensure all provided DataFrames, column names, and other parameters are valid and consistent.\n",
    "    - The function dynamically adjusts subplot arrangements and legends based on input parameters.\n",
    "    - 'default_groupby2' is a placeholder and should be replaced or handled appropriately within the function.\n",
    "    \"\"\"   \n",
    "    num_subplots = len(subplot_positions)\n",
    "    num_cols = max(pos[1] for pos in subplot_positions) + 1\n",
    "    num_rows = max(pos[0] for pos in subplot_positions) + 1\n",
    "\n",
    "    if rotate_landscape:\n",
    "        num_cols, num_rows = num_rows, num_cols  # Swap rows and columns for landscape\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=figure_size, sharex=sharex, sharey=sharey)\n",
    "\n",
    "    # Convert axes to a 2A array to match the grid\n",
    "    axes = axes.reshape((num_rows, num_cols))\n",
    "\n",
    "    # Default x_labels if not provided\n",
    "    if x_labels is None:\n",
    "        x_labels = x_cols\n",
    "\n",
    "    # Default plot_titles if not provided\n",
    "    if plot_titles is None:\n",
    "        plot_titles = [''] * num_subplots\n",
    "\n",
    "    # Default y_labels if not provided\n",
    "    if y_labels is None:\n",
    "        y_labels = [''] * num_subplots\n",
    "\n",
    "    # Define the parameters for each subplot\n",
    "    for idx, df in enumerate(dataframes):\n",
    "        # Apply the filter_fuel if provided\n",
    "        if filter_fuel:\n",
    "            df = df[df['base_fuel'].isin(filter_fuel)]\n",
    "\n",
    "        pos = subplot_positions[idx]  # Define the subplot position\n",
    "\n",
    "        params = {\n",
    "            'ax': axes[pos[0], pos[1]] if not rotate_landscape else axes[pos[1], pos[0]],\n",
    "            'df': df,\n",
    "            'main_data_column': x_cols[idx],\n",
    "            'groups': groups,\n",
    "            'groupby1': groupby1,\n",
    "            'groupby2': groupby2,\n",
    "            'x_label': x_labels[idx],\n",
    "            'y_label': y_labels[idx],\n",
    "            'plot_title': plot_titles[idx],\n",
    "            'desired_order': ['Existing Equipment', 'Adoption', 'Potential Adoption with Subsidy', 'Averse to Adoption'],  # Pass the desired_order as a parameter\n",
    "            'display_obs': display_obs  # Pass display_obs to the create_subplot_adoption function\n",
    "        }\n",
    "\n",
    "        # Plot each subplot using the defined parameters\n",
    "        create_subplot_adoption(**params)  # Call your custom function\n",
    "\n",
    "        if suptitle:\n",
    "            plt.suptitle(suptitle, fontweight='bold')\n",
    "\n",
    "    # Add a legend for the color mapping at the bottom of the entire figure\n",
    "    legend_labels = list(color_mapping.keys())\n",
    "    legend_handles = [plt.Rectangle((0, 0), 1, 1, color=color_mapping[label]) for label in legend_labels]\n",
    "            \n",
    "    fig.legend(legend_handles, legend_labels, loc='lower center', ncol=len(legend_labels), prop={'size': 32}, labelspacing=0.5, bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Export the figure if export_filename is provided\n",
    "    if export_filename:\n",
    "        save_figure_path = os.path.join(save_figure_directory, export_filename)\n",
    "        plt.savefig(save_figure_path, format=export_format, dpi=dpi)\n",
    "    # Otherwise show the plot in Jupyter Notebook\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a6449",
   "metadata": {},
   "source": [
    "# Adoption Rate Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb1d50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_group_percentages(counts, group):\n",
    "    # Initialize total adoption with subsidy to 0\n",
    "    total_adoption_with_subsidy = 0\n",
    "    \n",
    "    # Check and sum 'Adoption' and 'Potential Adoption with Subsidy' if they exist\n",
    "    if 'Adoption' in counts.columns:\n",
    "        total_adoption_with_subsidy += counts.loc[group, 'Adoption']\n",
    "    if 'Potential Adoption with Subsidy' in counts.columns:\n",
    "        total_adoption_with_subsidy += counts.loc[group, 'Potential Adoption with Subsidy']\n",
    "\n",
    "    # Format percentages, including checks for existence before accessing\n",
    "    formatted_percentages = ', '.join(f\"{decision_prefix}{counts.loc[group, decision]:.1f}%\" \n",
    "                                      for decision, decision_prefix in [('Adoption', 'AD '), ('Potential Adoption with Subsidy', 'PAS ')]\n",
    "                                      if decision in counts.columns)\n",
    "    formatted_percentages += f\", TAS {total_adoption_with_subsidy:.1f}%\"\n",
    "    return formatted_percentages\n",
    "\n",
    "def print_combined_adoption_decision_percentages(dataframes, data_columns, groups, groupby1, groupby2=None, filter_fuel=None):\n",
    "    # Initialize a dictionary to hold the results\n",
    "    results = {}\n",
    "    \n",
    "    # Add a key for overall percentages\n",
    "    overall_key = \"('Overall')\"\n",
    "    results[overall_key] = []\n",
    "\n",
    "    # Iterate over each DataFrame and corresponding main_data_column\n",
    "    for df, data_column in zip(dataframes, data_columns):\n",
    "#         df_filtered = df.copy()\n",
    "\n",
    "        # Filter out the 'Existing Equipment' category from the dataframe\n",
    "        df_filtered = df[df[data_column] != 'Existing Equipment']\n",
    "\n",
    "        # Apply the filter_fuel if provided\n",
    "        if filter_fuel:\n",
    "            df_filtered = df_filtered[df_filtered['base_fuel'].isin(filter_fuel)]\n",
    "        \n",
    "        # Calculate overall percentages for the entire data column\n",
    "        overall_counts = df_filtered[data_column].value_counts(normalize=True) * 100\n",
    "        # Calculate Total Adoption with Subsidy\n",
    "        total_adoption_with_subsidy = overall_counts.get('Adoption', 0) + overall_counts.get('Potential Adoption with Subsidy', 0)\n",
    "\n",
    "        overall_percentages = ', '.join(f\"{decision_prefix}{overall_counts[decision]:.1f}%\" \n",
    "                                        for decision, decision_prefix in [('Adoption', 'AD '), ('Potential Adoption with Subsidy', 'PAS ')]\n",
    "                                        if decision in overall_counts.index)\n",
    "        overall_percentages += f\", TAS {total_adoption_with_subsidy:.1f}%\"\n",
    "        results[overall_key].append(overall_percentages)\n",
    "        \n",
    "        if groups == 1 or groups == '1':\n",
    "            # Calculate the percentages for each combination of categories\n",
    "            counts = df_filtered.groupby(f'{groupby1}')[f'{data_column}'].value_counts(normalize=True).unstack() * 100\n",
    "            for group in counts.index:\n",
    "                key = f\"('{groupby1}', '{group}')\"\n",
    "                if key not in results:\n",
    "                    results[key] = []\n",
    "                \n",
    "                # Calculate and format percentages including Total Adoption with Subsidy\n",
    "                formatted_percentages = format_group_percentages(counts, group)\n",
    "                results[key].append(formatted_percentages)\n",
    "                \n",
    "        elif groups == 2 or groups == '2' and groupby2 is not None:\n",
    "            # Calculate the percentages for each combination of categories\n",
    "            counts = df_filtered.groupby([groupby1, groupby2])[f'{data_column}'].value_counts(normalize=True).unstack() * 100\n",
    "            for group1_group2 in counts.index:\n",
    "                key = f\"('{group1_group2[0]}', '{group1_group2[1]}')\"\n",
    "                if key not in results:\n",
    "                    results[key] = []\n",
    "\n",
    "                # Calculate and format percentages including Total Adoption with Subsidy\n",
    "                formatted_percentages = format_group_percentages(counts, group1_group2)\n",
    "                results[key].append(formatted_percentages)\n",
    "    \n",
    "    # Print combined results for overall and then for each group\n",
    "    for key, values in results.items():\n",
    "        combined_values = ' | '.join(values)\n",
    "        print(f\"{key}: {combined_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1d344010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to filter out specific decision columns\n",
    "# def filter_columns(df):\n",
    "#     # We want to keep columns containing 'Adoption' or 'Total Adoption with Subsidy' only\n",
    "#     keep_columns = [col for col in df.columns if ('Adoption' in col[1] or 'Total Adoption with Subsidy' in col[1]) and 'Averse to Adoption' not in col[1] and 'Potential Adoption with Subsidy' not in col[1] and 'Existing Equipment' not in col[1]]\n",
    "#     return df[keep_columns]\n",
    "\n",
    "# def create_multiIndex_adoption_df(df, menu_mp, category, cost_assumptions):\n",
    "#     # Create a copy of the dataframe\n",
    "#     df_copy = df.copy()\n",
    "        \n",
    "#     # Begin df with these cols\n",
    "#     adoption_cols = ['bldg_id', 'federal_poverty_level', 'lowModerateIncome_designation', 'state', 'end_use', 'base_fuel']\n",
    "\n",
    "#     for cost_type in cost_assumptions:\n",
    "#         cols = [\n",
    "#             f'mp{menu_mp}_{category}_adoption',\n",
    "#             f'ira_mp{menu_mp}_{category}_adoption',\n",
    "#             f'ira_gridDecarb_mp{menu_mp}_{category}_adoption',\n",
    "#         ]\n",
    "#         adoption_cols.extend(cols)\n",
    "        \n",
    "#     df_copy = df_copy[adoption_cols]\n",
    "    \n",
    "#     df_multi_index = round((df_copy.groupby(['base_fuel', 'lowModerateIncome_designation'])[adoption_columns].apply(lambda x: x.apply(lambda y: y.value_counts(normalize=True))).unstack().fillna(0) * 100), 2)\n",
    "    \n",
    "#     # Iterate through each adoption reference column and add the new category\n",
    "#     for column in adoption_columns:\n",
    "#         # Sum the 'Adoption' and 'Potential Adoption with Subsidy' categories\n",
    "#         df_multi_index[(column, 'Total Adoption with Subsidy')] = (\n",
    "#             df_multi_index[(column, 'Adoption')] + df_multi_index[(column, 'Potential Adoption with Subsidy')]\n",
    "#         )\n",
    "    \n",
    "#     # Apply the function to your DataFrame\n",
    "#     df_multi_index = filter_columns(df_multi_index)\n",
    "\n",
    "#     # Specify new column order with 'Adoption' first and 'Total Adoption with Subsidy' second for each group\n",
    "#     for col in adoption_cols:\n",
    "#         new_order = [\n",
    "#             (col, 'Adoption'),\n",
    "#             (col, 'Total Adoption with Subsidy'),\n",
    "#         ]\n",
    "\n",
    "#     # Reorder the columns according to the new_order\n",
    "#     df_multi_index = df_multi_index.loc[:, new_order]\n",
    "    \n",
    "#     # Set the 'lowModerateIncome_designation' index as a categorical index with a specified order\n",
    "#     filtered_df.index = filtered_df.index.set_levels(\n",
    "#         pd.Categorical(filtered_df.index.levels[1], categories=['Low-Income', 'Moderate-Income', 'Middle-to-Upper-Income'], ordered=True),\n",
    "#         level=1\n",
    "#     )\n",
    "    \n",
    "#     # Sort the DataFrame based on the entire index\n",
    "#     df_multi_index = df_multi_index.sort_index(level=['base_fuel', 'lowModerateIncome_designation'])\n",
    "\n",
    "#     return df_multi_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "411fe3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost_assumptions=['progressive', 'reference', 'conservative']\n",
    "# menu_mp=8\n",
    "# category='heating'\n",
    "\n",
    "# adoption_cols = []\n",
    "\n",
    "# for cost_type in cost_assumptions:\n",
    "#     cols = [\n",
    "#         f'mp{menu_mp}_{category}_adoption',\n",
    "#         f'ira_mp{menu_mp}_{category}_adoption',\n",
    "#         f'ira_gridDecarb_mp{menu_mp}_{category}_adoption',\n",
    "#     ]\n",
    "#     adoption_cols.extend(cols)\n",
    "# print(adoption_cols)\n",
    "\n",
    "# # Group by 'base_fuel' and 'federal_poverty_level', then apply value_counts normalized\n",
    "# percentages_df = round((df_basic_adoption_heating.groupby(['base_fuel', 'lowModerateIncome_designation'])[adoption_columns].apply(lambda x: x.apply(lambda y: y.value_counts(normalize=True))).unstack().fillna(0) * 100), 2)\n",
    "\n",
    "# # Iterate through each adoption reference column and add the new category\n",
    "# for column in adoption_columns:\n",
    "#     # Sum the 'Adoption' and 'Potential Adoption with Subsidy' categories\n",
    "#     percentages_df[(column, 'Total Adoption with Subsidy')] = (\n",
    "#         percentages_df[(column, 'Adoption')] + percentages_df[(column, 'Potential Adoption with Subsidy')]\n",
    "#     )\n",
    "\n",
    "# # Rebuild the column MultiIndex to ensure proper structure and order\n",
    "# percentages_df.columns = pd.MultiIndex.from_tuples(percentages_df.columns)\n",
    "\n",
    "# # Sort the columns to ensure that 'Total Adoption with Subsidy' appears directly under its respective category\n",
    "# percentages_df = percentages_df.sort_index(axis=1, ascending=False)\n",
    "# # percentages_df\n",
    "\n",
    "# def filter_columns(df):\n",
    "#     # We want to keep columns containing 'Adoption' or 'Total Adoption with Subsidy' only\n",
    "#     keep_columns = [col for col in df.columns if ('Adoption' in col[1] or 'Total Adoption with Subsidy' in col[1]) and 'Averse to Adoption' not in col[1] and 'Potential Adoption with Subsidy' not in col[1] and 'Existing Equipment' not in col[1]]\n",
    "#     return df[keep_columns]\n",
    "\n",
    "# # Apply the function to your DataFrame\n",
    "# filtered_df = filter_columns(percentages_df)\n",
    "# # filtered_df\n",
    "\n",
    "# # Set the 'lowModerateIncome_designation' index as a categorical index with a specified order\n",
    "# filtered_df.index = filtered_df.index.set_levels(\n",
    "#     pd.Categorical(filtered_df.index.levels[1], categories=['Low-Income', 'Moderate-Income', 'Middle-to-Upper-Income'], ordered=True),\n",
    "#     level=1\n",
    "# )\n",
    "\n",
    "# # Specify new column order with 'Adoption' first and 'Total Adoption with Subsidy' second for each group\n",
    "# new_order = [\n",
    "#     ('mp8_heating_adoption_reference', 'Adoption'),\n",
    "#     ('mp8_heating_adoption_reference', 'Total Adoption with Subsidy'),\n",
    "#     ('ira_mp8_heating_adoption_reference', 'Adoption'),\n",
    "#     ('ira_mp8_heating_adoption_reference', 'Total Adoption with Subsidy'),\n",
    "#     ('ira_gridDecarb_mp8_heating_adoption_reference', 'Adoption'),\n",
    "#     ('ira_gridDecarb_mp8_heating_adoption_reference', 'Total Adoption with Subsidy')\n",
    "# ]\n",
    "\n",
    "# # Reorder the columns according to the new_order\n",
    "# filtered_df = filtered_df.loc[:, new_order]\n",
    "\n",
    "# # Sort the DataFrame based on the entire index\n",
    "# filtered_df = filtered_df.sort_index(level=['base_fuel', 'lowModerateIncome_designation'])\n",
    "# filtered_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
